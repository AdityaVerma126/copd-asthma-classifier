{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaVerma126/copd-asthma-classifier/blob/main/COPD_ASTHMA_CLASSIFIER_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK1ohA8XfJOs"
      },
      "source": [
        "# üìò Project Title: *Lung Sound Classification with Deep Learning*\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Objective\n",
        "\n",
        "## üîß Setup & Installations\n",
        "\n",
        "## üìÇ Data Loading and Preprocessing\n",
        "\n",
        "## üéõÔ∏è Feature Extraction: MFCC\n",
        "\n",
        "## üß™ Data Augmentation\n",
        "\n",
        "## üèóÔ∏è Model Building: GRU\n",
        "\n",
        "## ‚öôÔ∏è Training\n",
        "\n",
        "## üìà Evaluation\n",
        "\n",
        "## üìä Visualize Performance\n",
        "\n",
        "## ‚úÖ Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu6ehGQRfJLW"
      },
      "source": [
        "## üìö Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5deR3Kf_nllT"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# üìö IMPORTING REQUIRED LIBRARIES\n",
        "# =============================================\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# File operations\n",
        "import os\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# Model utilities\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AgiZcZFowEH",
        "outputId": "c6e99215-fada-4916-a49b-07c3354d7433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHBRVFsShTUa"
      },
      "source": [
        "## üì• Downloading Dataset from KaggleHub\n",
        "\n",
        "We will use `kagglehub` to download the **lung sound dataset** hosted on Kaggle. This will help in streamlining data access and keeping the notebook modular.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcPK651jowAo",
        "outputId": "d669bf82-a4a6-4d66-8935-a3df998e0ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'lung-dataset' dataset.\n",
            "‚úÖ Path to dataset files: /kaggle/input/lung-dataset\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üì• DOWNLOADING DATASET FROM KAGGLEHUB\n",
        "# ============================================\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download the latest version of the lung dataset\n",
        "path = kagglehub.dataset_download(\"arashnic/lung-dataset\")\n",
        "\n",
        "print(\"‚úÖ Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPWuk76jJQg"
      },
      "source": [
        "## üìä Exploratory Data Analysis (EDA): Lung Disease Distribution\n",
        "\n",
        "We begin by analyzing the distribution of different lung disease diagnoses based on the annotated dataset. Plotly is used for interactive visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "ni3plRJZov9Y",
        "outputId": "d5baa561-d732-4c08-dc24-3105b6fe9ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ Disease Counts:\n",
            "\n",
            "Disease\n",
            "N                                 35\n",
            "Asthma                            17\n",
            "asthma                            15\n",
            "heart failure                     15\n",
            "COPD                               8\n",
            "pneumonia                          5\n",
            "Lung Fibrosis                      4\n",
            "BRON                               3\n",
            "Heart Failure                      3\n",
            "Plueral Effusion                   2\n",
            "Heart Failure + COPD               2\n",
            "Heart Failure + Lung Fibrosis      1\n",
            "Asthma and lung fibrosis           1\n",
            "copd                               1\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"aeefee22-283a-4fc8-99a2-b8e877080ac3\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aeefee22-283a-4fc8-99a2-b8e877080ac3\")) {                    Plotly.newPlot(                        \"aeefee22-283a-4fc8-99a2-b8e877080ac3\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"\\u003cb\\u003e%{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eCount: %{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[35,17,15,15,8,5,4,3,3,2,2,1,1,1],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"N\",\"Asthma\",\"asthma\",\"heart failure\",\"COPD\",\"pneumonia\",\"Lung Fibrosis\",\"BRON\",\"Heart Failure\",\"Plueral Effusion\",\"Heart Failure + COPD\",\"Heart Failure + Lung Fibrosis \",\"Asthma and lung fibrosis\",\"copd\"],\"xaxis\":\"x\",\"y\":[35,17,15,15,8,5,4,3,3,2,2,1,1,1],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Disease Type\"},\"tickangle\":-90},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ü¶† Distribution of Lung Disease Diagnoses\"},\"barmode\":\"relative\",\"font\":{\"size\":12},\"showlegend\":false,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('aeefee22-283a-4fc8-99a2-b8e877080ac3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ====================================================\n",
        "# üìä LOAD & VISUALIZE LUNG DISEASE DISTRIBUTION\n",
        "# ====================================================\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# üîπ Load diagnosis annotation file\n",
        "diagnosis_data = pd.read_excel(\n",
        "    r\"/kaggle/input/lung-dataset/Data annotation.xlsx\",\n",
        "    usecols='B, E',\n",
        "    names=['Sex', 'Disease']\n",
        ")\n",
        "\n",
        "# üîπ Preview data\n",
        "diagnosis_data.head(4)\n",
        "\n",
        "# üîπ Print disease label counts\n",
        "print(\"üß¨ Disease Counts:\\n\")\n",
        "print(diagnosis_data.Disease.value_counts())\n",
        "\n",
        "# üîπ Count frequency of diseases\n",
        "disease_counts = diagnosis_data.Disease.value_counts()\n",
        "\n",
        "# üîπ Create an interactive bar chart\n",
        "fig = px.bar(\n",
        "    x=disease_counts.index,\n",
        "    y=disease_counts.values,\n",
        "    title='ü¶† Distribution of Lung Disease Diagnoses',\n",
        "    labels={'x': 'Disease Type', 'y': 'Count'},\n",
        "    color=disease_counts.values,\n",
        "    color_continuous_scale='viridis'\n",
        ")\n",
        "\n",
        "# üîπ Customize layout\n",
        "fig.update_layout(\n",
        "    xaxis_tickangle=-90,\n",
        "    showlegend=False,\n",
        "    height=500,\n",
        "    font=dict(size=12)\n",
        ")\n",
        "\n",
        "# üîπ Enhance hover tooltips\n",
        "fig.update_traces(\n",
        "    hovertemplate='<b>%{x}</b><br>Count: %{y}<extra></extra>'\n",
        ")\n",
        "\n",
        "# üîπ Show plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM_H2EetjeEJ"
      },
      "source": [
        "## üß† Custom Audio File Handler Class\n",
        "\n",
        "This section defines a robust `AudioFileHandler` class that performs:\n",
        "\n",
        "- üîç Recursive file discovery  \n",
        "- üß™ Validation based on size and duration  \n",
        "- üìä Label-based file organization  \n",
        "- üìù Manifest creation  \n",
        "- üìà Audio statistics summary  \n",
        "- üóÇÔ∏è Compatibility functions for both simple and advanced use cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJLGXtbnov7p",
        "outputId": "29a8f38d-6106-4cc1-bcd5-8652d5017b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Original approach (enhanced):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating files:   0%|          | 0/336 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join, exists, getsize\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import glob\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s %(message)s',\n",
        "                   datefmt='%H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AudioFileHandler:\n",
        "    \"\"\"Enhanced audio file handling with validation, organization, and analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str, supported_formats: List[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the AudioFileHandler.\n",
        "\n",
        "        Args:\n",
        "            base_path: Base directory path containing audio files\n",
        "            supported_formats: List of supported audio formats (default: common formats)\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_path)\n",
        "        self.supported_formats = supported_formats or ['.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg']\n",
        "        self.file_info = {}\n",
        "        self.stats = {\n",
        "            'total_files': 0,\n",
        "            'valid_files': 0,\n",
        "            'invalid_files': 0,\n",
        "            'by_format': Counter(),\n",
        "            'by_label': Counter(),\n",
        "            'file_sizes': [],\n",
        "            'durations': []\n",
        "        }\n",
        "\n",
        "        logger.info(f\"üîß Initialized AudioFileHandler\")\n",
        "        logger.info(f\"   üìÅ Base path: {self.base_path}\")\n",
        "        logger.info(f\"   üéµ Supported formats: {self.supported_formats}\")\n",
        "\n",
        "    def discover_audio_files(self,\n",
        "                           recursive: bool = True,\n",
        "                           validate_files: bool = True,\n",
        "                           min_size_kb: float = 1.0,\n",
        "                           max_size_mb: float = 100.0) -> List[str]:\n",
        "        \"\"\"\n",
        "        Discover and validate audio files in the directory.\n",
        "\n",
        "        Args:\n",
        "            recursive: Search subdirectories recursively\n",
        "            validate_files: Validate file integrity\n",
        "            min_size_kb: Minimum file size in KB\n",
        "            max_size_mb: Maximum file size in MB\n",
        "\n",
        "        Returns:\n",
        "            List of valid audio file paths\n",
        "        \"\"\"\n",
        "        logger.info(f\"üîç Discovering audio files...\")\n",
        "\n",
        "        # Check if base path exists\n",
        "        if not self.base_path.exists():\n",
        "            logger.error(f\"‚ùå Base path does not exist: {self.base_path}\")\n",
        "            return []\n",
        "\n",
        "        # Find all audio files\n",
        "        all_files = []\n",
        "\n",
        "        if recursive:\n",
        "            # Recursive search\n",
        "            for format_ext in self.supported_formats:\n",
        "                pattern = f\"**/*{format_ext}\"\n",
        "                files = list(self.base_path.glob(pattern))\n",
        "                all_files.extend([str(f) for f in files])\n",
        "        else:\n",
        "            # Non-recursive search (original approach)\n",
        "            try:\n",
        "                filenames = [f for f in listdir(str(self.base_path))\n",
        "                           if isfile(join(str(self.base_path), f))]\n",
        "\n",
        "                # Filter by supported formats\n",
        "                audio_files = [f for f in filenames\n",
        "                             if any(f.lower().endswith(ext) for ext in self.supported_formats)]\n",
        "\n",
        "                all_files = [join(str(self.base_path), f) for f in audio_files]\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Error reading directory: {e}\")\n",
        "                return []\n",
        "\n",
        "        logger.info(f\"üìÅ Found {len(all_files)} potential audio files\")\n",
        "\n",
        "        if not all_files:\n",
        "            logger.warning(\"‚ö†Ô∏è No audio files found!\")\n",
        "            return []\n",
        "\n",
        "        # Validate and filter files\n",
        "        valid_files = []\n",
        "\n",
        "        if validate_files:\n",
        "            logger.info(\"üîç Validating audio files...\")\n",
        "            valid_files = self._validate_audio_files(all_files, min_size_kb, max_size_mb)\n",
        "        else:\n",
        "            # Basic size filtering without audio validation\n",
        "            for filepath in all_files:\n",
        "                if self._check_file_size(filepath, min_size_kb, max_size_mb):\n",
        "                    valid_files.append(filepath)\n",
        "\n",
        "        # Update statistics\n",
        "        self.stats['total_files'] = len(all_files)\n",
        "        self.stats['valid_files'] = len(valid_files)\n",
        "        self.stats['invalid_files'] = len(all_files) - len(valid_files)\n",
        "\n",
        "        self._analyze_files(valid_files)\n",
        "        self._print_discovery_summary()\n",
        "\n",
        "        return sorted(valid_files)\n",
        "\n",
        "    def _validate_audio_files(self, filepaths: List[str],\n",
        "                            min_size_kb: float, max_size_mb: float) -> List[str]:\n",
        "        \"\"\"Validate audio files for integrity and properties.\"\"\"\n",
        "        valid_files = []\n",
        "\n",
        "        for filepath in tqdm(filepaths, desc=\"Validating files\"):\n",
        "            try:\n",
        "                # Check file size\n",
        "                if not self._check_file_size(filepath, min_size_kb, max_size_mb):\n",
        "                    continue\n",
        "\n",
        "                # Try to load audio metadata (quick check)\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "                    duration = librosa.get_duration(path=filepath)\n",
        "\n",
        "                if duration > 0:\n",
        "                    self.file_info[filepath] = {\n",
        "                        'size_mb': getsize(filepath) / (1024 * 1024),\n",
        "                        'duration': duration,\n",
        "                        'format': Path(filepath).suffix.lower()\n",
        "                    }\n",
        "                    valid_files.append(filepath)\n",
        "                    self.stats['durations'].append(duration)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"‚ùå Invalid audio file {os.path.basename(filepath)}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def _check_file_size(self, filepath: str, min_size_kb: float, max_size_mb: float) -> bool:\n",
        "        \"\"\"Check if file size is within acceptable range.\"\"\"\n",
        "        try:\n",
        "            size_bytes = getsize(filepath)\n",
        "            size_kb = size_bytes / 1024\n",
        "            size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "            self.stats['file_sizes'].append(size_mb)\n",
        "\n",
        "            return min_size_kb <= size_kb <= (max_size_mb * 1024)\n",
        "\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _analyze_files(self, filepaths: List[str]):\n",
        "        \"\"\"Analyze file distribution by format and label.\"\"\"\n",
        "        for filepath in filepaths:\n",
        "            # Count by format\n",
        "            format_ext = Path(filepath).suffix.lower()\n",
        "            self.stats['by_format'][format_ext] += 1\n",
        "\n",
        "            # Count by label (extracted from filename)\n",
        "            label = self._extract_label_from_filename(filepath)\n",
        "            self.stats['by_label'][label] += 1\n",
        "\n",
        "    def _extract_label_from_filename(self, filepath: str) -> str:\n",
        "        \"\"\"Extract label from filename or directory structure.\"\"\"\n",
        "        filename = os.path.basename(filepath).lower()\n",
        "        parent_dir = os.path.basename(os.path.dirname(filepath)).lower()\n",
        "\n",
        "        # Define label patterns\n",
        "        label_patterns = {\n",
        "            'Asthma': ['asthma'],\n",
        "            'COPD': ['copd','COPD'],\n",
        "           ######### 'Pneumonia': ['pneumonia'],\n",
        "            'Healthy': ['healthy', 'normal', 'n', 'control']\n",
        "        }\n",
        "\n",
        "        # Check filename first\n",
        "        for label, patterns in label_patterns.items():\n",
        "            if any(pattern in filename for pattern in patterns):\n",
        "                return label\n",
        "\n",
        "        # Check parent directory\n",
        "        for label, patterns in label_patterns.items():\n",
        "            if any(pattern in parent_dir for pattern in patterns):\n",
        "                return label\n",
        "\n",
        "        return 'Unknown'\n",
        "\n",
        "    def organize_by_labels(self, filepaths: List[str]) -> Dict[str, List[str]]:\n",
        "        \"\"\"Organize files by their extracted labels.\"\"\"\n",
        "        organized = defaultdict(list)\n",
        "\n",
        "        for filepath in filepaths:\n",
        "            label = self._extract_label_from_filename(filepath)\n",
        "            organized[label].append(filepath)\n",
        "\n",
        "        logger.info(\"üìä Files organized by labels:\")\n",
        "        for label, files in organized.items():\n",
        "            logger.info(f\"   {label}: {len(files)} files\")\n",
        "\n",
        "        return dict(organized)\n",
        "\n",
        "    def get_file_statistics(self) -> Dict:\n",
        "        \"\"\"Get comprehensive file statistics.\"\"\"\n",
        "        stats = self.stats.copy()\n",
        "\n",
        "        if self.stats['file_sizes']:\n",
        "            stats['size_stats'] = {\n",
        "                'mean_mb': sum(self.stats['file_sizes']) / len(self.stats['file_sizes']),\n",
        "                'min_mb': min(self.stats['file_sizes']),\n",
        "                'max_mb': max(self.stats['file_sizes']),\n",
        "                'total_mb': sum(self.stats['file_sizes'])\n",
        "            }\n",
        "\n",
        "        if self.stats['durations']:\n",
        "            stats['duration_stats'] = {\n",
        "                'mean_sec': sum(self.stats['durations']) / len(self.stats['durations']),\n",
        "                'min_sec': min(self.stats['durations']),\n",
        "                'max_sec': max(self.stats['durations']),\n",
        "                'total_sec': sum(self.stats['durations'])\n",
        "            }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def create_file_manifest(self, filepaths: List[str], save_path: Optional[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Create a detailed manifest of all audio files.\"\"\"\n",
        "        manifest_data = []\n",
        "\n",
        "        for filepath in filepaths:\n",
        "            info = self.file_info.get(filepath, {})\n",
        "\n",
        "            manifest_data.append({\n",
        "                'filepath': filepath,\n",
        "                'filename': os.path.basename(filepath),\n",
        "                'directory': os.path.dirname(filepath),\n",
        "                'label': self._extract_label_from_filename(filepath),\n",
        "                'format': info.get('format', Path(filepath).suffix.lower()),\n",
        "                'size_mb': info.get('size_mb', getsize(filepath) / (1024 * 1024)),\n",
        "                'duration_sec': info.get('duration', None),\n",
        "                'exists': os.path.exists(filepath)\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(manifest_data)\n",
        "\n",
        "        if save_path:\n",
        "            df.to_csv(save_path, index=False)\n",
        "            logger.info(f\"üíæ File manifest saved to: {save_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _print_discovery_summary(self):\n",
        "        \"\"\"Print file discovery summary.\"\"\"\n",
        "        logger.info(\"üìä File Discovery Summary:\")\n",
        "        logger.info(f\"   üìÅ Total files found: {self.stats['total_files']}\")\n",
        "        logger.info(f\"   ‚úÖ Valid files: {self.stats['valid_files']}\")\n",
        "        logger.info(f\"   ‚ùå Invalid files: {self.stats['invalid_files']}\")\n",
        "\n",
        "        if self.stats['by_format']:\n",
        "            logger.info(\"   üìÑ By format:\")\n",
        "            for format_ext, count in self.stats['by_format'].most_common():\n",
        "                logger.info(f\"      {format_ext}: {count} files\")\n",
        "\n",
        "        if self.stats['by_label']:\n",
        "            logger.info(\"   üè∑Ô∏è By label:\")\n",
        "            for label, count in self.stats['by_label'].most_common():\n",
        "                logger.info(f\"      {label}: {count} files\")\n",
        "\n",
        "        if self.stats['file_sizes']:\n",
        "            avg_size = sum(self.stats['file_sizes']) / len(self.stats['file_sizes'])\n",
        "            total_size = sum(self.stats['file_sizes'])\n",
        "            logger.info(f\"   üíæ Average file size: {avg_size:.2f} MB\")\n",
        "            logger.info(f\"   üíæ Total dataset size: {total_size:.2f} MB\")\n",
        "\n",
        "        if self.stats['durations']:\n",
        "            avg_duration = sum(self.stats['durations']) / len(self.stats['durations'])\n",
        "            total_duration = sum(self.stats['durations'])\n",
        "            logger.info(f\"   ‚è±Ô∏è Average duration: {avg_duration:.2f} seconds\")\n",
        "            logger.info(f\"   ‚è±Ô∏è Total duration: {total_duration/60:.1f} minutes\")\n",
        "\n",
        "# Enhanced file discovery functions\n",
        "def discover_audio_files(base_path: str,\n",
        "                        recursive: bool = True,\n",
        "                        validate_files: bool = True,\n",
        "                        supported_formats: List[str] = None,\n",
        "                        min_size_kb: float = 1.0,\n",
        "                        max_size_mb: float = 100.0,\n",
        "                        create_manifest: bool = False,\n",
        "                        manifest_path: str = \"audio_manifest.csv\") -> List[str]:\n",
        "    \"\"\"\n",
        "    Enhanced audio file discovery function.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory path containing audio files\n",
        "        recursive: Search subdirectories recursively\n",
        "        validate_files: Validate audio file integrity\n",
        "        supported_formats: List of supported audio formats\n",
        "        min_size_kb: Minimum file size in KB\n",
        "        max_size_mb: Maximum file size in MB\n",
        "        create_manifest: Create CSV manifest of files\n",
        "        manifest_path: Path to save manifest CSV\n",
        "\n",
        "    Returns:\n",
        "        List of valid audio file paths\n",
        "    \"\"\"\n",
        "    handler = AudioFileHandler(base_path, supported_formats)\n",
        "\n",
        "    filepaths = handler.discover_audio_files(\n",
        "        recursive=recursive,\n",
        "        validate_files=validate_files,\n",
        "        min_size_kb=min_size_kb,\n",
        "        max_size_mb=max_size_mb\n",
        "    )\n",
        "\n",
        "    if create_manifest and filepaths:\n",
        "        handler.create_file_manifest(filepaths, manifest_path)\n",
        "\n",
        "    return filepaths\n",
        "\n",
        "def get_organized_file_paths(base_path: str, **kwargs) -> Tuple[List[str], Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Get file paths organized by labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (all_filepaths, organized_by_label_dict)\n",
        "    \"\"\"\n",
        "    handler = AudioFileHandler(base_path)\n",
        "    filepaths = handler.discover_audio_files(**kwargs)\n",
        "    organized = handler.organize_by_labels(filepaths)\n",
        "\n",
        "    return filepaths, organized\n",
        "\n",
        "# Simple function that maintains your original interface\n",
        "def get_audio_filepaths(mypath: str,\n",
        "                       recursive: bool = False,\n",
        "                       validate: bool = True,\n",
        "                       show_info: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple function that maintains compatibility with your original code.\n",
        "\n",
        "    Args:\n",
        "        mypath: Path to audio files directory\n",
        "        recursive: Search subdirectories\n",
        "        validate: Validate audio files\n",
        "        show_info: Print file information\n",
        "\n",
        "    Returns:\n",
        "        List of audio file paths\n",
        "    \"\"\"\n",
        "    handler = AudioFileHandler(mypath)\n",
        "\n",
        "    filepaths = handler.discover_audio_files(\n",
        "        recursive=recursive,\n",
        "        validate_files=validate\n",
        "    )\n",
        "\n",
        "    if show_info and filepaths:\n",
        "        print(f\"\\nüìÅ Found {len(filepaths)} audio files:\")\n",
        "\n",
        "        # Show first few files\n",
        "        for i, filepath in enumerate(filepaths[:5]):\n",
        "            print(f\"   {i+1}. {os.path.basename(filepath)}\")\n",
        "\n",
        "        if len(filepaths) > 5:\n",
        "            print(f\"   ... and {len(filepaths) - 5} more files\")\n",
        "\n",
        "        # Show file distribution\n",
        "        organized = handler.organize_by_labels(filepaths)\n",
        "        print(f\"\\nüè∑Ô∏è Distribution by labels:\")\n",
        "        for label, files in organized.items():\n",
        "            print(f\"   {label}: {len(files)} files\")\n",
        "\n",
        "    return filepaths\n",
        "\n",
        "# Example usage with your original structure improved\n",
        "if __name__ == \"__main__\":\n",
        "    # Your original approach - enhanced\n",
        "    mypath = \"/kaggle/input/lung-dataset/Audio Files\"\n",
        "\n",
        "    print(\"üîÑ Original approach (enhanced):\")\n",
        "    filepaths = get_audio_filepaths(mypath, validate=True, show_info=True)\n",
        "\n",
        "    # Advanced usage\n",
        "    print(\"\\nüöÄ Advanced approach:\")\n",
        "    filepaths_advanced = discover_audio_files(\n",
        "        base_path=mypath,\n",
        "        recursive=True,          # Search subdirectories\n",
        "        validate_files=True,     # Validate audio integrity\n",
        "        min_size_kb=1.0,        # Minimum 1KB files\n",
        "        max_size_mb=50.0,       # Maximum 50MB files\n",
        "        create_manifest=True,    # Create CSV manifest\n",
        "        manifest_path=\"lung_audio_manifest.csv\"\n",
        "    )\n",
        "\n",
        "    # Organized by labels\n",
        "    print(\"\\nüìä Organized by labels:\")\n",
        "    all_files, organized_files = get_organized_file_paths(mypath)\n",
        "\n",
        "    for label, files in organized_files.items():\n",
        "        print(f\"{label}: {len(files)} files\")\n",
        "        if files:\n",
        "            print(f\"   Sample: {os.path.basename(files[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9vNtlerkEUI"
      },
      "source": [
        "## üõ† Audio Preprocessing: Frame Segmentation & Baseline Wander Removal\n",
        "\n",
        "In this section, we:\n",
        "\n",
        "- ‚öôÔ∏è Configure core parameters (frame length, overlap, sampling rate)  \n",
        "- ‚úÇÔ∏è Segment raw lung sound signals into overlapping frames  \n",
        "- üåÄ Remove low‚Äëfrequency baseline wander via DFT filtering  \n",
        "- üöÄ Provide a parallelized preprocessing pipeline for entire datasets  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rMNOo0LQov5c"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ü´Å LUNG SOUND PREPROCESSING PIPELINE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# Advanced preprocessing system for lung sound analysis with frame segmentation\n",
        "# and baseline wander removal using Discrete Fourier Transform (DFT)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, Tuple, List\n",
        "import logging\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import warnings\n",
        "\n",
        "# Configure logging for monitoring preprocessing progress\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress librosa warnings to keep output clean\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ===========================================\n",
        "# üîπ CONFIGURATION PARAMETERS\n",
        "# ===========================================\n",
        "class Config:\n",
        "    FRAME_LENGTH = 20  # seconds - Duration of each audio frame for analysis\n",
        "    OVERLAP_RATIO = 0.4  # 40% overlap between consecutive frames\n",
        "    SAMPLING_RATE = 22050  # Hz - Standard sampling rate for audio processing\n",
        "    BASELINE_CUTOFF = 1  # Hz - Frequency threshold for baseline wander removal\n",
        "    N_PROCESSES = max(1, cpu_count() - 1)  # Leave one core free for system stability\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üîπ FRAME SEGMENTATION FUNCTION\n",
        "# ===========================================\n",
        "def segment_signal(\n",
        "    signal: np.ndarray,\n",
        "    sr: int,\n",
        "    frame_length: float = Config.FRAME_LENGTH,\n",
        "    overlap_ratio: float = Config.OVERLAP_RATIO\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Splits lung sound signal into frames of specified length with given overlap.\n",
        "\n",
        "    Parameters:\n",
        "        signal: Raw lung sound signal (1D NumPy array)\n",
        "        sr: Sampling rate of the signal (in Hz)\n",
        "        frame_length: Frame duration in seconds (default=20 sec)\n",
        "        overlap_ratio: Percentage of overlap between frames (default=0.4)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of framed signals (n_frames, frame_size)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input parameters are invalid\n",
        "    \"\"\"\n",
        "    # Input validation - ensure all parameters are valid\n",
        "    if len(signal) == 0:\n",
        "        raise ValueError(\"Input signal is empty\")\n",
        "    if sr <= 0:\n",
        "        raise ValueError(\"Sampling rate must be positive\")\n",
        "    if frame_length <= 0:\n",
        "        raise ValueError(\"Frame length must be positive\")\n",
        "    if not 0 <= overlap_ratio < 1:\n",
        "        raise ValueError(\"Overlap ratio must be in [0, 1)\")\n",
        "\n",
        "    # Convert frame duration from seconds to number of samples\n",
        "    frame_size = int(sr * frame_length)  # Convert seconds to samples\n",
        "\n",
        "    # Handle edge case where signal is shorter than desired frame length\n",
        "    if frame_size > len(signal):\n",
        "        logger.warning(f\"Signal length ({len(signal)/sr:.2f}s) is shorter than frame length ({frame_length}s)\")\n",
        "        return np.array([signal])  # Return entire signal as single frame\n",
        "\n",
        "    # Calculate step size based on overlap ratio (how much to advance for next frame)\n",
        "    step_size = int(frame_size * (1 - overlap_ratio))  # Step between frames\n",
        "\n",
        "    # Calculate total number of frames that can be extracted\n",
        "    n_frames = max(1, (len(signal) - frame_size) // step_size + 1)\n",
        "\n",
        "    # Vectorized frame creation using numpy's sliding window view for efficiency\n",
        "    starts = np.arange(0, (n_frames - 1) * step_size + 1, step_size)\n",
        "    frames = np.lib.stride_tricks.sliding_window_view(signal, frame_size)[starts]\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üîπ BASELINE WANDER REMOVAL USING DFT\n",
        "# ===========================================\n",
        "def remove_baseline_wander(\n",
        "    signal: np.ndarray,\n",
        "    sr: int,\n",
        "    cutoff_freq: float = Config.BASELINE_CUTOFF\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Removes baseline wander noise (0-1 Hz) using Discrete Fourier Transform (DFT).\n",
        "\n",
        "    Parameters:\n",
        "        signal: Input frame signal (1D array)\n",
        "        sr: Sampling rate (in Hz)\n",
        "        cutoff_freq: Frequency below which DFT coefficients are removed (default=1 Hz)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Filtered signal after inverse DFT\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input parameters are invalid\n",
        "    \"\"\"\n",
        "    # Input validation to ensure valid parameters\n",
        "    if len(signal) == 0:\n",
        "        raise ValueError(\"Input signal is empty\")\n",
        "    if sr <= 0:\n",
        "        raise ValueError(\"Sampling rate must be positive\")\n",
        "    if cutoff_freq <= 0:\n",
        "        raise ValueError(\"Cutoff frequency must be positive\")\n",
        "\n",
        "    # DFT processing: transform signal to frequency domain\n",
        "    M = len(signal)  # DFT length\n",
        "    freqs = np.fft.fftfreq(M, d=1/sr)  # Frequency bins corresponding to DFT coefficients\n",
        "    dft_coeffs = np.fft.fft(signal)  # Compute forward DFT\n",
        "\n",
        "    # Find indices of low-frequency components and set them to zero\n",
        "    # This removes baseline wander by eliminating frequencies below cutoff\n",
        "    dft_coeffs[np.abs(freqs) < cutoff_freq] = 0\n",
        "\n",
        "    # Reconstruct signal using inverse DFT (take real part to avoid complex numbers)\n",
        "    filtered_signal = np.fft.ifft(dft_coeffs).real\n",
        "\n",
        "    return filtered_signal\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üîπ PROCESS SINGLE FILE (FOR PARALLEL PROCESSING)\n",
        "# ===========================================\n",
        "def _process_single_file(args: Tuple[str, str]) -> Tuple[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Helper function for parallel processing of a single file.\n",
        "\n",
        "    Parameters:\n",
        "        args: Tuple of (filename, file_path)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filename, processed_frames)\n",
        "    \"\"\"\n",
        "    filename, file_path = args\n",
        "    try:\n",
        "        # Load the lung sound signal using librosa with specified sampling rate\n",
        "        signal, sr = librosa.load(file_path, sr=Config.SAMPLING_RATE)\n",
        "\n",
        "        # Segment signal into overlapping frames for analysis\n",
        "        frames = segment_signal(signal, sr)\n",
        "\n",
        "        # Apply baseline wander removal to each frame using DFT filtering\n",
        "        processed_frames = np.array([remove_baseline_wander(frame, sr) for frame in frames])\n",
        "\n",
        "        return filename, processed_frames\n",
        "    except Exception as e:\n",
        "        # Log any errors that occur during processing\n",
        "        logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "        return filename, None\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üîπ PREPROCESSING FUNCTION FOR DATASET\n",
        "# ===========================================\n",
        "def preprocess_dataset(\n",
        "    folder_path: str,\n",
        "    parallel: bool = True\n",
        ") -> Tuple[Dict[str, np.ndarray], int]:\n",
        "    \"\"\"\n",
        "    Processes all .wav files in a folder:\n",
        "    - Segments each audio file into frames\n",
        "    - Removes baseline wander using DFT filtering\n",
        "\n",
        "    Parameters:\n",
        "        folder_path: Path to dataset folder containing .wav files\n",
        "        parallel: Whether to use parallel processing (default=True)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (processed_data, sampling_rate) where:\n",
        "        - processed_data: Dictionary with file names as keys and processed frames as values\n",
        "        - sampling_rate: The sampling rate used for processing\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If folder_path doesn't exist\n",
        "    \"\"\"\n",
        "    # Check if the dataset folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"Dataset folder not found: {folder_path}\")\n",
        "\n",
        "    processed_data = {}  # Store preprocessed frames\n",
        "\n",
        "    # Get all .wav files in the dataset folder\n",
        "    wav_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
        "    if not wav_files:\n",
        "        logger.warning(f\"No .wav files found in {folder_path}\")\n",
        "        return processed_data, Config.SAMPLING_RATE\n",
        "\n",
        "    logger.info(f\"üîç Found {len(wav_files)} .wav files. Processing...\")\n",
        "\n",
        "    # Prepare arguments for parallel processing (filename, full_path pairs)\n",
        "    file_args = [(f, os.path.join(folder_path, f)) for f in wav_files]\n",
        "\n",
        "    # Choose processing method based on parallel flag and number of files\n",
        "    if parallel and len(wav_files) > 1:\n",
        "        # Parallel processing using multiprocessing Pool\n",
        "        with Pool(processes=min(Config.N_PROCESSES, len(wav_files))) as pool:\n",
        "            results = list(tqdm(\n",
        "                pool.imap(_process_single_file, file_args),\n",
        "                total=len(wav_files),\n",
        "                desc=\"Processing Files\"\n",
        "            ))\n",
        "    else:\n",
        "        # Sequential processing (useful for debugging or small datasets)\n",
        "        results = []\n",
        "        for args in tqdm(file_args, desc=\"Processing Files\"):\n",
        "            results.append(_process_single_file(args))\n",
        "\n",
        "    # Collect results from processing and store successful ones\n",
        "    for filename, frames in results:\n",
        "        if frames is not None:\n",
        "            processed_data[filename] = frames\n",
        "\n",
        "    logger.info(\"‚úÖ Preprocessing completed!\")\n",
        "    return processed_data, Config.SAMPLING_RATE\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ‚úÖ MAIN EXECUTION\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set the path to your lung sound dataset\n",
        "        dataset_path = r\"/kaggle/input/lung-dataset/Audio Files\"  # Change to your dataset path\n",
        "\n",
        "        # Execute the preprocessing pipeline\n",
        "        preprocessed_data, sampling_rate = preprocess_dataset(dataset_path)\n",
        "\n",
        "        # Display results and statistics if processing was successful\n",
        "        if preprocessed_data:\n",
        "            # Example: Access preprocessed frames of a specific file\n",
        "            example_filename = next(iter(preprocessed_data))\n",
        "            example_frames = preprocessed_data[example_filename]\n",
        "\n",
        "            # Log processing results and frame statistics\n",
        "            logger.info(\n",
        "                f\"üîπ Processed frames for {example_filename}: \"\n",
        "                f\"{example_frames.shape} (n_frames={example_frames.shape[0]}, \"\n",
        "                f\"frame_size={example_frames.shape[1]})\"\n",
        "            )\n",
        "            logger.info(f\"üîπ Sampling rate: {sampling_rate} Hz\")\n",
        "        else:\n",
        "            logger.warning(\"No valid data was processed\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during preprocessing: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0S2KxPRLnO9K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4XY2_CMnPxW"
      },
      "source": [
        "# üéß Lung Sound Augmentation & Balancing Module\n",
        "A complete Python module for audio data augmentation and class balancing, designed specifically for respiratory sound classification tasks. This module includes multiple augmentation strategies like pitch shifting, noise injection, reverb, and more ‚Äî all wrapped in a clean, reusable architecture.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Features Implemented:\n",
        "- üîº Pitch Shift\n",
        "- üå´Ô∏è Gaussian Noise\n",
        "- üì¢ Volume Scaling\n",
        "- ‚úÇÔ∏è Crop & Pad\n",
        "- üì° Sine Interference\n",
        "- üîä Simple Reverb\n",
        "- ‚öñÔ∏è Dataset Balancing by Class\n",
        "\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ITu8TDbLpClW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üéõÔ∏è DATA AUGMENTATION MODULE: NOISE, PITCH, REVERB & BALANCING\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "# ===========================================\n",
        "# üìù LOGGING FUNCTION\n",
        "# ===========================================\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üé® DATA AUGMENTATION FUNCTIONS\n",
        "# ===========================================\n",
        "def pitch_shift(signal: np.ndarray, sr: int, steps: int) -> np.ndarray:\n",
        "    \"\"\"Shift pitch by n_steps semitones.\"\"\"\n",
        "    try:\n",
        "        if steps == 0:\n",
        "            return signal\n",
        "        shifted = librosa.effects.pitch_shift(signal, sr=sr, n_steps=steps)\n",
        "        return librosa.util.fix_length(shifted, size=len(signal))\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è Pitch shift error: {e}\")\n",
        "        return signal\n",
        "\n",
        "def add_gaussian_noise(signal: np.ndarray, noise_level: float) -> np.ndarray:\n",
        "    \"\"\"Add Gaussian noise to the signal.\"\"\"\n",
        "    noise = np.random.normal(0, noise_level, signal.shape)\n",
        "    return signal + noise\n",
        "\n",
        "def random_volume_scaling(signal: np.ndarray, min_gain: float = 0.8, max_gain: float = 1.2) -> np.ndarray:\n",
        "    \"\"\"Scale volume randomly within given gain range.\"\"\"\n",
        "    try:\n",
        "        gain = np.random.uniform(min_gain, max_gain)\n",
        "        scaled = signal * gain\n",
        "        max_val = np.max(np.abs(scaled))\n",
        "        if max_val > 1.0:\n",
        "            scaled = scaled / max_val * 0.95\n",
        "        return scaled.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è Volume scaling error: {e}\")\n",
        "        return signal.copy()\n",
        "\n",
        "def random_crop_and_pad(signal: np.ndarray, crop_ratio: float = 0.9) -> np.ndarray:\n",
        "    \"\"\"Randomly crop and pad the signal to original length.\"\"\"\n",
        "    try:\n",
        "        if len(signal) < 10:\n",
        "            return signal.copy()\n",
        "        crop_len = int(len(signal) * crop_ratio)\n",
        "        start = np.random.randint(0, len(signal) - crop_len + 1)\n",
        "        cropped = signal[start:start + crop_len]\n",
        "        padded = librosa.util.fix_length(cropped, size=len(signal))\n",
        "        return padded.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è Crop-pad error: {e}\")\n",
        "        return signal.copy()\n",
        "\n",
        "def add_sine_interference(signal: np.ndarray, sr: int, freq_range: Tuple[int, int] = (50, 300)) -> np.ndarray:\n",
        "    \"\"\"Overlay a faint sine wave at random frequency.\"\"\"\n",
        "    try:\n",
        "        freq = np.random.uniform(*freq_range)\n",
        "        amplitude = np.random.uniform(0.001, 0.01)\n",
        "        t = np.arange(len(signal)) / sr\n",
        "        sine_wave = amplitude * np.sin(2 * np.pi * freq * t)\n",
        "        return (signal + sine_wave).astype(np.float32)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è Sine interference error: {e}\")\n",
        "        return signal.copy()\n",
        "\n",
        "def add_simple_reverb(signal: np.ndarray, sr: int, room_size: float = 0.3, damping: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Add a simple delay-based reverb effect.\"\"\"\n",
        "    try:\n",
        "        delay_samples = int(sr * room_size * 0.05)\n",
        "        if delay_samples >= len(signal) or delay_samples == 0:\n",
        "            return signal.copy()\n",
        "        delayed = np.zeros_like(signal)\n",
        "        delayed[delay_samples:] = signal[:-delay_samples] * (1 - damping)\n",
        "        reverb_signal = signal + delayed * 0.2\n",
        "        max_val = np.max(np.abs(reverb_signal))\n",
        "        if max_val > 1.0:\n",
        "            reverb_signal = reverb_signal / max_val * 0.95\n",
        "        return reverb_signal.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è Reverb error: {e}\")\n",
        "        return signal.copy()\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üß© AUGMENTOR CLASS\n",
        "# ===========================================\n",
        "class AudioAugmentor:\n",
        "    \"\"\"Combine multiple augmentation transforms.\"\"\"\n",
        "    def __init__(self, sr: int):\n",
        "        self.sr = sr\n",
        "\n",
        "    def augment(self, signal: np.ndarray) -> List[np.ndarray]:\n",
        "        return [\n",
        "            pitch_shift(signal, self.sr, steps=random.randint(-3, 3)),\n",
        "            add_gaussian_noise(signal, noise_level=random.uniform(0.002, 0.01)),\n",
        "            random_volume_scaling(signal, min_gain=0.85, max_gain=1.15),\n",
        "            random_crop_and_pad(signal, crop_ratio=random.uniform(0.85, 0.95)),\n",
        "            add_sine_interference(signal, self.sr, freq_range=(60, 250)),\n",
        "            add_simple_reverb(signal, self.sr, room_size=random.uniform(0.2, 0.4), damping=0.5)\n",
        "        ]\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ‚öñÔ∏è BALANCING & AUGMENTING DATASET\n",
        "# ===========================================\n",
        "def balance_and_augment_dataset(\n",
        "    preprocessed_data: Dict[str, np.ndarray],\n",
        "    sr: int,\n",
        "    target_counts: Dict[str, int],\n",
        "    augment: bool = True\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Balance classes by label and optionally augment to reach target counts.\n",
        "    \"\"\"\n",
        "    class_frame_map: Dict[str, List[np.ndarray]] = {}\n",
        "    result: Dict[str, np.ndarray] = {}\n",
        "    augmentor = AudioAugmentor(sr)\n",
        "\n",
        "    log(f\"üîÑ Balancing and augmenting {len(preprocessed_data)} files...\")\n",
        "\n",
        "    # Group frames by label\n",
        "    for filename, frames in preprocessed_data.items():\n",
        "        label = filename.split(\"_\")[0].lower()\n",
        "        class_frame_map.setdefault(label, []).extend(frames)\n",
        "\n",
        "    # Process each label\n",
        "    for label, frames in class_frame_map.items():\n",
        "        frames = [f for f in frames if isinstance(f, np.ndarray) and f.size > 0]\n",
        "        total = len(frames)\n",
        "        if total == 0:\n",
        "            log(f\"‚ö†Ô∏è No frames for label '{label}', skipping.\")\n",
        "            continue\n",
        "\n",
        "        target = target_counts.get(label, total)\n",
        "        random.shuffle(frames)\n",
        "\n",
        "        # Select base frames\n",
        "        selected = frames[:min(target, total)]\n",
        "        augmented = []\n",
        "\n",
        "        if augment:\n",
        "            for frame in selected:\n",
        "                augmented.extend(augmentor.augment(frame))\n",
        "        else:\n",
        "            augmented = selected.copy()\n",
        "\n",
        "        # Trim or pad augmentation list to match target\n",
        "        if len(augmented) > target:\n",
        "            augmented = augmented[:target]\n",
        "        elif len(augmented) < target:\n",
        "            augmented.extend(random.choices(augmented, k=target - len(augmented)))\n",
        "\n",
        "        result[label] = np.stack(augmented, dtype=np.float32)\n",
        "        log(f\"‚úÖ {label.capitalize()}: {total} ‚Üí {len(augmented)} samples\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# üöÄ USAGE EXAMPLE\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    target_counts = {\n",
        "        \"asthma\": 150,\n",
        "        \"copd\":   135,\n",
        "        \"n\":      150,\n",
        "        # \"pneumonia\": 75\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Ensure `preprocessed_data` and `sampling_rate` are defined beforehand\n",
        "        if 'preprocessed_data' not in globals() or 'sampling_rate' not in globals():\n",
        "            raise NameError(\"Define `preprocessed_data` and `sampling_rate` before running.\")\n",
        "\n",
        "        balanced_augmented_data = balance_and_augment_dataset(\n",
        "            preprocessed_data, sampling_rate, target_counts, augment=True\n",
        "        )\n",
        "\n",
        "        for label, data in balanced_augmented_data.items():\n",
        "            log(f\"üîπ {label} ‚Üí Final shape: {data.shape}\")\n",
        "\n",
        "    except NameError as ne:\n",
        "        log(f\"‚ùå Setup Error: {ne}\")\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Runtime Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75vKSVmVqQoy"
      },
      "source": [
        "# üìà Lung Sound Signal Visualization (Waveform Plots)\n",
        "This module visualizes preprocessed lung sound signals using **Plotly**, showcasing the first frame of each selected audio file. It's useful for:\n",
        "- Verifying segmentation quality\n",
        "- Comparing waveform characteristics across disease labels\n",
        "- Visual inspection before feeding data into ML models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z2uOc_Gzn_sO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import random\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Start of code to define preprocessed_data and sampling_rate ---\n",
        "# This part is necessary to make the visualization code runnable independently\n",
        "# assuming the preprocessing step from the preceding code has run.\n",
        "# If running the preceding code block, this part can be removed.\n",
        "\n",
        "# Placeholder data: Replace with actual data from your preprocessing step\n",
        "# Example structure: {'filename1.wav': array_of_frames_1, 'filename2.wav': array_of_frames_2, ...}\n",
        "preprocessed_data = {}\n",
        "sampling_rate = Config.SAMPLING_RATE # Use the sampling rate from your config\n",
        "\n",
        "# Assuming you have run the preprocessing step and have preprocessed_data defined\n",
        "# Example of loading a few files for visualization if preprocessed_data is not available\n",
        "# You would need to adjust the path and potentially the file names.\n",
        "# This is just a placeholder/example.\n",
        "mypath = \"/kaggle/input/lung-dataset/Audio Files\"\n",
        "handler = AudioFileHandler(mypath)\n",
        "all_filepaths = handler.discover_audio_files(recursive=True, validate_files=True)\n",
        "\n",
        "# Select a few diverse files for visualization\n",
        "selected_files_for_viz = {}\n",
        "labels_to_find = ['copd', 'asthma', 'n', 'other']\n",
        "found_labels = set()\n",
        "\n",
        "for filepath in all_filepaths:\n",
        "    label = handler._extract_label_from_filename(filepath).lower()\n",
        "    if label in labels_to_find and label not in found_labels:\n",
        "        try:\n",
        "            # Load and preprocess the file manually for visualization purposes\n",
        "            signal, sr = librosa.load(filepath, sr=Config.SAMPLING_RATE)\n",
        "            frames = segment_signal(signal, sr)\n",
        "            processed_frames = np.array([remove_baseline_wander(frame, sr) for frame in frames])\n",
        "            if processed_frames.shape[0] > 0:\n",
        "                 preprocessed_data[os.path.basename(filepath)] = processed_frames\n",
        "                 selected_files_for_viz[label] = os.path.basename(filepath) # Store one file per label\n",
        "                 found_labels.add(label)\n",
        "                 if len(found_labels) == len(labels_to_find):\n",
        "                     break # Stop once we have one file for each target label\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process {os.path.basename(filepath)} for visualization: {e}\")\n",
        "\n",
        "# If you don't have one sample per target label, add more files until you have at least 10 total\n",
        "if len(preprocessed_data) < 10:\n",
        "    random.shuffle(all_filepaths)\n",
        "    files_added = len(preprocessed_data)\n",
        "    for filepath in all_filepaths:\n",
        "         if files_added >= 10:\n",
        "             break\n",
        "         filename = os.path.basename(filepath)\n",
        "         if filename not in preprocessed_data:\n",
        "             try:\n",
        "                signal, sr = librosa.load(filepath, sr=Config.SAMPLING_RATE)\n",
        "                frames = segment_signal(signal, sr)\n",
        "                processed_frames = np.array([remove_baseline_wander(frame, sr) for frame in frames])\n",
        "                if processed_frames.shape[0] > 0:\n",
        "                    preprocessed_data[filename] = processed_frames\n",
        "                    files_added += 1\n",
        "             except Exception as e:\n",
        "                 print(f\"Could not process {os.path.basename(filepath)} for visualization: {e}\")\n",
        "\n",
        "# Ensure we have at least one file to visualize\n",
        "if not preprocessed_data:\n",
        "    print(\"Could not load any audio files for visualization.\")\n",
        "# --- End of code to define preprocessed_data and sampling_rate ---\n",
        "\n",
        "\n",
        "# --- Visualization Code ---\n",
        "if preprocessed_data:\n",
        "    # Select up to 10 files for visualization\n",
        "    viz_files = list(preprocessed_data.keys())[:10]\n",
        "    num_plots = len(viz_files)\n",
        "\n",
        "    if num_plots > 0:\n",
        "        # Determine grid size for subplots\n",
        "        rows = (num_plots + 1) // 2 if num_plots > 1 else 1\n",
        "        cols = 2 if num_plots > 1 else 1\n",
        "\n",
        "        fig = make_subplots(rows=rows, cols=cols,\n",
        "                            subplot_titles=[f\"{fname} ({handler._extract_label_from_filename(fname)})\" for fname in viz_files],\n",
        "                            shared_xaxes=True)\n",
        "\n",
        "        for i, filename in enumerate(viz_files):\n",
        "            frames = preprocessed_data[filename]\n",
        "            # Use the first frame for plotting as an example\n",
        "            signal_to_plot = frames[0] if frames.shape[0] > 0 else np.array([])\n",
        "\n",
        "            if signal_to_plot.size > 0:\n",
        "                time = np.linspace(0, len(signal_to_plot) / sampling_rate, len(signal_to_plot))\n",
        "\n",
        "                # Add trace to subplot\n",
        "                row_idx = i // cols + 1\n",
        "                col_idx = i % cols + 1\n",
        "\n",
        "                fig.add_trace(go.Scatter(x=time, y=signal_to_plot, mode='lines', name=filename),\n",
        "                              row=row_idx, col=col_idx)\n",
        "\n",
        "                # Update subplot titles and axis labels\n",
        "                fig.update_xaxes(title_text=\"Time (s)\", row=row_idx, col=col_idx)\n",
        "                fig.update_yaxes(title_text=\"Amplitude\", row=row_idx, col=col_idx)\n",
        "\n",
        "        fig.update_layout(height=rows * 400, width=1000,\n",
        "                          title_text=\"Sample Lung Sound Signals (First Frame)\",\n",
        "                          showlegend=False) # Hide individual trace legends\n",
        "\n",
        "        fig.show()\n",
        "    else:\n",
        "        print(\"No valid audio frames available for visualization.\")\n",
        "else:\n",
        "    print(\"Preprocessing data is empty. Cannot generate plots.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aCsw2RvqpCi3"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import joblib\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KZ149XZxpCgl"
      },
      "outputs": [],
      "source": [
        "!pip install PyWavelets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jMdnMbhqtvu"
      },
      "source": [
        "# üîç Advanced Lung Sound Feature Extraction Pipeline (ASTHMA, COPD, NORMAL)\n",
        "\n",
        "This notebook implements a **comprehensive and robust feature extraction pipeline** for lung sound classification involving **Asthma**, **COPD**, and **Normal** respiratory sounds.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Functionalities\n",
        "\n",
        "### ‚úÖ Augmentation Techniques:\n",
        "- `add_noise()`: Add Gaussian noise to improve model robustness.\n",
        "- `shift()`: Time-shift audio to simulate real-world variance.\n",
        "- `stretch()`: Time-stretching for temporal distortions.\n",
        "- `pitch_shift()`: Modify pitch to increase dataset diversity.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Feature Extraction Modules:\n",
        "\n",
        "| Feature Type                     | Description                                                                             |\n",
        "|----------------------------------|-----------------------------------------------------------------------------------------|\n",
        "| üéµ **Advanced MFCCs**            | 40 MFCCs + Delta + Delta-Delta + Statistical Moments (mean, std, skewness).             |\n",
        "| üìâ **Fourier-Bessel Entropy**    | Energy distribution across frequency bands using spectral entropy.                      |\n",
        "| üìä **Enhanced Mel-Spectrogram** | 128-bin Mel-spectrograms with mean, std, max, min ‚Äî suitable for 2D CNNs.              |\n",
        "| üìà **Wavelet Features**         | Multi-resolution transient detection using `db4` wavelet decomposition.                |\n",
        "| üîÅ **Sequence Features**        | Frame-wise MFCC temporal variation for attention/transformer models.                   |\n",
        "| üéº **Spectral Features**        | Centroid, bandwidth, rolloff, flatness, ZCR, chroma, and tonnetz (auto-handled).       |\n",
        "| üî£ **Fourier-Bessel Coeffs**    | Original signal decomposition based on Fourier-Bessel transforms.                      |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Pipeline Highlights\n",
        "\n",
        "- ‚úÖ Supports `asthma`, `copd`, and `normal` only.\n",
        "- ‚úÖ Error-handling for empty, invalid, or corrupt audio files.\n",
        "- ‚úÖ Auto-parsing of disease label from filename.\n",
        "- ‚úÖ Feature normalization using `StandardScaler`.\n",
        "- ‚úÖ Augmented samples are included for better generalization.\n",
        "- ‚úÖ Informative logging and final summary of class balance and feature breakdown.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Usage Guide\n",
        "\n",
        "To run the pipeline, make sure your `filepaths` list contains the full paths to your `.wav` files.\n",
        "\n",
        "```python\n",
        "X_data, y_data, scaler = run_enhanced_feature_extraction(filepaths)\n",
        "joblib.dump(scaler, 'scaler.pkl')  # Save the scaler for consistent inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BvXeBqs_pCd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from scipy.signal import hilbert\n",
        "import pywt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced function to add noise\n",
        "def add_noise(data, noise_level=0.005):\n",
        "    \"\"\"Add Gaussian noise to audio data with improved stability.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return data\n",
        "    noise = np.random.randn(len(data)) * noise_level\n",
        "    noisy_data = data + noise\n",
        "    # Normalize to prevent clipping\n",
        "    return np.clip(noisy_data, -1.0, 1.0)\n",
        "\n",
        "# Enhanced function to shift the audio\n",
        "def shift(data, shift_max, sampling_rate):\n",
        "    \"\"\"Shift audio with improved parameter handling.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return data\n",
        "    # Ensure shift_max is reasonable relative to data length\n",
        "    max_shift = min(shift_max, len(data) // 4)\n",
        "    shift = np.random.randint(low=-max_shift, high=max_shift)\n",
        "    return np.roll(data, shift)\n",
        "\n",
        "# Enhanced function to stretch the audio\n",
        "def stretch(data, rate=0.8):\n",
        "    \"\"\"Time stretch audio with error handling.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return data\n",
        "        # Ensure rate is within reasonable bounds\n",
        "        rate = np.clip(rate, 0.5, 2.0)\n",
        "        stretched = librosa.effects.time_stretch(data, rate=rate)\n",
        "        return stretched\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Time stretch failed, returning original data: {e}\")\n",
        "        return data\n",
        "\n",
        "# Enhanced function to change pitch\n",
        "def pitch_shift(data, sampling_rate, n_steps=2):\n",
        "    \"\"\"Pitch shift audio with improved error handling.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return data\n",
        "        # Ensure n_steps is within reasonable bounds\n",
        "        n_steps = np.clip(n_steps, -12, 12)\n",
        "        shifted = librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=n_steps)\n",
        "        return shifted\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Pitch shift failed, returning original data: {e}\")\n",
        "        return data\n",
        "\n",
        "# NEW: Advanced MFCC Features with Delta and Delta-Delta\n",
        "def extract_advanced_mfcc_features(data, sampling_rate, n_mfcc=40):\n",
        "    \"\"\"Extract MFCCs with delta and delta-delta features plus statistical moments.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(n_mfcc * 3 + n_mfcc * 3)  # MFCC + Delta + Delta-Delta + Stats\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=data,\n",
        "            sr=sampling_rate,\n",
        "            n_mfcc=n_mfcc,\n",
        "            n_fft=2048,\n",
        "            hop_length=512\n",
        "        )\n",
        "\n",
        "        if mfccs.shape[1] == 0:\n",
        "            return np.zeros(n_mfcc * 3 + n_mfcc * 3)\n",
        "\n",
        "        # Compute Delta (first derivative) and Delta-Delta (second derivative)\n",
        "        delta_mfccs = librosa.feature.delta(mfccs)\n",
        "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "        # Statistical moments across time for each coefficient\n",
        "        mfcc_mean = np.mean(mfccs, axis=1)\n",
        "        mfcc_std = np.std(mfccs, axis=1)\n",
        "        mfcc_skew = stats.skew(mfccs, axis=1)\n",
        "\n",
        "        delta_mean = np.mean(delta_mfccs, axis=1)\n",
        "        delta_std = np.std(delta_mfccs, axis=1)\n",
        "        delta_skew = stats.skew(delta_mfccs, axis=1)\n",
        "\n",
        "        delta2_mean = np.mean(delta2_mfccs, axis=1)\n",
        "        delta2_std = np.std(delta2_mfccs, axis=1)\n",
        "        delta2_skew = stats.skew(delta2_mfccs, axis=1)\n",
        "\n",
        "        # Combine all MFCC-based features\n",
        "        advanced_mfcc_features = np.concatenate([\n",
        "            mfcc_mean, mfcc_std, mfcc_skew,\n",
        "            delta_mean, delta_std, delta_skew,\n",
        "            delta2_mean, delta2_std, delta2_skew\n",
        "        ])\n",
        "\n",
        "        # Handle NaN or infinite values\n",
        "        advanced_mfcc_features = np.nan_to_num(advanced_mfcc_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return advanced_mfcc_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Advanced MFCC extraction failed: {e}\")\n",
        "        return np.zeros(n_mfcc * 9)  # 9 = 3 features * 3 statistics\n",
        "\n",
        "# NEW: Fourier-Bessel Spectral Entropy (FBSE)\n",
        "def extract_fbse_features(data, sampling_rate, n_bands=10):\n",
        "    \"\"\"Extract Fourier-Bessel Spectral Entropy features.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(n_bands)\n",
        "\n",
        "        # Compute power spectral density\n",
        "        freqs, psd = librosa.power_to_db(np.abs(librosa.stft(data))**2), np.abs(librosa.stft(data))**2\n",
        "\n",
        "        # Divide frequency range into bands\n",
        "        freq_bands = np.linspace(0, sampling_rate//2, n_bands + 1)\n",
        "        entropy_features = []\n",
        "\n",
        "        for i in range(n_bands):\n",
        "            # Get frequency band indices\n",
        "            start_idx = int(freq_bands[i] * len(psd) / (sampling_rate//2))\n",
        "            end_idx = int(freq_bands[i+1] * len(psd) / (sampling_rate//2))\n",
        "\n",
        "            if end_idx > start_idx:\n",
        "                band_psd = np.mean(psd[start_idx:end_idx], axis=0)\n",
        "                # Normalize to create probability distribution\n",
        "                band_psd_norm = band_psd / (np.sum(band_psd) + 1e-10)\n",
        "                # Calculate entropy\n",
        "                entropy = -np.sum(band_psd_norm * np.log(band_psd_norm + 1e-10))\n",
        "                entropy_features.append(np.mean(entropy))\n",
        "            else:\n",
        "                entropy_features.append(0.0)\n",
        "\n",
        "        fbse_features = np.array(entropy_features)\n",
        "        fbse_features = np.nan_to_num(fbse_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return fbse_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: FBSE extraction failed: {e}\")\n",
        "        return np.zeros(n_bands)\n",
        "\n",
        "# NEW: Enhanced Mel-Spectrogram with 2D features for CNN\n",
        "def extract_enhanced_melspectrogram(data, sampling_rate, n_mels=128):\n",
        "    \"\"\"Extract enhanced Mel-spectrogram features suitable for 2D CNN processing.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(n_mels * 4)  # Statistical features\n",
        "\n",
        "        # Enhanced mel-spectrogram computation\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=data,\n",
        "            sr=sampling_rate,\n",
        "            n_mels=n_mels,\n",
        "            n_fft=2048,\n",
        "            hop_length=512,\n",
        "            fmax=sampling_rate//2\n",
        "        )\n",
        "\n",
        "        if mel_spec.size == 0:\n",
        "            return np.zeros(n_mels * 4)\n",
        "\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Extract statistical features from 2D mel-spectrogram\n",
        "        if mel_spec_db.shape[1] > 0:\n",
        "            mel_mean = np.mean(mel_spec_db, axis=1)\n",
        "            mel_std = np.std(mel_spec_db, axis=1)\n",
        "            mel_max = np.max(mel_spec_db, axis=1)\n",
        "            mel_min = np.min(mel_spec_db, axis=1)\n",
        "\n",
        "            # Combine statistical features\n",
        "            mel_features = np.concatenate([mel_mean, mel_std, mel_max, mel_min])\n",
        "        else:\n",
        "            mel_features = np.zeros(n_mels * 4)\n",
        "\n",
        "        # Handle NaN or infinite values\n",
        "        mel_features = np.nan_to_num(mel_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return mel_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Enhanced Mel-spectrogram extraction failed: {e}\")\n",
        "        return np.zeros(n_mels * 4)\n",
        "\n",
        "# NEW: Wavelet Features for transient detection\n",
        "def extract_wavelet_features(data, wavelet='db4', levels=5):\n",
        "    \"\"\"Extract wavelet features for transient detection.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(levels * 4)  # 4 stats per level\n",
        "\n",
        "        # Perform wavelet decomposition\n",
        "        coeffs = pywt.wavedec(data, wavelet, level=levels)\n",
        "\n",
        "        wavelet_features = []\n",
        "        for coeff in coeffs:\n",
        "            if len(coeff) > 0:\n",
        "                # Statistical features for each decomposition level\n",
        "                wavelet_features.extend([\n",
        "                    np.mean(np.abs(coeff)),  # Mean absolute value\n",
        "                    np.std(coeff),           # Standard deviation\n",
        "                    np.max(np.abs(coeff)),   # Maximum absolute value\n",
        "                    np.sum(coeff**2)         # Energy\n",
        "                ])\n",
        "            else:\n",
        "                wavelet_features.extend([0.0, 0.0, 0.0, 0.0])\n",
        "\n",
        "        wavelet_features = np.array(wavelet_features)\n",
        "        wavelet_features = np.nan_to_num(wavelet_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return wavelet_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Wavelet feature extraction failed: {e}\")\n",
        "        return np.zeros(levels * 4)\n",
        "\n",
        "# NEW: Sequence-based features for transformer/attention models\n",
        "def extract_sequence_features(data, sampling_rate, frame_length=2048, hop_length=512):\n",
        "    \"\"\"Extract sequence-based features suitable for attention mechanisms.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(26)  # Summary statistics\n",
        "\n",
        "        # Extract frame-wise MFCCs for sequence modeling\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=data,\n",
        "            sr=sampling_rate,\n",
        "            n_mfcc=13,\n",
        "            n_fft=frame_length,\n",
        "            hop_length=hop_length\n",
        "        )\n",
        "\n",
        "        if mfccs.shape[1] == 0:\n",
        "            return np.zeros(26)\n",
        "\n",
        "        # Temporal dynamics features\n",
        "        # 1. Frame-to-frame variation\n",
        "        frame_variations = np.mean(np.abs(np.diff(mfccs, axis=1)), axis=1)\n",
        "\n",
        "        # 2. Long-term average and variation\n",
        "        long_term_mean = np.mean(mfccs, axis=1)\n",
        "\n",
        "        # Combine sequence features\n",
        "        sequence_features = np.concatenate([frame_variations, long_term_mean])\n",
        "        sequence_features = np.nan_to_num(sequence_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return sequence_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Sequence feature extraction failed: {e}\")\n",
        "        return np.zeros(26)\n",
        "\n",
        "# Enhanced Fourier-Bessel Feature Extraction (Original)\n",
        "def fourier_bessel_features(data, sampling_rate, n_coeff):\n",
        "    \"\"\"Enhanced Fourier-Bessel feature extraction with improved numerical stability.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return np.zeros(n_coeff)\n",
        "\n",
        "    t = np.arange(len(data)) / sampling_rate\n",
        "    fb_coeff = np.zeros(n_coeff)\n",
        "\n",
        "    # Normalize time for better numerical stability\n",
        "    t_norm = t / np.max(t) if np.max(t) > 0 else t\n",
        "\n",
        "    for i in range(n_coeff):\n",
        "        j = i + 1\n",
        "        # Enhanced computation with better numerical stability\n",
        "        cosine_term = np.cos(2 * np.pi * j * t_norm)\n",
        "        fb_coeff[i] = np.sum(data * cosine_term) / len(data)\n",
        "\n",
        "    # Handle NaN or infinite values\n",
        "    fb_coeff = np.nan_to_num(fb_coeff, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return fb_coeff\n",
        "# IMPROVED: Spectral Features with better tonnetz handling\n",
        "def extract_spectral_features(data, sampling_rate):\n",
        "    \"\"\"Extract spectral features with improved tonnetz handling for low sampling rates.\"\"\"\n",
        "    try:\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(7)\n",
        "\n",
        "        # Basic spectral features (these work fine with any sampling rate)\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sampling_rate)\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate)\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=sampling_rate)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
        "        zero_crossing_rate = librosa.feature.zero_crossing_rate(data)\n",
        "\n",
        "        # Chroma features (work with any sampling rate)\n",
        "        chroma = librosa.feature.chroma_stft(y=data, sr=sampling_rate)\n",
        "        chroma_mean = np.mean(chroma)\n",
        "\n",
        "        # IMPROVED: Tonnetz with proper frequency limit handling\n",
        "        try:\n",
        "            # Calculate safe frequency range for tonnetz\n",
        "            nyquist_freq = sampling_rate / 2\n",
        "            # Tonnetz typically needs frequencies up to ~4000Hz, but we must respect Nyquist\n",
        "            max_safe_freq = min(4000, nyquist_freq * 0.95)  # Use 95% of Nyquist as safety margin\n",
        "\n",
        "            if sampling_rate >= 8000:  # Safe threshold for full tonnetz\n",
        "                tonnetz = librosa.feature.tonnetz(y=data, sr=sampling_rate)\n",
        "                tonnetz_mean = np.mean(tonnetz)\n",
        "            elif sampling_rate >= 4000:  # Limited tonnetz for medium sampling rates\n",
        "                # Use chromagram-based approach for lower sampling rates\n",
        "                chroma_cqt = librosa.feature.chroma_cqt(\n",
        "                    y=data,\n",
        "                    sr=sampling_rate,\n",
        "                    fmin=librosa.note_to_hz('C1'),\n",
        "                    n_chroma=12\n",
        "                )\n",
        "                # Approximate tonnetz using chroma features\n",
        "                tonnetz_mean = np.mean(chroma_cqt) * 0.5  # Scale factor to approximate tonnetz range\n",
        "            else:  # Very low sampling rates - skip tonnetz\n",
        "                print(f\"Info: Skipping tonnetz for sampling rate {sampling_rate}Hz (too low)\")\n",
        "                tonnetz_mean = 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            # Suppress the specific Nyquist frequency warning since we handle it\n",
        "            if \"Nyquist frequency\" not in str(e):\n",
        "                print(f\"Warning: Tonnetz feature failed: {e}\")\n",
        "            tonnetz_mean = 0.0\n",
        "\n",
        "        spectral_features = np.array([\n",
        "            np.mean(spectral_centroid),\n",
        "            np.mean(spectral_bandwidth),\n",
        "            np.mean(spectral_rolloff),\n",
        "            np.mean(spectral_flatness),\n",
        "            np.mean(zero_crossing_rate),\n",
        "            chroma_mean,\n",
        "            tonnetz_mean\n",
        "        ])\n",
        "\n",
        "        spectral_features = np.nan_to_num(spectral_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return spectral_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Spectral feature extraction failed: {e}\")\n",
        "        return np.zeros(7)\n",
        "\n",
        "\n",
        "# Enhanced Feature Extraction Function with all new features\n",
        "def feature_extraction(dir_):\n",
        "    \"\"\"Enhanced feature extraction with all advanced features for maximum accuracy.\"\"\"\n",
        "    X_Features = []\n",
        "    y_Labels = []\n",
        "    X_Sequences = []  # For transformer/attention models\n",
        "\n",
        "    # Feature dimensions\n",
        "    n_mfcc = 40\n",
        "    fb_coeffs = 20\n",
        "    n_mels = 128\n",
        "    wavelet_levels = 5\n",
        "    fbse_bands = 10\n",
        "\n",
        "    # Statistics tracking\n",
        "    processed_files = 0\n",
        "    skipped_files = 0\n",
        "    augmented_samples = 0\n",
        "\n",
        "    print(\"üöÄ Starting ADVANCED feature extraction for ASTHMA, COPD, and NORMAL classes...\")\n",
        "    print(\"üìä Features being extracted:\")\n",
        "    print(\"   ‚Ä¢ Advanced MFCCs with Delta & Delta-Delta + Statistical Moments\")\n",
        "    print(\"   ‚Ä¢ Fourier-Bessel Spectral Entropy (FBSE)\")\n",
        "    print(\"   ‚Ä¢ Enhanced Mel-Spectrograms (128 bins) for 2D CNN\")\n",
        "    print(\"   ‚Ä¢ Wavelet Features for Transient Detection\")\n",
        "    print(\"   ‚Ä¢ Sequence Features for Attention Mechanisms\")\n",
        "    print(\"   ‚Ä¢ Comprehensive Spectral Features (with fixed tonnetz)\")\n",
        "    print(\"   ‚Ä¢ Original Fourier-Bessel Coefficients\")\n",
        "    print(\"   ‚Ä¢ Target Classes: ASTHMA, COPD, NORMAL\")\n",
        "\n",
        "    for soundDir in dir_:\n",
        "        try:\n",
        "            # ENHANCED disease name extraction from filename with lung fibrosis support\n",
        "            try:\n",
        "                filename = soundDir.split('/')[-1] if '/' in soundDir else soundDir.split('\\\\')[-1]\n",
        "                parts = filename.split('_')\n",
        "                if len(parts) < 2:\n",
        "                    print(f\"‚ö†Ô∏è  Invalid filename format: {filename}\")\n",
        "                    skipped_files += 1\n",
        "                    continue\n",
        "\n",
        "                disease_part = parts[1].split(',')[0].lower().strip()\n",
        "\n",
        "                # Disease mapping for ASTHMA, COPD, and NORMAL only\n",
        "                disease_mapping = {\n",
        "                    'asthma': 'asthma',\n",
        "                    'copd': 'copd',\n",
        "                    'n': 'normal',\n",
        "                    'c': 'normal',\n",
        "                    'normal': 'normal'\n",
        "                }\n",
        "\n",
        "                # More flexible disease detection (ASTHMA, COPD, NORMAL only)\n",
        "                if disease_part in disease_mapping:\n",
        "                    disease = disease_mapping[disease_part]\n",
        "                elif 'asthma' in disease_part:\n",
        "                    disease = 'asthma'\n",
        "                elif 'copd' in disease_part:\n",
        "                    disease = 'copd'\n",
        "                elif disease_part in ['n', 'normal', 'c']:\n",
        "                    disease = 'normal'\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Skipping unsupported disease label '{disease_part}' in: {filename}\")\n",
        "                    print(f\"    Only processing: asthma, copd, normal\")\n",
        "                    skipped_files += 1\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Failed to parse filename {soundDir}: {e}\")\n",
        "                skipped_files += 1\n",
        "                continue\n",
        "\n",
        "            # Disease validation for ASTHMA, COPD, and NORMAL only\n",
        "            valid_diseases = [\"asthma\", \"copd\", \"normal\"]\n",
        "            if disease not in valid_diseases:\n",
        "                print(f\"‚ö†Ô∏è  Skipping invalid disease label '{disease}' in: {soundDir}\")\n",
        "                print(f\"    Only processing: {valid_diseases}\")\n",
        "                skipped_files += 1\n",
        "                continue\n",
        "\n",
        "            # Enhanced audio loading with error handling\n",
        "            try:\n",
        "                data, sampling_rate = librosa.load(soundDir, sr=None)\n",
        "                if len(data) == 0:\n",
        "                    print(f\"‚ö†Ô∏è  Empty audio file: {soundDir}\")\n",
        "                    skipped_files += 1\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to load audio file {soundDir}: {e}\")\n",
        "                skipped_files += 1\n",
        "                continue\n",
        "\n",
        "            # EXTRACT ALL ADVANCED FEATURES\n",
        "            print(f\"üîÑ Processing: {filename[:50]}... (SR: {sampling_rate}Hz)\")\n",
        "\n",
        "            # 1. Advanced MFCCs with Delta and Delta-Delta\n",
        "            advanced_mfcc_features = extract_advanced_mfcc_features(data, sampling_rate, n_mfcc)\n",
        "\n",
        "            # 2. Fourier-Bessel Spectral Entropy (FBSE)\n",
        "            fbse_features = extract_fbse_features(data, sampling_rate, fbse_bands)\n",
        "\n",
        "            # 3. Enhanced Mel-Spectrogram Features\n",
        "            enhanced_mel_features = extract_enhanced_melspectrogram(data, sampling_rate, n_mels)\n",
        "\n",
        "            # 4. Wavelet Features\n",
        "            wavelet_features = extract_wavelet_features(data, 'db4', wavelet_levels)\n",
        "\n",
        "            # 5. Sequence Features\n",
        "            sequence_features = extract_sequence_features(data, sampling_rate)\n",
        "\n",
        "            # 6. FIXED: Spectral Features (with proper tonnetz handling)\n",
        "            spectral_features = extract_spectral_features(data, sampling_rate)\n",
        "\n",
        "            # 7. Original Fourier-Bessel Features\n",
        "            fb_features = fourier_bessel_features(data, sampling_rate, fb_coeffs)\n",
        "\n",
        "            # COMBINE ALL FEATURES\n",
        "            combined_features = np.concatenate([\n",
        "                advanced_mfcc_features,  # Advanced MFCCs with deltas + stats\n",
        "                fbse_features,           # Fourier-Bessel Spectral Entropy\n",
        "                enhanced_mel_features,   # Enhanced Mel-Spectrogram\n",
        "                wavelet_features,        # Wavelet features\n",
        "                sequence_features,       # Sequence features\n",
        "                spectral_features,       # Spectral features (fixed)\n",
        "                fb_features             # Original FB features\n",
        "            ])\n",
        "\n",
        "            # Final validation\n",
        "            combined_features = np.nan_to_num(combined_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            # Append Original Features\n",
        "            X_Features.append(combined_features)\n",
        "            y_Labels.append(disease)\n",
        "\n",
        "            # **ENHANCED AUGMENTATION with all feature types**\n",
        "            augmentations = [\n",
        "                (add_noise, 0.002),\n",
        "                (shift, 1600),\n",
        "                (stretch, 0.9),\n",
        "                (pitch_shift, 2)\n",
        "            ]\n",
        "\n",
        "            aug_count = 0\n",
        "            for aug_func, aug_param in augmentations:\n",
        "                try:\n",
        "                    # Enhanced augmentation with proper parameter passing\n",
        "                    if aug_func == shift:\n",
        "                        data_aug = aug_func(data, aug_param, sampling_rate)\n",
        "                    elif aug_func == pitch_shift:\n",
        "                        data_aug = aug_func(data, sampling_rate, aug_param)\n",
        "                    else:\n",
        "                        data_aug = aug_func(data, aug_param)\n",
        "\n",
        "                    # Validate augmented data\n",
        "                    if len(data_aug) == 0 or np.all(data_aug == 0):\n",
        "                        continue\n",
        "\n",
        "                    # Extract ALL features for augmented data\n",
        "                    advanced_mfcc_aug = extract_advanced_mfcc_features(data_aug, sampling_rate, n_mfcc)\n",
        "                    fbse_aug = extract_fbse_features(data_aug, sampling_rate, fbse_bands)\n",
        "                    enhanced_mel_aug = extract_enhanced_melspectrogram(data_aug, sampling_rate, n_mels)\n",
        "                    wavelet_aug = extract_wavelet_features(data_aug, 'db4', wavelet_levels)\n",
        "                    sequence_aug = extract_sequence_features(data_aug, sampling_rate)\n",
        "                    spectral_aug = extract_spectral_features(data_aug, sampling_rate)  # Fixed version\n",
        "                    fb_aug = fourier_bessel_features(data_aug, sampling_rate, fb_coeffs)\n",
        "\n",
        "                    # Combine all augmented features\n",
        "                    combined_features_aug = np.concatenate([\n",
        "                        advanced_mfcc_aug, fbse_aug, enhanced_mel_aug,\n",
        "                        wavelet_aug, sequence_aug, spectral_aug, fb_aug\n",
        "                    ])\n",
        "\n",
        "                    combined_features_aug = np.nan_to_num(combined_features_aug, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                    # Append Augmented Data\n",
        "                    X_Features.append(combined_features_aug)\n",
        "                    y_Labels.append(disease)\n",
        "                    aug_count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Augmentation {aug_func.__name__} failed: {e}\")\n",
        "                    continue\n",
        "\n",
        "            processed_files += 1\n",
        "            augmented_samples += aug_count\n",
        "\n",
        "            if processed_files % 25 == 0:  # Progress update every 25 files\n",
        "                print(f\"‚úÖ Processed {processed_files} files so far...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {soundDir}: {e}\")\n",
        "            skipped_files += 1\n",
        "            continue\n",
        "\n",
        "    # Enhanced final validation and conversion\n",
        "    if len(X_Features) == 0:\n",
        "        print(\"‚ùå No features extracted!\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    X_data = np.array(X_Features)\n",
        "    y_data = np.array(y_Labels)\n",
        "\n",
        "    # Feature normalization for better model performance\n",
        "    scaler = StandardScaler()\n",
        "    X_data_normalized = scaler.fit_transform(X_data)\n",
        "\n",
        "    # Final statistics and validation\n",
        "    print(f\"\\nüéØ ADVANCED Feature Extraction Summary (ASTHMA, COPD, NORMAL):\")\n",
        "    print(f\"   ‚Ä¢ Successfully processed files: {processed_files}\")\n",
        "    print(f\"   ‚Ä¢ Skipped files: {skipped_files}\")\n",
        "    print(f\"   ‚Ä¢ Original samples: {processed_files}\")\n",
        "    print(f\"   ‚Ä¢ Augmented samples: {augmented_samples}\")\n",
        "    print(f\"   ‚Ä¢ Total samples: {len(X_Features)}\")\n",
        "    print(f\"   ‚Ä¢ Feature dimensionality: {X_data.shape[1]}\")\n",
        "    print(f\"   ‚Ä¢ Features normalized: ‚úÖ\")\n",
        "    print(f\"   ‚Ä¢ Classes: ASTHMA, COPD, NORMAL\")\n",
        "\n",
        "    # Detailed feature breakdown\n",
        "    feature_breakdown = {\n",
        "        'Advanced MFCCs (with deltas & stats)': n_mfcc * 9,\n",
        "        'FBSE Features': fbse_bands,\n",
        "        'Enhanced Mel-Spectrogram': n_mels * 4,\n",
        "        'Wavelet Features': wavelet_levels * 4 + 4,  # +4 for approximation coeffs\n",
        "        'Sequence Features': 26,\n",
        "        'Spectral Features (Fixed)': 7,\n",
        "        'Original FB Features': fb_coeffs\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Feature Type Breakdown:\")\n",
        "    total_expected = 0\n",
        "    for feature_type, count in feature_breakdown.items():\n",
        "        print(f\"   ‚Ä¢ {feature_type}: {count} features\")\n",
        "        total_expected += count\n",
        "    print(f\"   ‚Ä¢ Total Expected: {total_expected}\")\n",
        "    print(f\"   ‚Ä¢ Actual Total: {X_data.shape[1]}\")\n",
        "\n",
        "    # Class distribution\n",
        "    unique_labels, counts = np.unique(y_data, return_counts=True)\n",
        "    print(f\"\\nüè∑Ô∏è  Class Distribution:\")\n",
        "    for label, count in zip(unique_labels, counts):\n",
        "        percentage = (count / len(y_data)) * 100\n",
        "        print(f\"   ‚Ä¢ {label}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "    # Feature quality validation\n",
        "    nan_count = np.sum(np.isnan(X_data))\n",
        "    inf_count = np.sum(np.isinf(X_data))\n",
        "    if nan_count > 0 or inf_count > 0:\n",
        "        print(f\"‚ö†Ô∏è  Found {nan_count} NaN and {inf_count} infinite values (cleaned)\")\n",
        "\n",
        "    print(f\"\\nüéâ ADVANCED feature extraction completed successfully!\")\n",
        "    print(f\"‚úÖ Final Feature Matrix Shape: {X_data_normalized.shape}\")\n",
        "    print(f\"‚úÖ Final Label Vector Shape: {y_data.shape}\")\n",
        "    print(f\"üéØ Ready for high-accuracy model training!\")\n",
        "\n",
        "    return X_data_normalized, y_data, scaler\n",
        "\n",
        "# Usage example with error handling\n",
        "def run_enhanced_feature_extraction(filepaths):\n",
        "    \"\"\"Run the enhanced feature extraction with comprehensive error handling.\"\"\"\n",
        "    try:\n",
        "        X_data, y_data, scaler = feature_extraction(filepaths)\n",
        "\n",
        "        if len(X_data) == 0:\n",
        "            print(\"‚ùå No features were extracted. Please check your file paths and audio files.\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"\\nüî• FEATURE EXTRACTION COMPLETE!\")\n",
        "        print(f\"üí™ Enhanced features ready for machine learning models:\")\n",
        "        print(f\"   ‚Ä¢ Traditional ML: Use X_data directly\")\n",
        "        print(f\"   ‚Ä¢ Deep Learning: Consider reshaping for CNN/RNN architectures\")\n",
        "        print(f\"   ‚Ä¢ Transformer Models: Use sequence-based features\")\n",
        "\n",
        "        return X_data, y_data, scaler\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"‚ùå Missing required variables: {e}\")\n",
        "        print(\"Please ensure 'filepaths' variable is defined with your audio file paths.\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error during feature extraction: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Example usage (uncomment when you have filepaths defined):\n",
        "X_data, y_data, scaler = run_enhanced_feature_extraction(filepaths)\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZyDa7Fhim18B"
      },
      "outputs": [],
      "source": [
        "DISEASE_COLORS = {\n",
        "    'normal': '#2E8B57',     # Sea Green\n",
        "    'asthma': '#FF6B6B',     # Coral Red\n",
        "    'copd': '#4ECDC4',       # Turquoise\n",
        "    'pneumonia': '#FFE66D',  # Golden Yellow\n",
        "    'bronchitis': '#A8E6CF'  # Light Green\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SaHKRdUEv7w1"
      },
      "outputs": [],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üìä ENHANCED LUNG DISEASE FEATURE VISUALIZATION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# Advanced visualization system for lung sound feature analysis across different diseases\n",
        "# Fixed subplot compatibility issues and enhanced three-class comparison\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Disease-specific color palette for consistent visualization\n",
        "DISEASE_COLORS = {\n",
        "    'normal': '#2E8B57',     # Sea Green\n",
        "    'asthma': '#FF6B6B',     # Coral Red\n",
        "    'copd': '#4ECDC4',       # Turquoise\n",
        "    'pneumonia': '#FFE66D',  # Golden Yellow\n",
        "    'bronchitis': '#A8E6CF'  # Light Green\n",
        "}\n",
        "\n",
        "# Feature-specific visualization styles\n",
        "FEATURE_STYLES = {\n",
        "    'Advanced MFCCs': {'type': 'heatmap', 'colorscale': 'Viridis'},\n",
        "    'FBSE': {'type': 'radar', 'colorscale': 'Plasma'},\n",
        "    'Spectral Features': {'type': 'bar', 'colorscale': 'Cividis'},\n",
        "    'Wavelet Features': {'type': 'line', 'colorscale': 'Turbo'},\n",
        "    'Sequence Features': {'type': 'scatter', 'colorscale': 'Inferno'}\n",
        "}\n",
        "\n",
        "def create_enhanced_feature_visualization(preprocessed_data, sampling_rate, handler):\n",
        "    \"\"\"\n",
        "    üé® Creates comprehensive multi-graph visualization of lung disease features\n",
        "\n",
        "    Parameters:\n",
        "        preprocessed_data: Dictionary of preprocessed audio frames\n",
        "        sampling_rate: Audio sampling rate\n",
        "        handler: Feature extraction handler object\n",
        "    \"\"\"\n",
        "\n",
        "    if not preprocessed_data:\n",
        "        print(\"‚ùå Preprocessing data is not available. Cannot generate feature plots.\")\n",
        "        return\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # üìã DISEASE SAMPLE SELECTION (Focus on 3 classes)\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    diseases_to_plot = ['healthy', 'asthma', 'copd']\n",
        "    sample_files = {}\n",
        "\n",
        "    # Select representative samples for each disease\n",
        "    for filename, frames in preprocessed_data.items():\n",
        "        if frames.shape[0] > 0:\n",
        "            label = handler._extract_label_from_filename(filename).lower()\n",
        "            if label in diseases_to_plot and label not in sample_files:\n",
        "                sample_files[label] = filename\n",
        "            if len(sample_files) == len(diseases_to_plot):\n",
        "                break\n",
        "\n",
        "    if not sample_files:\n",
        "        print(\"‚ùå Could not find sample files for the specified diseases.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìã Found samples for diseases: {list(sample_files.keys())}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # üîß FEATURE EXTRACTION CONFIGURATION\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    features_config = [\n",
        "        ('Advanced MFCCs', extract_advanced_mfcc_features, 360, 'heatmap'),\n",
        "        ('FBSE', extract_fbse_features, 10, 'line'),\n",
        "        ('Spectral Features', extract_spectral_features, 7, 'bar'),\n",
        "        ('Wavelet Features', extract_wavelet_features, 24, 'line'),\n",
        "        ('Sequence Features', extract_sequence_features, 26, 'scatter')\n",
        "    ]\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # üé® CREATE MASTER VISUALIZATION DASHBOARD\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "    # Main comparison dashboard\n",
        "    create_feature_comparison_dashboard(sample_files, preprocessed_data, sampling_rate, features_config)\n",
        "\n",
        "    # Individual detailed visualizations for each feature type\n",
        "    for feature_name, feature_func, expected_dim, viz_type in features_config:\n",
        "        create_individual_feature_visualization(\n",
        "            sample_files, preprocessed_data, sampling_rate,\n",
        "            feature_name, feature_func, viz_type\n",
        "        )\n",
        "\n",
        "    # Statistical comparison charts\n",
        "    create_statistical_comparison_charts(sample_files, preprocessed_data, sampling_rate, features_config)\n",
        "\n",
        "    # Create comprehensive three-class comparison\n",
        "    create_three_class_comparison(sample_files, preprocessed_data, sampling_rate, features_config)\n",
        "\n",
        "\n",
        "def create_feature_comparison_dashboard(sample_files, preprocessed_data, sampling_rate, features_config):\n",
        "    \"\"\"üèÜ Creates comprehensive feature comparison dashboard (FIXED VERSION)\"\"\"\n",
        "\n",
        "    num_diseases = len(sample_files)\n",
        "    num_features = len(features_config)\n",
        "\n",
        "    # Create subplot layout with compatible types\n",
        "    fig = make_subplots(\n",
        "        rows=num_diseases,\n",
        "        cols=num_features,\n",
        "        subplot_titles=[f\"{disease.upper()} - {feature_name}\"\n",
        "                       for disease in sample_files.keys()\n",
        "                       for feature_name, _, _, _ in features_config],\n",
        "        horizontal_spacing=0.08,\n",
        "        vertical_spacing=0.15\n",
        "    )\n",
        "\n",
        "    for i, (disease, filename) in enumerate(sample_files.items()):\n",
        "        frames = preprocessed_data[filename]\n",
        "        if frames.shape[0] > 0:\n",
        "            signal_to_analyze = frames[0]  # Use first frame\n",
        "            disease_color = DISEASE_COLORS.get(disease, '#636EFA')\n",
        "\n",
        "            for j, (feature_name, feature_func, expected_dim, viz_type) in enumerate(features_config):\n",
        "                try:\n",
        "                    # Extract features based on function requirements\n",
        "                    if feature_func == extract_wavelet_features:\n",
        "                        feature_vector = feature_func(signal_to_analyze)\n",
        "                    else:\n",
        "                        feature_vector = feature_func(signal_to_analyze, sampling_rate)\n",
        "\n",
        "                    if feature_vector.size > 0:\n",
        "                        plot_data = prepare_plot_data(feature_vector)\n",
        "\n",
        "                        # Create compatible visualizations for xy subplots\n",
        "                        if 'mfcc' in feature_name.lower():\n",
        "                            # Bar chart for MFCCs (compatible with xy subplot)\n",
        "                            add_enhanced_bar_trace(fig, plot_data[:20], disease_color, f'{disease} MFCC', i+1, j+1)\n",
        "\n",
        "                        elif 'spectral' in feature_name.lower():\n",
        "                            # Regular bar chart for spectral features (compatible)\n",
        "                            add_enhanced_bar_trace(fig, plot_data, disease_color, f'{disease} Spectral', i+1, j+1)\n",
        "\n",
        "                        elif 'wavelet' in feature_name.lower():\n",
        "                            # Line plot for wavelet features (compatible)\n",
        "                            add_enhanced_line_trace(fig, plot_data, disease_color, f'{disease} Wavelet', i+1, j+1)\n",
        "\n",
        "                        elif 'fbse' in feature_name.lower():\n",
        "                            # Line plot for FBSE features\n",
        "                            add_enhanced_line_trace(fig, plot_data, disease_color, f'{disease} FBSE', i+1, j+1)\n",
        "\n",
        "                        else:\n",
        "                            # Scatter plot for sequence features\n",
        "                            add_enhanced_scatter_trace(fig, plot_data, disease_color, f'{disease} {feature_name}', i+1, j+1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error processing {feature_name} for {disease}: {e}\")\n",
        "                    # Add error placeholder\n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=[0], y=[0],\n",
        "                            mode='text',\n",
        "                            text=[f\"Error processing<br>{feature_name}\"],\n",
        "                            textfont=dict(color='red', size=10),\n",
        "                            showlegend=False\n",
        "                        ),\n",
        "                        row=i+1, col=j+1\n",
        "                    )\n",
        "\n",
        "    # Enhanced layout styling\n",
        "    fig.update_layout(\n",
        "        height=num_diseases * 350,\n",
        "        width=num_features * 280,\n",
        "        title={\n",
        "            'text': \"ü´Å Lung Disease Feature Analysis: Normal vs COPD vs Asthma\",\n",
        "            'x': 0.5,\n",
        "            'xanchor': 'center',\n",
        "            'font': {'size': 22, 'color': '#2C3E50'}\n",
        "        },\n",
        "        showlegend=False,\n",
        "        plot_bgcolor='rgba(248, 249, 250, 0.8)',\n",
        "        paper_bgcolor='white',\n",
        "        font=dict(family=\"Arial, sans-serif\", size=10)\n",
        "    )\n",
        "\n",
        "    # Update subplot axes\n",
        "    for i in range(1, num_diseases + 1):\n",
        "        for j in range(1, num_features + 1):\n",
        "            fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray', row=i, col=j)\n",
        "            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray', row=i, col=j)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def create_individual_feature_visualization(sample_files, preprocessed_data, sampling_rate, feature_name, feature_func, viz_type):\n",
        "    \"\"\"üéØ Creates detailed individual feature visualization for three classes\"\"\"\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Extract features for all diseases\n",
        "    all_features = {}\n",
        "    for disease, filename in sample_files.items():\n",
        "        frames = preprocessed_data[filename]\n",
        "        if frames.shape[0] > 0:\n",
        "            signal_to_analyze = frames[0]\n",
        "\n",
        "            try:\n",
        "                if feature_func == extract_wavelet_features:\n",
        "                    feature_vector = feature_func(signal_to_analyze)\n",
        "                else:\n",
        "                    feature_vector = feature_func(signal_to_analyze, sampling_rate)\n",
        "\n",
        "                all_features[disease] = prepare_plot_data(feature_vector)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error extracting {feature_name} for {disease}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Create comparative visualization\n",
        "    if all_features:\n",
        "        for disease, features in all_features.items():\n",
        "            disease_color = DISEASE_COLORS.get(disease, '#636EFA')\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                y=features,\n",
        "                mode='lines+markers',\n",
        "                name=f'{disease.capitalize()}',\n",
        "                line=dict(color=disease_color, width=3),\n",
        "                marker=dict(size=6, opacity=0.8),\n",
        "                hovertemplate=f'<b>{disease.capitalize()}</b><br>Index: %{{x}}<br>Value: %{{y:.3f}}<extra></extra>'\n",
        "            ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"üìà {feature_name} - Three-Class Comparison\",\n",
        "        xaxis_title=\"Feature Index\",\n",
        "        yaxis_title=\"Feature Value\",\n",
        "        height=500,\n",
        "        width=800,\n",
        "        plot_bgcolor='rgba(248, 249, 250, 0.8)',\n",
        "        paper_bgcolor='white',\n",
        "        font=dict(family=\"Arial, sans-serif\", size=12),\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def create_statistical_comparison_charts(sample_files, preprocessed_data, sampling_rate, features_config):\n",
        "    \"\"\"üìä Creates statistical comparison charts for three classes\"\"\"\n",
        "\n",
        "    # Feature statistics comparison\n",
        "    stats_fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=['Feature Magnitudes Comparison', 'Feature Distributions', 'Disease Separation', 'Feature Statistics'],\n",
        "        specs=[[{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
        "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
        "    )\n",
        "\n",
        "    # Collect all feature statistics\n",
        "    disease_stats = {}\n",
        "    feature_means = {}\n",
        "\n",
        "    for disease, filename in sample_files.items():\n",
        "        frames = preprocessed_data[filename]\n",
        "        if frames.shape[0] > 0:\n",
        "            signal_to_analyze = frames[0]\n",
        "            disease_features = []\n",
        "\n",
        "            for feature_name, feature_func, _, _ in features_config:\n",
        "                try:\n",
        "                    if feature_func == extract_wavelet_features:\n",
        "                        feature_vector = feature_func(signal_to_analyze)\n",
        "                    else:\n",
        "                        feature_vector = feature_func(signal_to_analyze, sampling_rate)\n",
        "\n",
        "                    processed_features = prepare_plot_data(feature_vector)[:15]  # Take first 15 features\n",
        "                    disease_features.extend(processed_features)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            disease_stats[disease] = np.array(disease_features) if disease_features else np.array([0])\n",
        "            feature_means[disease] = np.mean(disease_features) if disease_features else 0\n",
        "\n",
        "    # Add statistical visualizations\n",
        "    if disease_stats:\n",
        "        diseases = list(disease_stats.keys())\n",
        "\n",
        "        # 1. Feature magnitude comparison (first 10 features)\n",
        "        for i, (disease, features) in enumerate(disease_stats.items()):\n",
        "            stats_fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=[f'F{j+1}' for j in range(min(10, len(features)))],\n",
        "                    y=features[:10],\n",
        "                    name=f'{disease.capitalize()}',\n",
        "                    marker_color=DISEASE_COLORS.get(disease, '#636EFA'),\n",
        "                    opacity=0.8,\n",
        "                    offsetgroup=i\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "        # 2. Feature distributions (box plot)\n",
        "        for disease, features in disease_stats.items():\n",
        "            stats_fig.add_trace(\n",
        "                go.Box(\n",
        "                    y=features,\n",
        "                    name=f'{disease.capitalize()}',\n",
        "                    marker_color=DISEASE_COLORS.get(disease, '#636EFA'),\n",
        "                    boxpoints='outliers'\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "\n",
        "        # 3. Disease separation visualization\n",
        "        x_coords = [np.mean(disease_stats[d]) for d in diseases]\n",
        "        y_coords = [np.std(disease_stats[d]) for d in diseases]\n",
        "\n",
        "        stats_fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=x_coords,\n",
        "                y=y_coords,\n",
        "                mode='markers+text',\n",
        "                text=[d.capitalize() for d in diseases],\n",
        "                textposition='top center',\n",
        "                marker=dict(\n",
        "                    size=25,\n",
        "                    color=[DISEASE_COLORS.get(d, '#636EFA') for d in diseases],\n",
        "                    opacity=0.8,\n",
        "                    line=dict(width=3, color='white')\n",
        "                ),\n",
        "                name='Disease Clusters',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Summary statistics\n",
        "        means = [np.mean(disease_stats[d]) for d in diseases]\n",
        "        stds = [np.std(disease_stats[d]) for d in diseases]\n",
        "\n",
        "        stats_fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=[f'{d.capitalize()} Mean' for d in diseases],\n",
        "                y=means,\n",
        "                name='Mean Values',\n",
        "                marker_color=[DISEASE_COLORS.get(d, '#636EFA') for d in diseases],\n",
        "                opacity=0.8\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "    stats_fig.update_layout(\n",
        "        height=800,\n",
        "        width=1200,\n",
        "        title={\n",
        "            'text': \"üìà Statistical Analysis: Normal vs COPD vs Asthma\",\n",
        "            'x': 0.5,\n",
        "            'xanchor': 'center',\n",
        "            'font': {'size': 20, 'color': '#2C3E50'}\n",
        "        },\n",
        "        plot_bgcolor='rgba(248, 249, 250, 0.8)',\n",
        "        paper_bgcolor='white',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Update subplot labels\n",
        "    stats_fig.update_xaxes(title_text=\"Features\", row=1, col=1)\n",
        "    stats_fig.update_yaxes(title_text=\"Magnitude\", row=1, col=1)\n",
        "    stats_fig.update_yaxes(title_text=\"Feature Values\", row=1, col=2)\n",
        "    stats_fig.update_xaxes(title_text=\"Mean Feature Value\", row=2, col=1)\n",
        "    stats_fig.update_yaxes(title_text=\"Standard Deviation\", row=2, col=1)\n",
        "    stats_fig.update_yaxes(title_text=\"Mean Value\", row=2, col=2)\n",
        "\n",
        "    stats_fig.show()\n",
        "\n",
        "\n",
        "def create_three_class_comparison(sample_files, preprocessed_data, sampling_rate, features_config):\n",
        "    \"\"\"üéØ Creates comprehensive three-class comparison visualization\"\"\"\n",
        "\n",
        "    # Create a comprehensive comparison figure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=3,\n",
        "        subplot_titles=['Feature Radar Comparison', 'Distribution Comparison', 'Feature Correlation',\n",
        "                       'Classification Boundaries', 'Feature Importance', 'Summary Statistics'],\n",
        "        specs=[[{\"type\": \"scatterpolar\"}, {\"type\": \"violin\"}, {\"type\": \"heatmap\"}],\n",
        "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"table\"}]]\n",
        "    )\n",
        "\n",
        "    # Extract comprehensive features for all diseases\n",
        "    all_disease_features = {}\n",
        "    feature_names = []\n",
        "\n",
        "    for disease, filename in sample_files.items():\n",
        "        frames = preprocessed_data[filename]\n",
        "        if frames.shape[0] > 0:\n",
        "            signal_to_analyze = frames[0]\n",
        "            combined_features = []\n",
        "\n",
        "            for i, (feature_name, feature_func, _, _) in enumerate(features_config):\n",
        "                try:\n",
        "                    if feature_func == extract_wavelet_features:\n",
        "                        feature_vector = feature_func(signal_to_analyze)\n",
        "                    else:\n",
        "                        feature_vector = feature_func(signal_to_analyze, sampling_rate)\n",
        "\n",
        "                    processed_features = prepare_plot_data(feature_vector)\n",
        "                    # Take representative features from each type\n",
        "                    if len(processed_features) > 0:\n",
        "                        combined_features.append(np.mean(processed_features))  # Mean of each feature type\n",
        "                        if disease == list(sample_files.keys())[0]:  # Only add names once\n",
        "                            feature_names.append(feature_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error in comprehensive analysis for {feature_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_disease_features[disease] = np.array(combined_features)\n",
        "\n",
        "    if all_disease_features and len(feature_names) > 0:\n",
        "        # 1. Radar chart comparison\n",
        "        for disease, features in all_disease_features.items():\n",
        "            fig.add_trace(go.Scatterpolar(\n",
        "                r=features,\n",
        "                theta=feature_names,\n",
        "                fill='toself',\n",
        "                name=f'{disease.capitalize()}',\n",
        "                line_color=DISEASE_COLORS.get(disease, '#636EFA'),\n",
        "                opacity=0.6\n",
        "            ), row=1, col=1)\n",
        "\n",
        "        # 2. Distribution comparison\n",
        "        for disease, features in all_disease_features.items():\n",
        "            fig.add_trace(go.Violin(\n",
        "                y=features,\n",
        "                name=f'{disease.capitalize()}',\n",
        "                box_visible=True,\n",
        "                meanline_visible=True,\n",
        "                fillcolor=DISEASE_COLORS.get(disease, '#636EFA'),\n",
        "                opacity=0.6\n",
        "            ), row=1, col=2)\n",
        "\n",
        "        # 3. Feature correlation matrix\n",
        "        diseases = list(all_disease_features.keys())\n",
        "        if len(diseases) >= 2:\n",
        "            feature_matrix = np.array([all_disease_features[d] for d in diseases])\n",
        "            correlation_matrix = np.corrcoef(feature_matrix)\n",
        "\n",
        "            fig.add_trace(go.Heatmap(\n",
        "                z=correlation_matrix,\n",
        "                x=[d.capitalize() for d in diseases],\n",
        "                y=[d.capitalize() for d in diseases],\n",
        "                colorscale='RdBu',\n",
        "                zmid=0,\n",
        "                showscale=True\n",
        "            ), row=1, col=3)\n",
        "\n",
        "        # 4. Classification boundaries (PCA-like)\n",
        "        means = [np.mean(all_disease_features[d]) for d in diseases]\n",
        "        stds = [np.std(all_disease_features[d]) for d in diseases]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=means,\n",
        "            y=stds,\n",
        "            mode='markers+text',\n",
        "            text=[d.capitalize() for d in diseases],\n",
        "            textposition='top center',\n",
        "            marker=dict(\n",
        "                size=30,\n",
        "                color=[DISEASE_COLORS.get(d, '#636EFA') for d in diseases],\n",
        "                opacity=0.8,\n",
        "                line=dict(width=3, color='white')\n",
        "            ),\n",
        "            name='Disease Separation'\n",
        "        ), row=2, col=1)\n",
        "\n",
        "        # 5. Feature importance (variance)\n",
        "        feature_importance = []\n",
        "        for i in range(len(feature_names)):\n",
        "            variance = np.var([all_disease_features[d][i] for d in diseases if i < len(all_disease_features[d])])\n",
        "            feature_importance.append(variance)\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=feature_names,\n",
        "            y=feature_importance,\n",
        "            marker_color='rgba(55, 128, 191, 0.8)',\n",
        "            name='Feature Importance'\n",
        "        ), row=2, col=2)\n",
        "\n",
        "        # 6. Summary table\n",
        "        table_data = []\n",
        "        for disease in diseases:\n",
        "            features = all_disease_features[disease]\n",
        "            table_data.append([\n",
        "                disease.capitalize(),\n",
        "                f\"{np.mean(features):.3f}\",\n",
        "                f\"{np.std(features):.3f}\",\n",
        "                f\"{np.max(features):.3f}\",\n",
        "                f\"{np.min(features):.3f}\"\n",
        "            ])\n",
        "\n",
        "        fig.add_trace(go.Table(\n",
        "            header=dict(values=['Disease', 'Mean', 'Std Dev', 'Max', 'Min'],\n",
        "                       fill_color='lightblue',\n",
        "                       align='left'),\n",
        "            cells=dict(values=list(zip(*table_data)),\n",
        "                      fill_color='white',\n",
        "                      align='left')\n",
        "        ), row=2, col=3)\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=1000,\n",
        "        width=1400,\n",
        "        title={\n",
        "            'text': \"üéØ Comprehensive Three-Class Analysis: Normal vs COPD vs Asthma\",\n",
        "            'x': 0.5,\n",
        "            'xanchor': 'center',\n",
        "            'font': {'size': 24, 'color': '#2C3E50'}\n",
        "        },\n",
        "        plot_bgcolor='rgba(248, 249, 250, 0.8)',\n",
        "        paper_bgcolor='white',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def prepare_plot_data(feature_vector, max_points=30):\n",
        "    \"\"\"üîß Prepares feature data for plotting\"\"\"\n",
        "    plot_data = feature_vector.flatten()\n",
        "    plot_data = np.nan_to_num(plot_data, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    if len(plot_data) > max_points:\n",
        "        indices = np.linspace(0, len(plot_data) - 1, max_points, dtype=int)\n",
        "        plot_data = plot_data[indices]\n",
        "\n",
        "    return plot_data\n",
        "\n",
        "\n",
        "def add_enhanced_bar_trace(fig, data, color, name, row, col):\n",
        "    \"\"\"üìä Adds enhanced bar trace with styling\"\"\"\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            y=data,\n",
        "            marker=dict(\n",
        "                color=color,\n",
        "                opacity=0.8,\n",
        "                line=dict(color='white', width=1)\n",
        "            ),\n",
        "            name=name,\n",
        "            showlegend=False,\n",
        "            hovertemplate='<b>%{fullData.name}</b><br>Index: %{x}<br>Value: %{y:.3f}<extra></extra>'\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "\n",
        "def add_enhanced_line_trace(fig, data, color, name, row, col):\n",
        "    \"\"\"üìà Adds enhanced line trace with styling\"\"\"\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            y=data,\n",
        "            mode='lines+markers',\n",
        "            line=dict(color=color, width=2),\n",
        "            marker=dict(size=4, opacity=0.8),\n",
        "            name=name,\n",
        "            showlegend=False,\n",
        "            hovertemplate='<b>%{fullData.name}</b><br>Index: %{x}<br>Value: %{y:.3f}<extra></extra>'\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "\n",
        "def add_enhanced_scatter_trace(fig, data, color, name, row, col):\n",
        "    \"\"\"üéØ Adds enhanced scatter trace with styling\"\"\"\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            y=data,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                color=color,\n",
        "                size=6,\n",
        "                opacity=0.8,\n",
        "                line=dict(color='white', width=1)\n",
        "            ),\n",
        "            name=name,\n",
        "            showlegend=False,\n",
        "            hovertemplate='<b>%{fullData.name}</b><br>Index: %{x}<br>Value: %{y:.3f}<extra></extra>'\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üöÄ MAIN EXECUTION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Execute the enhanced visualization\n",
        "if 'preprocessed_data' in globals() and preprocessed_data:\n",
        "    print(\"üé® Generating enhanced lung disease feature visualizations...\")\n",
        "    create_enhanced_feature_visualization(preprocessed_data, sampling_rate, handler)\n",
        "    print(\"‚úÖ Visualization generation completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Preprocessing data is not available. Cannot generate feature plots.\")\n",
        "    print(\"üìã Please ensure the following variables are defined:\")\n",
        "    print(\"   - preprocessed_data: Dictionary of preprocessed audio frames\")\n",
        "    print(\"   - sampling_rate: Audio sampling rate\")\n",
        "    print(\"   - handler: Feature extraction handler object\")\n",
        "    print(\"   - Feature extraction functions: extract_advanced_mfcc_features, extract_fbse_features, etc.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yUypBFKFk8DZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JPrYnGkGpCbm"
      },
      "outputs": [],
      "source": [
        "y_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_-6udcS7Gf3"
      },
      "source": [
        "# üß™ SMOTE Class Balancing for Imbalanced Lung Sound Data\n",
        "\n",
        "This section applies **SMOTE (Synthetic Minority Over-sampling Technique)** to balance the dataset for better model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Required Imports\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V02SS0NwpCZZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Check if X_data and y_data exist\n",
        "if 'X_data' not in locals() or 'y_data' not in locals():\n",
        "    print(\"Error: X_data or y_data is not defined. Please run the feature extraction step first.\")\n",
        "else:\n",
        "    try:\n",
        "        print(\"üîÑ Applying SMOTE for class balancing...\")\n",
        "\n",
        "        # Encode labels to numerical values for SMOTE\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_encoded = label_encoder.fit_transform(y_data)\n",
        "        joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "        # Check current class distribution to determine which classes need balancing\n",
        "        original_counts = Counter(y_encoded)\n",
        "        print(\"üìä Original Class Distribution (Encoded):\", original_counts)\n",
        "\n",
        "        # Set up SMOTE\n",
        "        # Determine sampling strategy: oversample all minority classes\n",
        "        # You can customize this if you only want to oversample specific classes\n",
        "        sampling_strategy = 'auto' # Oversample all minority classes to make them equal to the majority class\n",
        "\n",
        "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "\n",
        "        # Apply SMOTE\n",
        "        X_res, y_res_encoded = smote.fit_resample(X_data, y_encoded)\n",
        "\n",
        "        # Decode the balanced labels back to original strings\n",
        "        y_res = label_encoder.inverse_transform(y_res_encoded)\n",
        "\n",
        "        print(\"‚úÖ SMOTE application complete!\")\n",
        "        print(f\"üìä Original dataset shape: {X_data.shape}, {y_data.shape}\")\n",
        "        print(f\"üìä Resampled dataset shape: {X_res.shape}, {y_res.shape}\")\n",
        "\n",
        "        # Update the variables with the balanced data\n",
        "        X_data_balanced = X_res\n",
        "        y_data_balanced = y_res\n",
        "\n",
        "        # Print the new class distribution\n",
        "        balanced_counts = Counter(y_data_balanced)\n",
        "        print(\"üìä Balanced Class Distribution:\", balanced_counts)\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"‚ùå Error applying SMOTE: {ve}\")\n",
        "        print(\"This might happen if a class has too few samples to be resampled (e.g., less than k_neighbors, default is 5).\")\n",
        "        print(\"Consider removing classes with very few samples before applying SMOTE or reducing the k_neighbors parameter if appropriate.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An unexpected error occurred during SMOTE application: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-5WUJ7nY50A8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check if balanced data exists\n",
        "if 'X_data_balanced' in locals() and 'y_data_balanced' in locals():\n",
        "    print(\"\\nüé® Visualizing balanced data distribution after SMOTE...\")\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df_balanced = pd.DataFrame(X_data_balanced)\n",
        "    df_balanced['label'] = y_data_balanced\n",
        "\n",
        "    # Count the occurrences of each label in the balanced data\n",
        "    balanced_counts = Counter(y_data_balanced)\n",
        "    labels = list(balanced_counts.keys())\n",
        "    counts = list(balanced_counts.values())\n",
        "\n",
        "    # Create a bar chart to visualize the class distribution\n",
        "    fig_bar = px.bar(\n",
        "        x=labels,\n",
        "        y=counts,\n",
        "        color=labels,  # Use labels for coloring the bars\n",
        "        color_discrete_map={\n",
        "            'normal': DISEASE_COLORS.get('normal', '#2E8B57'),\n",
        "            'asthma': DISEASE_COLORS.get('asthma', '#FF6B6B'),\n",
        "            'copd': DISEASE_COLORS.get('copd', '#4ECDC4'),\n",
        "            # Add other classes if necessary, using DISEASE_COLORS or other colors\n",
        "        },\n",
        "        labels={'x': 'Disease Label', 'y': 'Number of Samples'},\n",
        "        title='<b>Balanced Dataset Class Distribution After SMOTE</b>',\n",
        "        template='plotly_white' # Use a clean template\n",
        "    )\n",
        "\n",
        "    # Enhance the layout\n",
        "    fig_bar.update_layout(\n",
        "        title_font_size=20,\n",
        "        xaxis_title_font_size=14,\n",
        "        yaxis_title_font_size=14,\n",
        "        uniformtext_minsize=8,\n",
        "        uniformtext_mode='hide',\n",
        "        hovermode='x unified' # Group hover info\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig_bar.show()\n",
        "\n",
        "    print(\"‚úÖ Visualization of balanced data completed!\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Balanced data (X_data_balanced, y_data_balanced) not found.\")\n",
        "    print(\"üìã Please ensure SMOTE was applied successfully before attempting to visualize.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LPV8g_-1pCXX"
      },
      "outputs": [],
      "source": [
        "y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "snjuYyzapCVL"
      },
      "outputs": [],
      "source": [
        "y_data_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pVjC6oUhpCSx"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import joblib\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FuNCmOnQov3J"
      },
      "outputs": [],
      "source": [
        "x_mfccs=X_data_balanced\n",
        "y_mfccs = y_data_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OsamGqH1peph"
      },
      "outputs": [],
      "source": [
        "x_mfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UrxU107HphBk"
      },
      "outputs": [],
      "source": [
        "y_mfccs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA7fvm_e8qCj"
      },
      "source": [
        "# üéß Interactive MFCC Visualization using Plotly\n",
        "\n",
        "This function generates a **dynamic MFCC heatmap** using `plotly.express`, ideal for analyzing and visually comparing lung sound signals.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Required Imports\n",
        "\n",
        "```python\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w5CsGFL1pjai"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_mfcc_advanced(filepath):\n",
        "    \"\"\"\n",
        "    Plots the MFCC features of the audio file using Plotly for interactive visualization.\n",
        "    Args:\n",
        "        filepath (str): The path to the audio file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(filepath, sr=None) # Use native sampling rate\n",
        "        if len(audio) == 0:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is empty: {filepath}\")\n",
        "            return\n",
        "\n",
        "        # Extract MFCCs\n",
        "        # Using parameters that might be more robust for various audio lengths\n",
        "        n_mfcc = 40\n",
        "        n_fft = 2048\n",
        "        hop_length = 512\n",
        "        n_mels = 128\n",
        "\n",
        "        # Handle short audio files\n",
        "        if len(audio) < n_fft:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is too short for MFCC extraction: {filepath}\")\n",
        "            # Pad the audio if it's too short\n",
        "            audio = np.pad(audio, (0, n_fft - len(audio)), 'constant')\n",
        "\n",
        "\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=audio,\n",
        "            sr=sr,\n",
        "            n_mfcc=n_mfcc,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels,\n",
        "            fmax=sr//2\n",
        "        )\n",
        "\n",
        "        # Handle potential empty MFCC array for very short sounds even after padding\n",
        "        if mfccs.shape[1] == 0:\n",
        "             print(f\"‚ö†Ô∏è  MFCC extraction resulted in an empty array for: {filepath}\")\n",
        "             return\n",
        "\n",
        "        # Convert MFCCs to dB scale for better visualization\n",
        "        mfccs_db = librosa.power_to_db(mfccs, ref=np.max)\n",
        "\n",
        "        # Create time and MFCC coefficient index labels\n",
        "        time_axis = librosa.times_like(mfccs_db, sr=sr, hop_length=hop_length)\n",
        "        mfcc_coeffs = [f'MFCC {i+1}' for i in range(n_mfcc)]\n",
        "\n",
        "        # Create interactive heatmap using Plotly\n",
        "        fig = px.imshow(mfccs_db,\n",
        "                        aspect=\"auto\",\n",
        "                        x=time_axis,\n",
        "                        y=mfcc_coeffs,\n",
        "                        labels=dict(x=\"Time (s)\", y=\"MFCC Coefficient\", color=\"Amplitude (dB)\"),\n",
        "                        title=f'MFCC Heatmap: {os.path.basename(filepath)}',\n",
        "                        color_continuous_scale='Viridis') # Choose a pleasant color scale\n",
        "\n",
        "        # Update layout for better readability\n",
        "        fig.update_layout(\n",
        "            title_x=0.5,\n",
        "            yaxis_title=\"MFCC Coefficient Index\",\n",
        "            xaxis_title=\"Time (s)\",\n",
        "            hovermode='closest' # Show tooltip on hover\n",
        "        )\n",
        "\n",
        "        # Show plot\n",
        "        fig.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during MFCC plotting for {filepath}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "# Assuming filepaths is a list of audio file paths from previous code\n",
        "if 'filepaths' in locals() and filepaths:\n",
        "    # Select one file to plot\n",
        "    example_file_to_plot = filepaths[0] # Replace with the path to your desired file\n",
        "    print(f\"Generating Plotly MFCC heatmap for: {os.path.basename(example_file_to_plot)}\")\n",
        "    plot_mfcc_advanced(example_file_to_plot)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  'filepaths' variable not found or is empty. Cannot plot example MFCC.\")\n",
        "    print(\"Please ensure the file discovery step has been successfully executed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "agZCurknmZn4"
      },
      "outputs": [],
      "source": [
        "# prompt: for above plot rest other featue seprately if possible consider waveforms to show the feature or heatmaps\n",
        "\n",
        "# Function to plot Spectral Centroid\n",
        "def plot_spectral_centroid(filepath):\n",
        "    \"\"\"Plots the Spectral Centroid of the audio file.\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "        if len(audio) == 0:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is empty: {filepath}\")\n",
        "            return\n",
        "\n",
        "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
        "        time_axis = librosa.times_like(spectral_centroids, sr=sr)\n",
        "\n",
        "        fig = px.line(x=time_axis, y=spectral_centroids,\n",
        "                      title=f'Spectral Centroid: {os.path.basename(filepath)}',\n",
        "                      labels={'x': 'Time (s)', 'y': 'Spectral Centroid (Hz)'})\n",
        "\n",
        "        fig.update_layout(title_x=0.5)\n",
        "        fig.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during Spectral Centroid plotting for {filepath}: {e}\")\n",
        "\n",
        "\n",
        "# Function to plot Zero Crossing Rate\n",
        "def plot_zero_crossing_rate(filepath):\n",
        "    \"\"\"Plots the Zero Crossing Rate of the audio file.\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "        if len(audio) == 0:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is empty: {filepath}\")\n",
        "            return\n",
        "\n",
        "        zero_crossings = librosa.feature.zero_crossing_rate(audio)[0]\n",
        "        time_axis = librosa.times_like(zero_crossings, sr=sr)\n",
        "\n",
        "        fig = px.line(x=time_axis, y=zero_crossings,\n",
        "                      title=f'Zero Crossing Rate: {os.path.basename(filepath)}',\n",
        "                      labels={'x': 'Time (s)', 'y': 'Zero Crossing Rate'})\n",
        "\n",
        "        fig.update_layout(title_x=0.5)\n",
        "        fig.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during Zero Crossing Rate plotting for {filepath}: {e}\")\n",
        "\n",
        "# Function to plot Chroma Features (Heatmap)\n",
        "def plot_chroma_features(filepath):\n",
        "    \"\"\"Plots the Chroma Features of the audio file as a heatmap.\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "        if len(audio) == 0:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is empty: {filepath}\")\n",
        "            return\n",
        "\n",
        "        # Ensure sampling rate is sufficient for chroma\n",
        "        if sr < 8000:\n",
        "             print(f\"‚ö†Ô∏è  Sampling rate {sr}Hz is too low for meaningful Chroma features. Skipping: {filepath}\")\n",
        "             return\n",
        "\n",
        "        chromagram = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "        time_axis = librosa.times_like(chromagram, sr=sr)\n",
        "        chroma_notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "\n",
        "        fig = px.imshow(chromagram,\n",
        "                        aspect=\"auto\",\n",
        "                        x=time_axis,\n",
        "                        y=chroma_notes,\n",
        "                        labels=dict(x=\"Time (s)\", y=\"Chroma Note\", color=\"Intensity\"),\n",
        "                        title=f'Chroma Features: {os.path.basename(filepath)}',\n",
        "                        color_continuous_scale='Plasma')\n",
        "\n",
        "        fig.update_layout(title_x=0.5, yaxis_title=\"Pitch Class\")\n",
        "        fig.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during Chroma plotting for {filepath}: {e}\")\n",
        "\n",
        "# Function to plot Mel-Spectrogram\n",
        "def plot_mel_spectrogram(filepath):\n",
        "    \"\"\"Plots the Mel-Spectrogram of the audio file as a heatmap.\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "        if len(audio) == 0:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is empty: {filepath}\")\n",
        "            return\n",
        "\n",
        "        n_fft = 2048\n",
        "        hop_length = 512\n",
        "\n",
        "        # Handle short audio files\n",
        "        if len(audio) < n_fft:\n",
        "            print(f\"‚ö†Ô∏è  Audio file is too short for Mel-Spectrogram: {filepath}\")\n",
        "            audio = np.pad(audio, (0, n_fft - len(audio)), 'constant')\n",
        "\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=sr,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length\n",
        "        )\n",
        "\n",
        "        if mel_spectrogram.size == 0:\n",
        "             print(f\"‚ö†Ô∏è  Mel-Spectrogram extraction resulted in an empty array for: {filepath}\")\n",
        "             return\n",
        "\n",
        "\n",
        "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "        time_axis = librosa.times_like(mel_spectrogram_db, sr=sr, hop_length=hop_length)\n",
        "        freq_axis = librosa.mel_frequencies(n_mels=mel_spectrogram.shape[0], fmin=0, fmax=sr/2)\n",
        "\n",
        "        fig = px.imshow(mel_spectrogram_db,\n",
        "                        aspect=\"auto\",\n",
        "                        x=time_axis,\n",
        "                        y=freq_axis,\n",
        "                        labels=dict(x=\"Time (s)\", y=\"Mel Frequency (Hz)\", color=\"Amplitude (dB)\"),\n",
        "                        title=f'Mel-Spectrogram: {os.path.basename(filepath)}',\n",
        "                        color_continuous_scale='Jet') # Another common color scale\n",
        "\n",
        "        fig.update_layout(title_x=0.5, yaxis_title=\"Frequency (Mel)\")\n",
        "        fig.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during Mel-Spectrogram plotting for {filepath}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "# Assuming filepaths is a list of audio file paths from previous code\n",
        "if 'filepaths' in locals() and filepaths:\n",
        "    # Select one file to plot\n",
        "    example_file_to_plot = filepaths[0] # Replace with the path to your desired file\n",
        "    print(f\"\\nGenerating visualizations for other features for: {os.path.basename(example_file_to_plot)}\")\n",
        "\n",
        "    plot_spectral_centroid(example_file_to_plot)\n",
        "    plot_zero_crossing_rate(example_file_to_plot)\n",
        "    plot_chroma_features(example_file_to_plot) # May be skipped if SR is too low\n",
        "    plot_mel_spectrogram(example_file_to_plot)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  'filepaths' variable not found or is empty. Cannot plot example features.\")\n",
        "    print(\"Please ensure the file discovery step has been successfully executed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MUK4wG3eoAEr"
      },
      "outputs": [],
      "source": [
        "# prompt: plot the above graphs for one model of each asthma as well as copd\n",
        "\n",
        "# Select one file for each of the target diseases (asthma, copd, normal)\n",
        "# Ensure these files exist in your filepaths list and are correctly labeled\n",
        "\n",
        "asthma_file = None\n",
        "copd_file = None\n",
        "normal_file = None\n",
        "\n",
        "# Iterate through filepaths to find one sample for each class\n",
        "if 'filepaths' in locals() and filepaths:\n",
        "    for f in filepaths:\n",
        "        filename = os.path.basename(f).lower()\n",
        "        if 'asthma' in filename and asthma_file is None:\n",
        "            asthma_file = f\n",
        "        elif 'copd' in filename and copd_file is None:\n",
        "            copd_file = f\n",
        "        elif ('_n_' in filename or '_normal_' in filename or '_c_' in filename) and normal_file is None:\n",
        "             normal_file = f # Assuming '_n_' or '_normal_' or '_c_' denotes normal\n",
        "        if asthma_file and copd_file and normal_file:\n",
        "            break\n",
        "\n",
        "    # Check if samples were found\n",
        "    if asthma_file:\n",
        "        print(f\"Generating plots for Asthma sample: {os.path.basename(asthma_file)}\")\n",
        "        plot_mfcc_advanced(asthma_file)\n",
        "        plot_spectral_centroid(asthma_file)\n",
        "        plot_zero_crossing_rate(asthma_file)\n",
        "        plot_chroma_features(asthma_file)\n",
        "        plot_mel_spectrogram(asthma_file)\n",
        "    else:\n",
        "        print(\"‚ùå Could not find a sample file for Asthma.\")\n",
        "\n",
        "    if copd_file:\n",
        "        print(f\"\\nGenerating plots for COPD sample: {os.path.basename(copd_file)}\")\n",
        "        plot_mfcc_advanced(copd_file)\n",
        "        plot_spectral_centroid(copd_file)\n",
        "        plot_zero_crossing_rate(copd_file)\n",
        "        plot_chroma_features(copd_file)\n",
        "        plot_mel_spectrogram(copd_file)\n",
        "    else:\n",
        "        print(\"‚ùå Could not find a sample file for COPD.\")\n",
        "\n",
        "    if normal_file:\n",
        "        print(f\"\\nGenerating plots for Normal sample: {os.path.basename(normal_file)}\")\n",
        "        plot_mfcc_advanced(normal_file)\n",
        "        plot_spectral_centroid(normal_file)\n",
        "        plot_zero_crossing_rate(normal_file)\n",
        "        plot_chroma_features(normal_file)\n",
        "        plot_mel_spectrogram(normal_file)\n",
        "    else:\n",
        "         print(\"‚ùå Could not find a sample file for Normal.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  'filepaths' variable not found or is empty. Cannot plot features.\")\n",
        "    print(\"Please ensure the file discovery step has been successfully executed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "li1yC9wipjXE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def plot_augmented_data_distribution(labels):\n",
        "    \"\"\"\n",
        "    Plots the count of each disease in the dataset with enhanced visualization using Plotly.\n",
        "\n",
        "    Args:\n",
        "        labels (array-like): List or array of disease labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the count of each unique disease.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    data_count = dict(zip(unique_labels, counts))\n",
        "\n",
        "    # Map labels for display: 'n' to 'Healthy', others remain as disease names\n",
        "    display_labels = []\n",
        "    for label in unique_labels:\n",
        "        if label == 'n':\n",
        "            display_labels.append('Healthy')\n",
        "        else:\n",
        "            display_labels.append(str(label))\n",
        "\n",
        "    # Create professional color palette with gradient and complementary colors\n",
        "    if len(unique_labels) <= 10:\n",
        "        # Professional color palette for up to 10 categories\n",
        "        professional_colors = [\n",
        "            '#2E86AB',  # Ocean Blue\n",
        "            '#A23B72',  # Deep Rose\n",
        "            '#F18F01',  # Amber Orange\n",
        "            '#C73E1D',  # Crimson Red\n",
        "            '#6A994E',  # Forest Green\n",
        "            '#7209B7',  # Royal Purple\n",
        "            '#F77F00',  # Burnt Orange\n",
        "            '#FCBF49',  # Golden Yellow\n",
        "            '#003566',  # Navy Blue\n",
        "            '#06FFA5'   # Mint Green\n",
        "        ]\n",
        "        colors = professional_colors[:len(unique_labels)]\n",
        "    else:\n",
        "        # For more than 10 categories, use a smooth gradient\n",
        "        colors = px.colors.sample_colorscale(\n",
        "            'viridis',\n",
        "            [i/(len(unique_labels)-1) for i in range(len(unique_labels))]\n",
        "        )\n",
        "\n",
        "    # Create the main bar chart\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add bar chart with enhanced styling\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=display_labels,\n",
        "        y=counts,\n",
        "        text=[f'{count}' for count in counts],\n",
        "        textposition='outside',\n",
        "        textfont=dict(size=12, color='black', family='Arial Black'),\n",
        "        marker=dict(\n",
        "            color=colors,\n",
        "            line=dict(color='rgba(255, 255, 255, 0.8)', width=2.5),\n",
        "            opacity=0.85,\n",
        "            # Add subtle gradient effect\n",
        "            pattern=dict(\n",
        "                shape=\"\",\n",
        "                bgcolor=\"rgba(255, 255, 255, 0.1)\"\n",
        "            )\n",
        "        ),\n",
        "        hovertemplate='<b>%{x}</b><br>' +\n",
        "                      'Count: %{y}<br>' +\n",
        "                      '<extra></extra>',\n",
        "        name='Disease Count'\n",
        "    ))\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_samples = sum(counts)\n",
        "    unique_classes = len(unique_labels)\n",
        "\n",
        "    # Update layout with enhanced styling\n",
        "    fig.update_layout(\n",
        "        title=dict(\n",
        "            text=\"Distribution of Diseases in Augmented Data\",\n",
        "            x=0.5,\n",
        "            font=dict(size=18, color='#2c3e50', family='Arial Black')\n",
        "        ),\n",
        "        xaxis=dict(\n",
        "            title=dict(\n",
        "                text=\"Diseases\",\n",
        "                font=dict(size=14, color='#2c3e50', family='Arial Black')\n",
        "            ),\n",
        "            tickfont=dict(size=12, color='#34495e'),\n",
        "            tickangle=45,\n",
        "            showgrid=False,\n",
        "            showline=True,\n",
        "            linewidth=2,\n",
        "            linecolor='#bdc3c7'\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=dict(\n",
        "                text=\"Count\",\n",
        "                font=dict(size=14, color='#2c3e50', family='Arial Black')\n",
        "            ),\n",
        "            tickfont=dict(size=12, color='#34495e'),\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='rgba(189, 195, 199, 0.3)',\n",
        "            showline=True,\n",
        "            linewidth=2,\n",
        "            linecolor='#bdc3c7'\n",
        "        ),\n",
        "        plot_bgcolor='rgba(248, 249, 250, 0.95)',\n",
        "        paper_bgcolor='#FEFEFE',\n",
        "        font=dict(family='Arial'),\n",
        "        showlegend=False,\n",
        "        margin=dict(l=80, r=80, t=100, b=120),\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "\n",
        "    # Add statistics annotation with professional styling\n",
        "    fig.add_annotation(\n",
        "        text=f\"<b>Statistics</b><br>Total Samples: {total_samples}<br>Unique Classes: {unique_classes}\",\n",
        "        xref=\"paper\", yref=\"paper\",\n",
        "        x=0.02, y=0.98,\n",
        "        xanchor=\"left\", yanchor=\"top\",\n",
        "        showarrow=False,\n",
        "        bgcolor=\"rgba(46, 134, 171, 0.15)\",\n",
        "        bordercolor=\"#2E86AB\",\n",
        "        borderwidth=2,\n",
        "        borderpad=12,\n",
        "        font=dict(size=11, color='#2c3e50', family='Arial Bold')\n",
        "    )\n",
        "\n",
        "    # Add hover effects and interactivity\n",
        "    fig.update_traces(\n",
        "        marker=dict(\n",
        "            line=dict(width=2),\n",
        "        ),\n",
        "        selector=dict(type=\"bar\")\n",
        "    )\n",
        "\n",
        "    # Show the interactive plot\n",
        "    fig.show()\n",
        "\n",
        "    return data_count\n",
        "\n",
        "# Example usage:\n",
        "# labels = np.array(['disease1', 'n', 'disease2', 'n', 'disease1'])  # Your dataset labels\n",
        "# disease_counts = plot_augmented_data_distribution(labels)\n",
        "# print(disease_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A5l29W1apjU8"
      },
      "outputs": [],
      "source": [
        " plot_augmented_data_distribution(y_mfccs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7dflKl085ve"
      },
      "source": [
        "# üß¨ One-Hot Encoding for Disease Labels (Normal, Asthma, COPD)\n",
        "\n",
        "This function performs **efficient, vectorized one-hot encoding** of lung disease labels:\n",
        "- `'normal'` ‚Üí `[1, 0, 0]`\n",
        "- `'asthma'` ‚Üí `[0, 1, 0]`\n",
        "- `'copd'` ‚Üí `[0, 0, 1]`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rw46dGzRpjTb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_disease_labels(y_data_balanced):\n",
        "    \"\"\"\n",
        "    Efficient one-hot encoding for disease labels using vectorized operations.\n",
        "\n",
        "    Args:\n",
        "        y_data_balanced (array-like): Input labels to encode\n",
        "                                    Expected labels: ['normal', 'asthma', 'copd']\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: One-hot encoded labels (n_samples, 3)\n",
        "                      [1,0,0] for 'normal'\n",
        "                      [0,1,0] for 'asthma'\n",
        "                      [0,0,1] for 'copd'\n",
        "    \"\"\"\n",
        "    # Convert to numpy array and flatten\n",
        "    y_flat = np.array(y_data_balanced).flatten()\n",
        "    n_samples = len(y_flat)\n",
        "\n",
        "    # Pre-allocate output array (3 classes, not 4)\n",
        "    Y_data = np.zeros((n_samples, 3), dtype=np.float64)\n",
        "\n",
        "    # Vectorized encoding using boolean indexing\n",
        "    Y_data[y_flat == 'normal', 0] = 1   # [1,0,0]\n",
        "    Y_data[y_flat == 'asthma', 1] = 1   # [0,1,0]\n",
        "    Y_data[y_flat == 'copd', 2] = 1     # [0,0,1]\n",
        "\n",
        "    return Y_data\n",
        "\n",
        "# Example usage:\n",
        "# y_data_balanced = ['normal', 'asthma', 'copd', 'normal', 'asthma']\n",
        "Y_data = encode_disease_labels(y_data_balanced)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dE4kl_VTpjRc"
      },
      "outputs": [],
      "source": [
        "X_data_balanced.shape, Y_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rnnk4iIspjPl"
      },
      "outputs": [],
      "source": [
        "X_data=X_data_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dSPgzeSJpjNN"
      },
      "outputs": [],
      "source": [
        "Y_data   #=> normal  , asthma , copd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdZzaismpzoz"
      },
      "source": [
        "# üîÑ GRU-Ready Data Split & Preparation Pipeline\n",
        "\n",
        "This function handles the complete pipeline for preparing your dataset to train a GRU-based deep learning model. It includes:\n",
        "- Validated stratified splits into **Train / Validation / Test**\n",
        "- Proper input reshaping to meet GRU input shape requirements\n",
        "- Optional label reshaping for time-series regression (optional)\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Required Imports\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fiR3mU_opjKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_and_prepare_for_gru(X_data, Y_data, train_ratio=0.75, val_ratio=0.175, test_ratio=0.075,\n",
        "                             random_state=10, reshape_labels=False):\n",
        "    \"\"\"\n",
        "    Complete pipeline: Split data and prepare for GRU training in one function.\n",
        "\n",
        "    Args:\n",
        "        X_data: Feature data\n",
        "        Y_data: Label data\n",
        "        train_ratio: Training set ratio (default: 0.75)\n",
        "        val_ratio: Validation set ratio (default: 0.175)\n",
        "        test_ratio: Test set ratio (default: 0.075)\n",
        "        random_state: Random seed for reproducibility\n",
        "        reshape_labels: Whether to add time dimension to labels (False for classification)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (x_train_gru, x_val_gru, x_test_gru, y_train_gru, y_val_gru, y_test_gru)\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Data Splitting\n",
        "    print(\"Step 1: Splitting data...\")\n",
        "\n",
        "    # Validate ratios\n",
        "    total_ratio = train_ratio + val_ratio + test_ratio\n",
        "    if not np.isclose(total_ratio, 1.0):\n",
        "        raise ValueError(f\"Ratios must sum to 1.0, got {total_ratio}\")\n",
        "\n",
        "    # First split: separate training from (validation + test)\n",
        "    val_test_ratio = val_ratio + test_ratio\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X_data, Y_data,\n",
        "        test_size=val_test_ratio,\n",
        "        random_state=random_state,\n",
        "        stratify=np.argmax(Y_data, axis=1) if Y_data.ndim > 1 else Y_data\n",
        "    )\n",
        "\n",
        "    # Second split: separate validation from test\n",
        "    test_ratio_adjusted = test_ratio / val_test_ratio\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=test_ratio_adjusted,\n",
        "        random_state=random_state,\n",
        "        stratify=np.argmax(y_temp, axis=1) if y_temp.ndim > 1 else y_temp\n",
        "    )\n",
        "\n",
        "    # Print split statistics\n",
        "    total_samples = len(X_train) + len(X_val) + len(X_test)\n",
        "    print(f\"\\nData Split Statistics:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Training:   {len(X_train)/total_samples:.1%} ({len(X_train):,} samples)\")\n",
        "    print(f\"Validation: {len(X_val)/total_samples:.1%} ({len(X_val):,} samples)\")\n",
        "    print(f\"Testing:    {len(X_test)/total_samples:.1%} ({len(X_test):,} samples)\")\n",
        "    print(f\"Total:      {total_samples:,} samples\")\n",
        "\n",
        "    print(\"\\nOriginal shapes after splitting:\")\n",
        "    print(f\"Features: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")\n",
        "    print(f\"Labels:   Train={y_train.shape}, Val={y_val.shape}, Test={y_test.shape}\")\n",
        "\n",
        "    # Step 2: GRU Data Preparation\n",
        "    print(\"\\nStep 2: Preparing data for GRU...\")\n",
        "\n",
        "    # Reshape features for GRU (add time dimension)\n",
        "    # GRU expects: (batch_size, timesteps, features)\n",
        "    x_train_gru = np.expand_dims(X_train, axis=1)\n",
        "    x_val_gru = np.expand_dims(X_val, axis=1)\n",
        "    x_test_gru = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "    # Handle labels based on reshape_labels parameter\n",
        "    if reshape_labels:\n",
        "        y_train_gru = np.expand_dims(y_train, axis=1)\n",
        "        y_val_gru = np.expand_dims(y_val, axis=1)\n",
        "        y_test_gru = np.expand_dims(y_test, axis=1)\n",
        "    else:\n",
        "        # Keep labels as-is for standard classification\n",
        "        y_train_gru = y_train\n",
        "        y_val_gru = y_val\n",
        "        y_test_gru = y_test\n",
        "\n",
        "    print(\"\\nFinal GRU-ready shapes:\")\n",
        "    print(f\"Features: Train={x_train_gru.shape}, Val={x_val_gru.shape}, Test={x_test_gru.shape}\")\n",
        "    print(f\"Labels:   Train={y_train_gru.shape}, Val={y_val_gru.shape}, Test={y_test_gru.shape}\")\n",
        "\n",
        "    # Validation checks\n",
        "    assert x_train_gru.shape[0] == y_train_gru.shape[0], \"Mismatch in training samples\"\n",
        "    assert x_val_gru.shape[0] == y_val_gru.shape[0], \"Mismatch in validation samples\"\n",
        "    assert x_test_gru.shape[0] == y_test_gru.shape[0], \"Mismatch in test samples\"\n",
        "\n",
        "    # Class distribution (if labels are one-hot encoded)\n",
        "    if y_train.ndim > 1 and y_train.shape[1] > 1:\n",
        "        class_names = ['COPD', 'Asthma', 'Healthy']\n",
        "        print(\"\\nClass Distribution:\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            train_count = np.sum(y_train[:, i])\n",
        "            val_count = np.sum(y_val[:, i])\n",
        "            test_count = np.sum(y_test[:, i])\n",
        "            total_class = train_count + val_count + test_count\n",
        "            print(f\"  {class_name:>10}: Train={train_count:>3.0f} | Val={val_count:>3.0f} | Test={test_count:>3.0f} | Total={total_class:>3.0f}\")\n",
        "\n",
        "    print(\"\\n‚úì Data splitting and GRU preparation completed successfully!\")\n",
        "\n",
        "    return x_train_gru, x_val_gru, x_test_gru, y_train_gru, y_val_gru, y_test_gru\n",
        "\n",
        "# Alternative: Enhanced version of original approach (matching your exact ratios)\n",
        "def split_and_prepare_original_enhanced(X_data, Y_data, random_state=10):\n",
        "    \"\"\"\n",
        "    Enhanced version matching your original splitting ratios exactly.\n",
        "    \"\"\"\n",
        "    print(\"Using original enhanced splitting approach...\")\n",
        "\n",
        "    # First split: 82.5% train+test, 17.5% validation\n",
        "    X_temp, X_val, y_temp, y_val = train_test_split(\n",
        "        X_data, Y_data,\n",
        "        test_size=0.175,\n",
        "        random_state=random_state,\n",
        "        stratify=np.argmax(Y_data, axis=1) if Y_data.ndim > 1 else Y_data\n",
        "    )\n",
        "\n",
        "    # Second split: ~9.1% of total for test (7.5% of remaining 82.5%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=0.075/0.825,  # Adjusted ratio\n",
        "        random_state=random_state,\n",
        "        stratify=np.argmax(y_temp, axis=1) if y_temp.ndim > 1 else y_temp\n",
        "    )\n",
        "\n",
        "    # Prepare for GRU\n",
        "    x_train_gru = np.expand_dims(X_train, axis=1)\n",
        "    x_val_gru = np.expand_dims(X_val, axis=1)\n",
        "    x_test_gru = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "    y_train_gru = y_train\n",
        "    y_val_gru = y_val\n",
        "    y_test_gru = y_test\n",
        "\n",
        "    # Print results\n",
        "    total_samples = len(X_train) + len(X_val) + len(X_test)\n",
        "    print(f\"\\nSplit ratios achieved:\")\n",
        "    print(f\"Training:   {len(X_train)/total_samples:.1%} ({len(X_train)} samples)\")\n",
        "    print(f\"Validation: {len(X_val)/total_samples:.1%} ({len(X_val)} samples)\")\n",
        "    print(f\"Testing:    {len(X_test)/total_samples:.1%} ({len(X_test)} samples)\")\n",
        "\n",
        "    print(f\"\\nGRU-ready shapes:\")\n",
        "    print(f\"x_train_gru: {x_train_gru.shape}\")\n",
        "    print(f\"x_val_gru:   {x_val_gru.shape}\")\n",
        "    print(f\"x_test_gru:  {x_test_gru.shape}\")\n",
        "    print(f\"y_train_gru: {y_train_gru.shape}\")\n",
        "    print(f\"y_val_gru:   {y_val_gru.shape}\")\n",
        "    print(f\"y_test_gru:  {y_test_gru.shape}\")\n",
        "\n",
        "    return x_train_gru, x_val_gru, x_test_gru, y_train_gru, y_val_gru, y_test_gru\n",
        "\n",
        "# Usage Examples:\n",
        "\n",
        "# Method 1: Complete pipeline with custom ratios (recommended)\n",
        "x_train_gru, x_val_gru, x_test_gru, y_train_gru, y_val_gru, y_test_gru = split_and_prepare_for_gru(\n",
        "    X_data, Y_data,\n",
        "    train_ratio=0.75,\n",
        "    val_ratio=0.175,\n",
        "    test_ratio=0.075,\n",
        "    random_state=10,\n",
        "    reshape_labels=False  # Set to False for classification tasks\n",
        ")\n",
        "\n",
        "# Method 2: Original enhanced approach\n",
        "# x_train_gru, x_val_gru, x_test_gru, y_train_gru, y_val_gru, y_test_gru = split_and_prepare_original_enhanced(\n",
        "#     X_data, Y_data, random_state=10\n",
        "# )\n",
        "\n",
        "# Your data is now ready for GRU training!\n",
        "print(f\"\\nüéâ Final shapes ready for GRU model:\")\n",
        "print(f\"Features: {x_train_gru.shape}, {x_val_gru.shape}, {x_test_gru.shape}\")\n",
        "print(f\"Labels:   {y_train_gru.shape}, {y_val_gru.shape}, {y_test_gru.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JA23bi3tpjIH"
      },
      "outputs": [],
      "source": [
        "x_train_gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "McEYz6pHpjGl"
      },
      "outputs": [],
      "source": [
        "y_train_gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qMWZBe3VpjEn"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner --upgrade --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8xVjO5LLpjCU"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow tensorflow-addons keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA_5vEPXrXEz"
      },
      "source": [
        "# **MODEL 1**\n",
        "# ü´Å Lung Sound Classification Training Pipeline ‚Äì ‚öôÔ∏è FIXED & Optimized\n",
        "\n",
        "This pipeline builds a **deep neural network** optimized for **lung disease detection** (Asthma, COPD, Healthy) using **GRU**, **Conv1D**, and **advanced regularization techniques**. Designed to be used with **pre-split data** in the format:\n",
        "\n",
        "- **Input Shape**: `(samples, 1, 959)`\n",
        "- **Labels**: One-hot encoded for 3 classes (`[Healthy, Asthma, COPD]`)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Features\n",
        "\n",
        "- üí° **Hybrid Conv1D + Bi-GRU** architecture for local + temporal pattern learning  \n",
        "- üß† Customizable architecture via config dictionary  \n",
        "- üß™ Advanced regularization: `Dropout`, `L1/L2`, and `BatchNorm`  \n",
        "- üìà Training callbacks: `EarlyStopping`, `ReduceLROnPlateau`, `LearningRateScheduler`  \n",
        "- üéØ Custom metrics including **weighted F1 Score** and **Precision/Recall**  \n",
        "- üìä Visual training history + Confusion Matrix  \n",
        "- üì¶ Easily pluggable into your data pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Model Architecture Highlights\n",
        "\n",
        "- `Conv1D ‚Üí Bi-GRU(128) ‚Üí Bi-GRU(64) ‚Üí Dense(256 ‚Üí 128) ‚Üí Output(Softmax)`\n",
        "- Support for:\n",
        "  - **Bidirectional RNNs**\n",
        "  - **Multiple pooling strategies**: `avg`, `max`, `both`\n",
        "  - **Dynamic learning rate scheduling**\n",
        "- Input: `1 √ó 959` (time √ó features)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How to Use\n",
        "\n",
        "### 1. Load your GRU-ready data\n",
        "\n",
        "```python\n",
        "# Expected shapes\n",
        "x_train_gru.shape  # (e.g., (1181, 1, 959))\n",
        "y_train_gru.shape  # (1181, 3)\n",
        "x_val_gru.shape    # (275, 1, 959)\n",
        "y_val_gru.shape    # (275, 3)\n",
        "x_test_gru.shape   # (119, 1, 959)\n",
        "y_test_gru.shape   # (119, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L4bxkSfepjAR"
      },
      "outputs": [],
      "source": [
        "# Lung Sound Classification Training Pipeline - FIXED VERSION\n",
        "# Ready to use with your pre-split data\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,\n",
        "    CSVLogger, LearningRateScheduler\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class OptimizedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Optimized Neural Network for Lung Sound Classification\n",
        "    Designed for your specific dataset: (samples, 1, 959) -> 3 classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_model(self, config=None):\n",
        "        \"\"\"Create optimized model architecture\"\"\"\n",
        "        if config is None:\n",
        "            config = {\n",
        "                'conv_filters': 64,\n",
        "                'gru_units_1': 128,\n",
        "                'gru_units_2': 64,\n",
        "                'dense_units_1': 256,\n",
        "                'dense_units_2': 128,\n",
        "                'dropout_rate': 0.4,\n",
        "                'l2_reg': 0.001,\n",
        "                'use_bidirectional': True,\n",
        "                'use_conv1d': True,\n",
        "                'pooling_strategy': 'both'\n",
        "            }\n",
        "\n",
        "        # Input layer\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Optional Conv1D for local pattern extraction\n",
        "        if config['use_conv1d']:\n",
        "            x = Conv1D(\n",
        "                filters=config['conv_filters'],\n",
        "                kernel_size=5,\n",
        "                padding='same',\n",
        "                kernel_regularizer=l2(config['l2_reg'])\n",
        "            )(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = Dropout(config['dropout_rate'] * 0.5)(x)\n",
        "\n",
        "        # First RNN layer\n",
        "        if config['use_bidirectional']:\n",
        "            x = Bidirectional(\n",
        "                GRU(\n",
        "                    config['gru_units_1'],\n",
        "                    return_sequences=True,\n",
        "                    kernel_regularizer=l2(config['l2_reg']),\n",
        "                    recurrent_regularizer=l2(config['l2_reg'] * 0.5),\n",
        "                    dropout=config['dropout_rate'] * 0.3,\n",
        "                    recurrent_dropout=config['dropout_rate'] * 0.3\n",
        "                ),\n",
        "                name='bi_gru_1'\n",
        "            )(x)\n",
        "        else:\n",
        "            x = GRU(\n",
        "                config['gru_units_1'] * 2,\n",
        "                return_sequences=True,\n",
        "                kernel_regularizer=l2(config['l2_reg']),\n",
        "                recurrent_regularizer=l2(config['l2_reg'] * 0.5),\n",
        "                dropout=config['dropout_rate'] * 0.3,\n",
        "                recurrent_dropout=config['dropout_rate'] * 0.3,\n",
        "                name='gru_1'\n",
        "            )(x)\n",
        "\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Second RNN layer\n",
        "        if config['use_bidirectional']:\n",
        "            x = Bidirectional(\n",
        "                GRU(\n",
        "                    config['gru_units_2'],\n",
        "                    return_sequences=True,\n",
        "                    kernel_regularizer=l2(config['l2_reg']),\n",
        "                    recurrent_regularizer=l2(config['l2_reg'] * 0.5),\n",
        "                    dropout=config['dropout_rate'] * 0.3,\n",
        "                    recurrent_dropout=config['dropout_rate'] * 0.3\n",
        "                ),\n",
        "                name='bi_gru_2'\n",
        "            )(x)\n",
        "        else:\n",
        "            x = GRU(\n",
        "                config['gru_units_2'] * 2,\n",
        "                return_sequences=True,\n",
        "                kernel_regularizer=l2(config['l2_reg']),\n",
        "                recurrent_regularizer=l2(config['l2_reg'] * 0.5),\n",
        "                dropout=config['dropout_rate'] * 0.3,\n",
        "                recurrent_dropout=config['dropout_rate'] * 0.3,\n",
        "                name='gru_2'\n",
        "            )(x)\n",
        "\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Global pooling\n",
        "        if config['pooling_strategy'] == 'both':\n",
        "            avg_pool = GlobalAveragePooling1D()(x)\n",
        "            max_pool = GlobalMaxPooling1D()(x)\n",
        "            x = Concatenate()([avg_pool, max_pool])\n",
        "        elif config['pooling_strategy'] == 'avg':\n",
        "            x = GlobalAveragePooling1D()(x)\n",
        "        else:\n",
        "            x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "        # Dense layers\n",
        "        x = Dense(\n",
        "            config['dense_units_1'],\n",
        "            kernel_regularizer=l1_l2(l1=config['l2_reg']*0.5, l2=config['l2_reg'])\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(config['dropout_rate'])(x)\n",
        "\n",
        "        x = Dense(\n",
        "            config['dense_units_2'],\n",
        "            kernel_regularizer=l1_l2(l1=config['l2_reg']*0.5, l2=config['l2_reg'])\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(config['dropout_rate'])(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(\n",
        "            self.num_classes,\n",
        "            activation='softmax',\n",
        "            kernel_regularizer=l2(config['l2_reg']),\n",
        "            name='classification_output'\n",
        "        )(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LungSoundClassifier')\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        \"\"\"Compute class weights for balanced training\"\"\"\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(f\"üìä Class weights computed:\")\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        for i, weight in self.class_weights.items():\n",
        "            print(f\"   ‚Ä¢ {class_names[i]}: {weight:.3f}\")\n",
        "\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='lung_sound_model'):\n",
        "        \"\"\"Create training callbacks\"\"\"\n",
        "\n",
        "        def scheduler(epoch, lr):\n",
        "            if epoch < 20:\n",
        "                return lr\n",
        "            elif epoch < 50:\n",
        "                return lr * 0.5\n",
        "            elif epoch < 80:\n",
        "                return lr * 0.25\n",
        "            else:\n",
        "                return lr * 0.1\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=25,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.0001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=8,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            ),\n",
        "            CSVLogger(f'{model_name}_training_log.csv', append=True),\n",
        "            LearningRateScheduler(scheduler, verbose=0)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.001):\n",
        "        \"\"\"Compile model with optimizer and metrics - FIXED VERSION\"\"\"\n",
        "\n",
        "        # Fixed F1 Score metric using Keras backend operations\n",
        "        def f1_score_metric(y_true, y_pred):\n",
        "            \"\"\"F1 score metric that works with TensorFlow's computation graph\"\"\"\n",
        "            # Convert predictions to class indices\n",
        "            y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "            y_true_classes = tf.argmax(y_true, axis=1)\n",
        "\n",
        "            # Calculate confusion matrix components\n",
        "            tp = tf.reduce_sum(tf.cast(\n",
        "                tf.logical_and(\n",
        "                    tf.equal(y_true_classes, y_pred_classes),\n",
        "                    tf.equal(y_true_classes, 1)  # Assuming class 1 for binary-like F1\n",
        "                ), tf.float32))\n",
        "\n",
        "            fp = tf.reduce_sum(tf.cast(\n",
        "                tf.logical_and(\n",
        "                    tf.not_equal(y_true_classes, y_pred_classes),\n",
        "                    tf.equal(y_pred_classes, 1)\n",
        "                ), tf.float32))\n",
        "\n",
        "            fn = tf.reduce_sum(tf.cast(\n",
        "                tf.logical_and(\n",
        "                    tf.not_equal(y_true_classes, y_pred_classes),\n",
        "                    tf.equal(y_true_classes, 1)\n",
        "                ), tf.float32))\n",
        "\n",
        "            # Calculate precision and recall\n",
        "            precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "            recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "            # Calculate F1 score\n",
        "            f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "            return f1\n",
        "\n",
        "        # Simpler categorical accuracy as alternative\n",
        "        def weighted_categorical_accuracy(y_true, y_pred):\n",
        "            \"\"\"Alternative metric that's more stable\"\"\"\n",
        "            return tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "        optimizer = AdamW(\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=0.01,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        # Use standard metrics that are guaranteed to work\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'),\n",
        "                weighted_categorical_accuracy\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val,\n",
        "              epochs=100, batch_size=32, config=None):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "\n",
        "        print(\"üöÄ Starting training...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Input shape: {x_train.shape[1:]}\")\n",
        "\n",
        "        # Create and compile model\n",
        "        self.model = self.create_model(config)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = self.compute_class_weights(y_train)\n",
        "\n",
        "        # Create callbacks\n",
        "        callbacks = self.create_callbacks()\n",
        "\n",
        "        # Print model summary\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "        self.model.summary()\n",
        "\n",
        "        # Train model\n",
        "        self.history = self.model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Metrics\n",
        "        evaluation_results = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_loss = evaluation_results[0]\n",
        "        test_acc = evaluation_results[1]\n",
        "        test_prec = evaluation_results[2] if len(evaluation_results) > 2 else 0\n",
        "        test_rec = evaluation_results[3] if len(evaluation_results) > 3 else 0\n",
        "\n",
        "        # Calculate F1 score manually using sklearn\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Set Performance:\")\n",
        "        print(f\"   ‚Ä¢ Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Test Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Test Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Test F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        # Classification report\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Detailed Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix - Test Set')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history available!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train Accuracy', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val Accuracy', color='orange')\n",
        "        axes[0, 0].set_title('Model Accuracy')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train Loss', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val Loss', color='orange')\n",
        "        axes[0, 1].set_title('Model Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        if 'precision' in self.history.history:\n",
        "            axes[1, 0].plot(self.history.history['precision'], label='Train Precision', color='blue')\n",
        "            axes[1, 0].plot(self.history.history['val_precision'], label='Val Precision', color='orange')\n",
        "            axes[1, 0].set_title('Precision')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Precision')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning Rate\n",
        "        if 'lr' in self.history.history:\n",
        "            axes[1, 1].plot(self.history.history['lr'], label='Learning Rate', color='green')\n",
        "            axes[1, 1].set_title('Learning Rate Schedule')\n",
        "            axes[1, 1].set_xlabel('Epoch')\n",
        "            axes[1, 1].set_ylabel('Learning Rate')\n",
        "            axes[1, 1].legend()\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            axes[1, 1].set_yscale('log')\n",
        "        else:\n",
        "            # Plot recall if learning rate not available\n",
        "            if 'recall' in self.history.history:\n",
        "                axes[1, 1].plot(self.history.history['recall'], label='Train Recall', color='blue')\n",
        "                axes[1, 1].plot(self.history.history['val_recall'], label='Val Recall', color='orange')\n",
        "                axes[1, 1].set_title('Recall')\n",
        "                axes[1, 1].set_xlabel('Epoch')\n",
        "                axes[1, 1].set_ylabel('Recall')\n",
        "                axes[1, 1].legend()\n",
        "                axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Training Pipeline - Ready to use with your data\n",
        "def train_lung_sound_classifier(x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru):\n",
        "    \"\"\"\n",
        "    Complete training pipeline for your lung sound data\n",
        "\n",
        "    Parameters:\n",
        "    - x_train_gru: (1181, 1, 959) - Training features\n",
        "    - y_train_gru: (1181, 3) - Training labels (one-hot encoded)\n",
        "    - x_val_gru: (275, 1, 959) - Validation features\n",
        "    - y_val_gru: (275, 3) - Validation labels\n",
        "    - x_test_gru: (119, 1, 959) - Test features\n",
        "    - y_test_gru: (119, 3) - Test labels\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Lung Sound Classification Training Pipeline - FIXED VERSION\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    # Verify data shapes\n",
        "    print(f\"üìä Data shapes:\")\n",
        "    print(f\"   ‚Ä¢ Train: {x_train_gru.shape} -> {y_train_gru.shape}\")\n",
        "    print(f\"   ‚Ä¢ Val:   {x_val_gru.shape} -> {y_val_gru.shape}\")\n",
        "    print(f\"   ‚Ä¢ Test:  {x_test_gru.shape} -> {y_test_gru.shape}\")\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = OptimizedLungSoundClassifier(input_shape=(1, 959), num_classes=3)\n",
        "\n",
        "    # Configuration for your dataset\n",
        "    config = {\n",
        "        'conv_filters': 64,\n",
        "        'gru_units_1': 96,        # Reduced for better generalization\n",
        "        'gru_units_2': 48,        # Reduced for better generalization\n",
        "        'dense_units_1': 128,     # Reduced to prevent overfitting\n",
        "        'dense_units_2': 64,      # Reduced to prevent overfitting\n",
        "        'dropout_rate': 0.5,      # Higher dropout for regularization\n",
        "        'l2_reg': 0.01,           # Strong L2 regularization\n",
        "        'use_bidirectional': True,\n",
        "        'use_conv1d': True,\n",
        "        'pooling_strategy': 'both'\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"\\nüöÄ Starting training with optimized configuration...\")\n",
        "    history = classifier.train(\n",
        "        x_train_gru, y_train_gru,\n",
        "        x_val_gru, y_val_gru,\n",
        "        epochs=150,     # Sufficient epochs with early stopping\n",
        "        batch_size=16,  # Smaller batch size for stable training\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(f\"\\nüß™ Evaluating on test set...\")\n",
        "    test_results = classifier.evaluate_model(x_test_gru, y_test_gru)\n",
        "\n",
        "    # Plot training history\n",
        "    print(f\"\\nüìà Plotting training history...\")\n",
        "    classifier.plot_training_history()\n",
        "\n",
        "    return classifier, history, test_results\n",
        "\n",
        "\n",
        "# Simple usage example with placeholder variables\n",
        "# Replace these with your actual variable names\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the fixed training pipeline\n",
        "    \"\"\"\n",
        "    print(\"üìù Example Usage:\")\n",
        "    print(\"1. Make sure your data variables are loaded:\")\n",
        "    print(\"   - x_train_gru, y_train_gru (training data)\")\n",
        "    print(\"   - x_val_gru, y_val_gru (validation data)\")\n",
        "    print(\"   - x_test_gru, y_test_gru (test data)\")\n",
        "    print(\"\\n2. Run the training pipeline:\")\n",
        "    print(\"   classifier, history, results = train_lung_sound_classifier(\")\n",
        "    print(\"       x_train_gru, y_train_gru,\")\n",
        "    print(\"       x_val_gru, y_val_gru,\")\n",
        "    print(\"       x_test_gru, y_test_gru\")\n",
        "    print(\"   )\")\n",
        "    print(\"\\n3. Check results:\")\n",
        "    print(\"   print(f'Final Test Accuracy: {results[\\\"accuracy\\\"]:.4f}')\")\n",
        "    print(\"   print(f'Final Test F1-Score: {results[\\\"f1_score\\\"]:.4f}')\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()\n",
        "\n",
        "    # Uncomment the lines below when you have your data ready:\n",
        "    model, history, results = train_lung_sound_classifier(\n",
        "        x_train_gru, y_train_gru,\n",
        "        x_val_gru, y_val_gru,\n",
        "        x_test_gru, y_test_gru\n",
        "    )\n",
        "\n",
        "    # Expected performance: 85-95% accuracy with good generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FIZNT5MTkb46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3xrORFVyyig"
      },
      "source": [
        "# üìä ROC & AUC Analysis ‚Äì Lung Sound Classification\n",
        "\n",
        "This module provides **visual diagnostic tools** to evaluate your trained classifier‚Äôs performance using **ROC Curves** and **AUC scores** for each class.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Features\n",
        "\n",
        "- üéØ **Main ROC Curve** (One-vs-Rest) with Micro & Macro averages  \n",
        "- üî¨ **Detailed Subplot Analysis** (1-vs-Rest per class + Combined View)  \n",
        "- üìà **AUC Comparison Bar Chart**  \n",
        "- üì¶ Integrated function for complete analysis in 1 line  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mo_UXtGwyx7b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_roc_curves(classifier, x_test_gru, y_test_gru, class_names=None):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for multi-class classification using OptimizedLungSoundClassifier\n",
        "\n",
        "    Parameters:\n",
        "    - classifier: Trained OptimizedLungSoundClassifier object\n",
        "    - x_test_gru: Test features (119, 1, 959)\n",
        "    - y_test_gru: Test labels one-hot encoded (119, 3)\n",
        "    - class_names: List of class names (default: [\"Normal\", \"Asthma\", \"COPD\"])\n",
        "    \"\"\"\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [\"Normal\", \"Asthma\", \"COPD\"]\n",
        "\n",
        "    n_classes = len(class_names)\n",
        "\n",
        "    print(\"üéØ Generating ROC Curves...\")\n",
        "    print(f\"   ‚Ä¢ Test samples: {x_test_gru.shape[0]}\")\n",
        "    print(f\"   ‚Ä¢ Classes: {class_names}\")\n",
        "\n",
        "    # Get prediction probabilities using the correct method\n",
        "    if hasattr(classifier, 'model') and classifier.model is not None:\n",
        "        y_score = classifier.model.predict(x_test_gru, verbose=0)\n",
        "    else:\n",
        "        raise ValueError(\"Classifier model not found. Make sure the model is trained.\")\n",
        "\n",
        "    y_true = y_test_gru  # Already one-hot encoded\n",
        "\n",
        "    print(f\"   ‚Ä¢ Prediction shape: {y_score.shape}\")\n",
        "    print(f\"   ‚Ä¢ True labels shape: {y_true.shape}\")\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Compute macro-average ROC curve and ROC area\n",
        "    # First aggregate all false positive rates\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes\n",
        "\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Define colors for each class\n",
        "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
        "\n",
        "    # Plot ROC curve for each class\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
        "\n",
        "    # Plot micro-average ROC curve\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "             label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n",
        "             color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "    # Plot macro-average ROC curve\n",
        "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "             label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n",
        "             color='navy', linestyle=':', linewidth=3)\n",
        "\n",
        "    # Plot diagonal (random classifier)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.8, label='Random Classifier')\n",
        "\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=14)\n",
        "    plt.ylabel('True Positive Rate', fontsize=14)\n",
        "    plt.title('ROC Curves - Lung Sound Classification\\n(One-vs-Rest)', fontsize=16, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print AUC scores\n",
        "    print(f\"\\nüìä AUC Scores Summary:\")\n",
        "    print(\"=\" * 40)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        performance = \"Excellent\" if roc_auc[i] > 0.9 else \"Good\" if roc_auc[i] > 0.8 else \"Fair\" if roc_auc[i] > 0.7 else \"Poor\"\n",
        "        print(f\"   ‚Ä¢ {class_name:8}: {roc_auc[i]:.3f} ({performance})\")\n",
        "\n",
        "    print(f\"   ‚Ä¢ {'Micro-avg':8}: {roc_auc['micro']:.3f}\")\n",
        "    print(f\"   ‚Ä¢ {'Macro-avg':8}: {roc_auc['macro']:.3f}\")\n",
        "\n",
        "    # Overall AUC using sklearn's built-in function (alternative calculation)\n",
        "    try:\n",
        "        overall_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "        print(f\"   ‚Ä¢ {'Weighted':8}: {overall_auc:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚Ä¢ Weighted calculation failed: {e}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc\n",
        "\n",
        "\n",
        "def plot_detailed_roc_analysis(classifier, x_test_gru, y_test_gru, class_names=None):\n",
        "    \"\"\"\n",
        "    Create a detailed ROC analysis with individual plots for each class\n",
        "    \"\"\"\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [\"Normal\", \"Asthma\", \"COPD\"]\n",
        "\n",
        "    n_classes = len(class_names)\n",
        "\n",
        "    # Get predictions\n",
        "    y_score = classifier.model.predict(x_test_gru, verbose=0)\n",
        "    y_true = y_test_gru\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Detailed ROC Analysis - Lung Sound Classification', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Individual ROC curves\n",
        "    colors = ['blue', 'red', 'green']\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        row = i // 2\n",
        "        col = i % 2\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        axes[row, col].plot(fpr, tpr, color=colors[i], lw=3,\n",
        "                           label=f'{class_names[i]} (AUC = {roc_auc:.3f})')\n",
        "        axes[row, col].plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.6)\n",
        "        axes[row, col].set_xlim([0.0, 1.0])\n",
        "        axes[row, col].set_ylim([0.0, 1.05])\n",
        "        axes[row, col].set_xlabel('False Positive Rate', fontsize=12)\n",
        "        axes[row, col].set_ylabel('True Positive Rate', fontsize=12)\n",
        "        axes[row, col].set_title(f'{class_names[i]} vs Rest', fontsize=14, fontweight='bold')\n",
        "        axes[row, col].legend(loc=\"lower right\", fontsize=11)\n",
        "        axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add text box with additional metrics\n",
        "        axes[row, col].text(0.6, 0.2, f'AUC: {roc_auc:.3f}\\nSamples: {np.sum(y_true[:, i])}',\n",
        "                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n",
        "                           fontsize=10)\n",
        "\n",
        "    # Combined plot in the last subplot\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        axes[1, 1].plot(fpr, tpr, color=colors[i], lw=2,\n",
        "                       label=f'{class_names[i]} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    axes[1, 1].plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.6, label='Random')\n",
        "    axes[1, 1].set_xlim([0.0, 1.0])\n",
        "    axes[1, 1].set_ylim([0.0, 1.05])\n",
        "    axes[1, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
        "    axes[1, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
        "    axes[1, 1].set_title('All Classes Combined', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].legend(loc=\"lower right\", fontsize=10)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_auc_comparison(classifier, x_test_gru, y_test_gru, class_names=None):\n",
        "    \"\"\"\n",
        "    Create a bar plot comparing AUC scores across classes\n",
        "    \"\"\"\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [\"Normal\", \"Asthma\", \"COPD\"]\n",
        "\n",
        "    # Get predictions and calculate AUC for each class\n",
        "    y_score = classifier.model.predict(x_test_gru, verbose=0)\n",
        "    y_true = y_test_gru\n",
        "\n",
        "    auc_scores = []\n",
        "    for i in range(len(class_names)):\n",
        "        fpr, tpr, _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        auc_scores.append(auc(fpr, tpr))\n",
        "\n",
        "    # Create bar plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(class_names, auc_scores,\n",
        "                   color=['skyblue', 'lightcoral', 'lightgreen'],\n",
        "                   alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars, auc_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.ylabel('AUC Score', fontsize=14)\n",
        "    plt.xlabel('Class', fontsize=14)\n",
        "    plt.title('AUC Score Comparison Across Classes', fontsize=16, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add horizontal line at 0.5 (random performance)\n",
        "    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random Performance')\n",
        "    plt.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='Good Performance')\n",
        "    plt.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='Excellent Performance')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return auc_scores\n",
        "\n",
        "\n",
        "# Complete ROC Analysis Function\n",
        "def complete_roc_analysis(classifier, x_test_gru, y_test_gru, class_names=None):\n",
        "    \"\"\"\n",
        "    Run complete ROC analysis with all visualizations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Starting Complete ROC Analysis...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. Main ROC curves plot\n",
        "    print(\"\\n1Ô∏è‚É£ Generating main ROC curves...\")\n",
        "    fpr, tpr, roc_auc = plot_roc_curves(classifier, x_test_gru, y_test_gru, class_names)\n",
        "\n",
        "    # 2. Detailed analysis\n",
        "    print(\"\\n2Ô∏è‚É£ Generating detailed ROC analysis...\")\n",
        "    plot_detailed_roc_analysis(classifier, x_test_gru, y_test_gru, class_names)\n",
        "\n",
        "    # 3. AUC comparison\n",
        "    print(\"\\n3Ô∏è‚É£ Generating AUC comparison...\")\n",
        "    auc_scores = plot_auc_comparison(classifier, x_test_gru, y_test_gru, class_names)\n",
        "\n",
        "    print(\"\\n‚úÖ ROC Analysis Complete!\")\n",
        "\n",
        "    return fpr, tpr, roc_auc, auc_scores\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# After training your model:\n",
        "# classifier, history, results = train_lung_sound_classifier(\n",
        "#     x_train_gru, y_train_gru,\n",
        "#     x_val_gru, y_val_gru,\n",
        "#     x_test_gru, y_test_gru\n",
        "# )\n",
        "\n",
        "# Generate ROC curves and AUC analysis:\n",
        "fpr, tpr, roc_auc, auc_scores = complete_roc_analysis(\n",
        "    model, x_test_gru, y_test_gru,\n",
        "    class_names=[\"Normal\", \"Asthma\", \"COPD\"]\n",
        ")\n",
        "\n",
        "# Or just the main ROC plot:\n",
        "plot_roc_curves(model, x_test_gru, y_test_gru)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3SqbPudrw17"
      },
      "source": [
        "# ü´Å Lung Sound Classification System ‚Äì For Unseen Audio Data\n",
        "\n",
        "A complete prediction pipeline using a pre-trained deep learning model for **asthma, COPD, and normal** lung sound detection from audio files.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Components\n",
        "\n",
        "- Loads trained Keras model (`.keras`)\n",
        "- Loads `StandardScaler` for feature normalization\n",
        "- Extracts advanced, multi-modal audio features\n",
        "- Supports both **single** and **batch predictions**\n",
        "- Gives **confidence levels**, **visual summaries**, and **class probabilities**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Class: `LungSoundPredictor`\n",
        "\n",
        "```python\n",
        "predictor = LungSoundPredictor(\n",
        "    model_path='lung_sound_model_best.keras',\n",
        "    scaler_path='scaler.pkl'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VD37ZYmLrbJp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from scipy.signal import hilbert\n",
        "import pywt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define the custom metric that was used during training\n",
        "@register_keras_serializable()\n",
        "def weighted_categorical_accuracy(y_true, y_pred):\n",
        "    \"\"\"Custom weighted categorical accuracy metric\"\"\"\n",
        "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "class LungSoundPredictor:\n",
        "    \"\"\"\n",
        "    Complete Lung Sound Classification System for Unseen Data\n",
        "    Supports: ASTHMA, COPD, NORMAL classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path='lung_sound_model_best.keras', scaler_path='scaler.pkl'):\n",
        "        \"\"\"\n",
        "        Initialize the predictor with trained model and scaler\n",
        "\n",
        "        Parameters:\n",
        "        - model_path: Path to saved Keras model\n",
        "        - scaler_path: Path to saved StandardScaler\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.scaler_path = scaler_path\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.class_names = ['Normal', 'Asthma', 'COPD']\n",
        "\n",
        "        # Load model and scaler\n",
        "        self.load_model_and_scaler()\n",
        "\n",
        "    def load_model_and_scaler(self):\n",
        "        \"\"\"Load the trained model and feature scaler\"\"\"\n",
        "        try:\n",
        "            # Load the trained model with custom objects\n",
        "            if os.path.exists(self.model_path):\n",
        "                custom_objects = {\n",
        "                    'weighted_categorical_accuracy': weighted_categorical_accuracy\n",
        "                }\n",
        "                self.model = load_model(self.model_path, custom_objects=custom_objects)\n",
        "                print(f\"‚úÖ Model loaded successfully from: {self.model_path}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Model file not found: {self.model_path}\")\n",
        "                print(\"Please ensure you have trained and saved the model first.\")\n",
        "                return False\n",
        "\n",
        "            # Load the scaler\n",
        "            if os.path.exists(self.scaler_path):\n",
        "                self.scaler = joblib.load(self.scaler_path)\n",
        "                print(f\"‚úÖ Scaler loaded successfully from: {self.scaler_path}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Scaler file not found: {self.scaler_path}\")\n",
        "                print(\"Please ensure you have the scaler.pkl file from training.\")\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model or scaler: {e}\")\n",
        "\n",
        "            # Alternative loading method - compile=False\n",
        "            try:\n",
        "                print(\"üîÑ Trying alternative loading method...\")\n",
        "                self.model = load_model(self.model_path, compile=False)\n",
        "\n",
        "                # Recompile the model with standard metrics\n",
        "                self.model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                print(f\"‚úÖ Model loaded successfully with alternative method\")\n",
        "\n",
        "                # Load scaler\n",
        "                if os.path.exists(self.scaler_path):\n",
        "                    self.scaler = joblib.load(self.scaler_path)\n",
        "                    print(f\"‚úÖ Scaler loaded successfully from: {self.scaler_path}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"‚ùå Scaler file not found: {self.scaler_path}\")\n",
        "                    return False\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"‚ùå Alternative loading method also failed: {e2}\")\n",
        "                return False\n",
        "\n",
        "    # Feature extraction functions (same as your training code)\n",
        "    def add_noise(self, data, noise_level=0.005):\n",
        "        \"\"\"Add Gaussian noise to audio data with improved stability.\"\"\"\n",
        "        if len(data) == 0:\n",
        "            return data\n",
        "        noise = np.random.randn(len(data)) * noise_level\n",
        "        noisy_data = data + noise\n",
        "        return np.clip(noisy_data, -1.0, 1.0)\n",
        "\n",
        "    def extract_advanced_mfcc_features(self, data, sampling_rate, n_mfcc=40):\n",
        "        \"\"\"Extract MFCCs with delta and delta-delta features plus statistical moments.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_mfcc * 9)\n",
        "\n",
        "            # Extract MFCCs\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mfcc=n_mfcc,\n",
        "                n_fft=2048,\n",
        "                hop_length=512\n",
        "            )\n",
        "\n",
        "            if mfccs.shape[1] == 0:\n",
        "                return np.zeros(n_mfcc * 9)\n",
        "\n",
        "            # Compute Delta and Delta-Delta\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            # Statistical moments\n",
        "            mfcc_mean = np.mean(mfccs, axis=1)\n",
        "            mfcc_std = np.std(mfccs, axis=1)\n",
        "            mfcc_skew = stats.skew(mfccs, axis=1)\n",
        "\n",
        "            delta_mean = np.mean(delta_mfccs, axis=1)\n",
        "            delta_std = np.std(delta_mfccs, axis=1)\n",
        "            delta_skew = stats.skew(delta_mfccs, axis=1)\n",
        "\n",
        "            delta2_mean = np.mean(delta2_mfccs, axis=1)\n",
        "            delta2_std = np.std(delta2_mfccs, axis=1)\n",
        "            delta2_skew = stats.skew(delta2_mfccs, axis=1)\n",
        "\n",
        "            # Combine all MFCC-based features\n",
        "            advanced_mfcc_features = np.concatenate([\n",
        "                mfcc_mean, mfcc_std, mfcc_skew,\n",
        "                delta_mean, delta_std, delta_skew,\n",
        "                delta2_mean, delta2_std, delta2_skew\n",
        "            ])\n",
        "\n",
        "            return np.nan_to_num(advanced_mfcc_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Advanced MFCC extraction failed: {e}\")\n",
        "            return np.zeros(n_mfcc * 9)\n",
        "\n",
        "    def extract_fbse_features(self, data, sampling_rate, n_bands=10):\n",
        "        \"\"\"Extract Fourier-Bessel Spectral Entropy features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_bands)\n",
        "\n",
        "            # Compute power spectral density\n",
        "            stft = librosa.stft(data)\n",
        "            psd = np.abs(stft)**2\n",
        "\n",
        "            # Divide frequency range into bands\n",
        "            freq_bands = np.linspace(0, sampling_rate//2, n_bands + 1)\n",
        "            entropy_features = []\n",
        "\n",
        "            for i in range(n_bands):\n",
        "                start_idx = int(freq_bands[i] * len(psd) / (sampling_rate//2))\n",
        "                end_idx = int(freq_bands[i+1] * len(psd) / (sampling_rate//2))\n",
        "\n",
        "                if end_idx > start_idx:\n",
        "                    band_psd = np.mean(psd[start_idx:end_idx], axis=0)\n",
        "                    band_psd_norm = band_psd / (np.sum(band_psd) + 1e-10)\n",
        "                    entropy = -np.sum(band_psd_norm * np.log(band_psd_norm + 1e-10))\n",
        "                    entropy_features.append(np.mean(entropy))\n",
        "                else:\n",
        "                    entropy_features.append(0.0)\n",
        "\n",
        "            return np.nan_to_num(np.array(entropy_features), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: FBSE extraction failed: {e}\")\n",
        "            return np.zeros(n_bands)\n",
        "\n",
        "    def extract_enhanced_melspectrogram(self, data, sampling_rate, n_mels=128):\n",
        "        \"\"\"Extract enhanced Mel-spectrogram features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_mels * 4)\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mels=n_mels,\n",
        "                n_fft=2048,\n",
        "                hop_length=512,\n",
        "                fmax=sampling_rate//2\n",
        "            )\n",
        "\n",
        "            if mel_spec.size == 0:\n",
        "                return np.zeros(n_mels * 4)\n",
        "\n",
        "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            if mel_spec_db.shape[1] > 0:\n",
        "                mel_mean = np.mean(mel_spec_db, axis=1)\n",
        "                mel_std = np.std(mel_spec_db, axis=1)\n",
        "                mel_max = np.max(mel_spec_db, axis=1)\n",
        "                mel_min = np.min(mel_spec_db, axis=1)\n",
        "                mel_features = np.concatenate([mel_mean, mel_std, mel_max, mel_min])\n",
        "            else:\n",
        "                mel_features = np.zeros(n_mels * 4)\n",
        "\n",
        "            return np.nan_to_num(mel_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Enhanced Mel-spectrogram extraction failed: {e}\")\n",
        "            return np.zeros(n_mels * 4)\n",
        "\n",
        "    def extract_wavelet_features(self, data, wavelet='db4', levels=5):\n",
        "        \"\"\"Extract wavelet features for transient detection.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(levels * 4 + 4)\n",
        "\n",
        "            coeffs = pywt.wavedec(data, wavelet, level=levels)\n",
        "            wavelet_features = []\n",
        "\n",
        "            for coeff in coeffs:\n",
        "                if len(coeff) > 0:\n",
        "                    wavelet_features.extend([\n",
        "                        np.mean(np.abs(coeff)),\n",
        "                        np.std(coeff),\n",
        "                        np.max(np.abs(coeff)),\n",
        "                        np.sum(coeff**2)\n",
        "                    ])\n",
        "                else:\n",
        "                    wavelet_features.extend([0.0, 0.0, 0.0, 0.0])\n",
        "\n",
        "            return np.nan_to_num(np.array(wavelet_features), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Wavelet feature extraction failed: {e}\")\n",
        "            return np.zeros(levels * 4 + 4)\n",
        "\n",
        "    def extract_sequence_features(self, data, sampling_rate, frame_length=2048, hop_length=512):\n",
        "        \"\"\"Extract sequence-based features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(26)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mfcc=13,\n",
        "                n_fft=frame_length,\n",
        "                hop_length=hop_length\n",
        "            )\n",
        "\n",
        "            if mfccs.shape[1] == 0:\n",
        "                return np.zeros(26)\n",
        "\n",
        "            frame_variations = np.mean(np.abs(np.diff(mfccs, axis=1)), axis=1)\n",
        "            long_term_mean = np.mean(mfccs, axis=1)\n",
        "            sequence_features = np.concatenate([frame_variations, long_term_mean])\n",
        "\n",
        "            return np.nan_to_num(sequence_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Sequence feature extraction failed: {e}\")\n",
        "            return np.zeros(26)\n",
        "\n",
        "    def extract_spectral_features(self, data, sampling_rate):\n",
        "        \"\"\"Extract spectral features with improved tonnetz handling.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(7)\n",
        "\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sampling_rate)\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate)\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=sampling_rate)\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
        "            zero_crossing_rate = librosa.feature.zero_crossing_rate(data)\n",
        "            chroma = librosa.feature.chroma_stft(y=data, sr=sampling_rate)\n",
        "            chroma_mean = np.mean(chroma)\n",
        "\n",
        "            # Improved tonnetz handling\n",
        "            try:\n",
        "                if sampling_rate >= 8000:\n",
        "                    tonnetz = librosa.feature.tonnetz(y=data, sr=sampling_rate)\n",
        "                    tonnetz_mean = np.mean(tonnetz)\n",
        "                elif sampling_rate >= 4000:\n",
        "                    chroma_cqt = librosa.feature.chroma_cqt(\n",
        "                        y=data,\n",
        "                        sr=sampling_rate,\n",
        "                        fmin=librosa.note_to_hz('C1'),\n",
        "                        n_chroma=12\n",
        "                    )\n",
        "                    tonnetz_mean = np.mean(chroma_cqt) * 0.5\n",
        "                else:\n",
        "                    tonnetz_mean = 0.0\n",
        "            except:\n",
        "                tonnetz_mean = 0.0\n",
        "\n",
        "            spectral_features = np.array([\n",
        "                np.mean(spectral_centroid),\n",
        "                np.mean(spectral_bandwidth),\n",
        "                np.mean(spectral_rolloff),\n",
        "                np.mean(spectral_flatness),\n",
        "                np.mean(zero_crossing_rate),\n",
        "                chroma_mean,\n",
        "                tonnetz_mean\n",
        "            ])\n",
        "\n",
        "            return np.nan_to_num(spectral_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Spectral feature extraction failed: {e}\")\n",
        "            return np.zeros(7)\n",
        "\n",
        "    def fourier_bessel_features(self, data, sampling_rate, n_coeff=20):\n",
        "        \"\"\"Enhanced Fourier-Bessel feature extraction.\"\"\"\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(n_coeff)\n",
        "\n",
        "        t = np.arange(len(data)) / sampling_rate\n",
        "        fb_coeff = np.zeros(n_coeff)\n",
        "        t_norm = t / np.max(t) if np.max(t) > 0 else t\n",
        "\n",
        "        for i in range(n_coeff):\n",
        "            j = i + 1\n",
        "            cosine_term = np.cos(2 * np.pi * j * t_norm)\n",
        "            fb_coeff[i] = np.sum(data * cosine_term) / len(data)\n",
        "\n",
        "        return np.nan_to_num(fb_coeff, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    def extract_features_from_audio(self, audio_file_path):\n",
        "        \"\"\"\n",
        "        Extract all features from a single audio file\n",
        "\n",
        "        Parameters:\n",
        "        - audio_file_path: Path to the audio file\n",
        "\n",
        "        Returns:\n",
        "        - features: Normalized feature vector ready for prediction\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            data, sampling_rate = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "            if len(data) == 0:\n",
        "                print(f\"‚ùå Empty audio file: {audio_file_path}\")\n",
        "                return None\n",
        "\n",
        "            print(f\"üéµ Processing: {os.path.basename(audio_file_path)}\")\n",
        "            print(f\"   ‚Ä¢ Duration: {len(data)/sampling_rate:.2f}s\")\n",
        "            print(f\"   ‚Ä¢ Sampling Rate: {sampling_rate}Hz\")\n",
        "\n",
        "            # Extract all features (same as training)\n",
        "            n_mfcc = 40\n",
        "            fb_coeffs = 20\n",
        "            n_mels = 128\n",
        "            wavelet_levels = 5\n",
        "            fbse_bands = 10\n",
        "\n",
        "            # Extract each feature type\n",
        "            advanced_mfcc_features = self.extract_advanced_mfcc_features(data, sampling_rate, n_mfcc)\n",
        "            fbse_features = self.extract_fbse_features(data, sampling_rate, fbse_bands)\n",
        "            enhanced_mel_features = self.extract_enhanced_melspectrogram(data, sampling_rate, n_mels)\n",
        "            wavelet_features = self.extract_wavelet_features(data, 'db4', wavelet_levels)\n",
        "            sequence_features = self.extract_sequence_features(data, sampling_rate)\n",
        "            spectral_features = self.extract_spectral_features(data, sampling_rate)\n",
        "            fb_features = self.fourier_bessel_features(data, sampling_rate, fb_coeffs)\n",
        "\n",
        "            # Combine all features\n",
        "            combined_features = np.concatenate([\n",
        "                advanced_mfcc_features,\n",
        "                fbse_features,\n",
        "                enhanced_mel_features,\n",
        "                wavelet_features,\n",
        "                sequence_features,\n",
        "                spectral_features,\n",
        "                fb_features\n",
        "            ])\n",
        "\n",
        "            # Final validation\n",
        "            combined_features = np.nan_to_num(combined_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            print(f\"   ‚Ä¢ Features extracted: {len(combined_features)} dimensions\")\n",
        "\n",
        "            # Normalize using training scaler\n",
        "            if self.scaler is not None:\n",
        "                features_normalized = self.scaler.transform(combined_features.reshape(1, -1))\n",
        "                return features_normalized[0]\n",
        "            else:\n",
        "                print(\"‚ùå Scaler not loaded. Cannot normalize features.\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extracting features from {audio_file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_single_file(self, audio_file_path, show_confidence=True):\n",
        "        \"\"\"\n",
        "        Predict lung condition for a single audio file\n",
        "\n",
        "        Parameters:\n",
        "        - audio_file_path: Path to the audio file\n",
        "        - show_confidence: Whether to show confidence scores\n",
        "\n",
        "        Returns:\n",
        "        - prediction_result: Dictionary with prediction details\n",
        "        \"\"\"\n",
        "        if self.model is None or self.scaler is None:\n",
        "            print(\"‚ùå Model or scaler not loaded properly.\")\n",
        "            return None\n",
        "\n",
        "        # Extract features\n",
        "        features = self.extract_features_from_audio(audio_file_path)\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Reshape for model input (1, 1, feature_size) format\n",
        "            features_reshaped = features.reshape(1, 1, -1)\n",
        "\n",
        "            print(f\"   ‚Ä¢ Input shape: {features_reshaped.shape}\")\n",
        "\n",
        "            # Make prediction\n",
        "            prediction_proba = self.model.predict(features_reshaped, verbose=0)\n",
        "            predicted_class_idx = np.argmax(prediction_proba[0])\n",
        "            predicted_class = self.class_names[predicted_class_idx]\n",
        "            confidence = prediction_proba[0][predicted_class_idx]\n",
        "\n",
        "            # Prepare result\n",
        "            result = {\n",
        "                'file': os.path.basename(audio_file_path),\n",
        "                'predicted_class': predicted_class,\n",
        "                'confidence': confidence,\n",
        "                'all_probabilities': {\n",
        "                    self.class_names[i]: prediction_proba[0][i]\n",
        "                    for i in range(len(self.class_names))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Display results\n",
        "            print(f\"\\nüéØ Prediction Results:\")\n",
        "            print(f\"   ‚Ä¢ File: {result['file']}\")\n",
        "            print(f\"   ‚Ä¢ Predicted Class: {predicted_class}\")\n",
        "            print(f\"   ‚Ä¢ Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "            if show_confidence:\n",
        "                print(f\"   ‚Ä¢ Detailed Probabilities:\")\n",
        "                for class_name, prob in result['all_probabilities'].items():\n",
        "                    print(f\"     - {class_name}: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "\n",
        "            # Confidence interpretation\n",
        "            if confidence >= 0.8:\n",
        "                confidence_level = \"High\"\n",
        "                emoji = \"üü¢\"\n",
        "            elif confidence >= 0.6:\n",
        "                confidence_level = \"Medium\"\n",
        "                emoji = \"üü°\"\n",
        "            else:\n",
        "                confidence_level = \"Low\"\n",
        "                emoji = \"üî¥\"\n",
        "\n",
        "            print(f\"   ‚Ä¢ Confidence Level: {emoji} {confidence_level}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_multiple_files(self, audio_files_list):\n",
        "        \"\"\"\n",
        "        Predict lung conditions for multiple audio files\n",
        "\n",
        "        Parameters:\n",
        "        - audio_files_list: List of audio file paths\n",
        "\n",
        "        Returns:\n",
        "        - results: List of prediction results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(f\"üîÑ Processing {len(audio_files_list)} audio files...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for i, audio_file in enumerate(audio_files_list):\n",
        "            print(f\"\\n[{i+1}/{len(audio_files_list)}] Processing: {os.path.basename(audio_file)}\")\n",
        "\n",
        "            result = self.predict_single_file(audio_file, show_confidence=False)\n",
        "            if result is not None:\n",
        "                results.append(result)\n",
        "\n",
        "        # Summary\n",
        "        if results:\n",
        "            print(f\"\\nüìä Summary of {len(results)} successful predictions:\")\n",
        "            class_counts = {}\n",
        "            for result in results:\n",
        "                pred_class = result['predicted_class']\n",
        "                class_counts[pred_class] = class_counts.get(pred_class, 0) + 1\n",
        "\n",
        "            for class_name, count in class_counts.items():\n",
        "                percentage = (count / len(results)) * 100\n",
        "                print(f\"   ‚Ä¢ {class_name}: {count} files ({percentage:.1f}%)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_model_info(self):\n",
        "        \"\"\"Display information about the loaded model\"\"\"\n",
        "        if self.model is not None:\n",
        "            print(\"üè• Model Information:\")\n",
        "            print(f\"   ‚Ä¢ Input shape: {self.model.input_shape}\")\n",
        "            print(f\"   ‚Ä¢ Output shape: {self.model.output_shape}\")\n",
        "            print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "            print(f\"   ‚Ä¢ Classes: {self.class_names}\")\n",
        "            if self.scaler is not None:\n",
        "                print(f\"   ‚Ä¢ Feature dimensions: {len(self.scaler.mean_)}\")\n",
        "        else:\n",
        "            print(\"‚ùå No model loaded\")\n",
        "\n",
        "\n",
        "# Simple usage functions\n",
        "def predict_single_audio(audio_file_path, model_path='/content/lung_sound_model_best.keras', scaler_path='/content/scaler.pkl'):\n",
        "    \"\"\"\n",
        "    Simple function to predict a single audio file\n",
        "\n",
        "    Parameters:\n",
        "    - audio_file_path: Path to your audio file\n",
        "    - model_path: Path to saved model (default: 'lung_sound_model_best.keras')\n",
        "    - scaler_path: Path to saved scaler (default: 'scaler.pkl')\n",
        "\n",
        "    Returns:\n",
        "    - prediction_result: Dictionary with prediction details\n",
        "    \"\"\"\n",
        "    predictor = LungSoundPredictor(model_path, scaler_path)\n",
        "    return predictor.predict_single_file(audio_file_path)\n",
        "\n",
        "def predict_audio_directory(directory_path, model_path='/content/lung_sound_model_best.keras', scaler_path='scaler.pkl'):\n",
        "    \"\"\"\n",
        "    Simple function to predict all audio files in a directory\n",
        "\n",
        "    Parameters:\n",
        "    - directory_path: Directory containing audio files\n",
        "    - model_path: Path to saved model\n",
        "    - scaler_path: Path to saved scaler\n",
        "\n",
        "    Returns:\n",
        "    - results: List of prediction results\n",
        "    \"\"\"\n",
        "    import glob\n",
        "\n",
        "    # Find all audio files in directory\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n",
        "    audio_files = []\n",
        "\n",
        "    for ext in audio_extensions:\n",
        "        audio_files.extend(glob.glob(os.path.join(directory_path, ext)))\n",
        "        audio_files.extend(glob.glob(os.path.join(directory_path, ext.upper())))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"‚ùå No audio files found in: {directory_path}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"üìÅ Found {len(audio_files)} audio files in: {directory_path}\")\n",
        "\n",
        "    predictor = LungSoundPredictor(model_path, scaler_path)\n",
        "    return predictor.predict_multiple_files(audio_files)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü´Å Lung Sound Classification for Unseen Data\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Example 1: Predict a single file\n",
        "    print(\"\\nüìñ Example 1: Predict single file\")\n",
        "    result = predict_single_audio('/content/BP50_N,N,P R L ,27,M.wav')\n",
        "\n",
        "    # Example 2: Predict multiple files\n",
        "    print(\"\\nüìñ Example 2: Predict all files in directory\")\n",
        "    print(\"results = predict_audio_directory('path/to/your/audio/directory')\")\n",
        "\n",
        "    # Example 3: Using the class directly\n",
        "    print(\"\\nüìñ Example 3: Using the class directly\")\n",
        "    print(\"predictor = LungSoundPredictor()\")\n",
        "    print(\"predictor.get_model_info()  # Show model details\")\n",
        "    print(\"result = predictor.predict_single_file('audio_file.wav')\")\n",
        "\n",
        "    print(\"\\n‚úÖ Ready to use! Make sure you have:\")\n",
        "    print(\"   1. Your trained model file (lung_sound_model_best.keras)\")\n",
        "    print(\"   2. Your scaler file (scaler.pkl)\")\n",
        "    print(\"   3. Audio files to classify\")\n",
        "\n",
        "    # Test if files exist\n",
        "    print(\"\\nüîç Checking for required files...\")\n",
        "    if os.path.exists('lung_sound_model_best.keras'):\n",
        "        print(\"   ‚úÖ Model file found\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Model file 'lung_sound_model_best.keras' not found\")\n",
        "\n",
        "    if os.path.exists('scaler.pkl'):\n",
        "        print(\"   ‚úÖ Scaler file found\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Scaler file 'scaler.pkl' not found\")\n",
        "\n",
        "# ans ==> /content/142_1b1_Pl_mc_LittC2SE.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGDwWRc4rbiq"
      },
      "source": [
        "## **Model 2**\n",
        "# ü©∫ Improved Lung Sound Classification - Focus on Generalization\n",
        "\n",
        "> **Goal:** Build a robust, generalizable model for classifying lung sounds into `Healthy`, `Asthma`, and `COPD` categories using deep learning and ensemble techniques.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Highlights\n",
        "\n",
        "- ‚úÖ **Simplified models** to reduce overfitting\n",
        "- üîÅ **Data augmentation** to boost generalization\n",
        "- üìä **Attention mechanism** for better temporal understanding\n",
        "- üß† **Ensemble support** for robust prediction\n",
        "- üõ†Ô∏è Conservative training with early stopping and LR reduction\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Class: `ImprovedLungSoundClassifier`\n",
        "\n",
        "### üîß Initialization\n",
        "```python\n",
        "classifier = ImprovedLungSoundClassifier(input_shape=(1, 959), num_classes=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knQQaCq0rdJr"
      },
      "outputs": [],
      "source": [
        "# Improved Lung Sound Classification - Focus on Generalization\n",
        "# Addresses overfitting issues for better unseen data performance\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU, LSTM,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU, SpatialDropout1D,\n",
        "    MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Improved Neural Network for Lung Sound Classification\n",
        "    Focus: Better generalization on unseen data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        \"\"\"Create a simpler, more generalizable model\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Simple feature extraction with lighter regularization\n",
        "        x = Conv1D(filters=32, kernel_size=7, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # Single RNN layer to reduce complexity\n",
        "        x = Bidirectional(\n",
        "            GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "        # Simpler dense layers\n",
        "        x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='SimpleLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_attention_model(self):\n",
        "        \"\"\"Create attention-based model for better feature learning\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Light conv preprocessing\n",
        "        x = Conv1D(filters=32, kernel_size=5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # RNN processing\n",
        "        x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=48,\n",
        "            dropout=0.2\n",
        "        )(x, x)\n",
        "        x = LayerNormalization()(x + attention)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = Dense(32, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='AttentionLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_ensemble_ready_model(self, model_variant='simple'):\n",
        "        \"\"\"Create different model variants for ensemble\"\"\"\n",
        "        if model_variant == 'simple':\n",
        "            return self.create_simple_model()\n",
        "        elif model_variant == 'attention':\n",
        "            return self.create_attention_model()\n",
        "        elif model_variant == 'lstm':\n",
        "            return self.create_lstm_model()\n",
        "        else:\n",
        "            return self.create_simple_model()\n",
        "\n",
        "    def create_lstm_model(self):\n",
        "        \"\"\"LSTM variant for ensemble diversity\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Feature extraction\n",
        "        x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # LSTM layers\n",
        "        x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification\n",
        "        x = Dense(48, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LSTMLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.0005):\n",
        "        \"\"\"Compile with conservative settings for better generalization\"\"\"\n",
        "        optimizer = Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        \"\"\"Compute balanced class weights\"\"\"\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(f\"üìä Class weights:\")\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        for i, weight in self.class_weights.items():\n",
        "            print(f\"   ‚Ä¢ {class_names[i]}: {weight:.3f}\")\n",
        "\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='improved_lung_model'):\n",
        "        \"\"\"Conservative callbacks for better generalization\"\"\"\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,  # Shorter patience to prevent overfitting\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,  # Reduce LR more aggressively\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train_with_data_augmentation(self, x_train, y_train, x_val, y_val,\n",
        "                                   model_type='simple', epochs=80, batch_size=32):\n",
        "        \"\"\"Train with data augmentation for better generalization\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Training {model_type} model with data augmentation...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "\n",
        "        # Data augmentation\n",
        "        x_train_aug, y_train_aug = self.augment_data(x_train, y_train)\n",
        "        print(f\"   ‚Ä¢ Augmented training samples: {x_train_aug.shape[0]}\")\n",
        "\n",
        "        # Create model\n",
        "        self.model = self.create_ensemble_ready_model(model_type)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = self.compute_class_weights(y_train_aug)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = self.create_callbacks(f'{model_type}_lung_model')\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture ({model_type}):\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            x_train_aug, y_train_aug,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def augment_data(self, x_data, y_data, augment_factor=0.5):\n",
        "        \"\"\"Simple data augmentation techniques\"\"\"\n",
        "        augmented_x = []\n",
        "        augmented_y = []\n",
        "\n",
        "        # Original data\n",
        "        augmented_x.append(x_data)\n",
        "        augmented_y.append(y_data)\n",
        "\n",
        "        n_augment = int(len(x_data) * augment_factor)\n",
        "        indices = np.random.choice(len(x_data), n_augment, replace=True)\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = x_data[idx].copy()\n",
        "            label = y_data[idx].copy()\n",
        "\n",
        "            # Random noise addition (5% of signal std)\n",
        "            noise_level = 0.05 * np.std(sample)\n",
        "            sample += np.random.normal(0, noise_level, sample.shape)\n",
        "\n",
        "            # Random scaling (¬±10%)\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            sample *= scale_factor\n",
        "\n",
        "            augmented_x.append(sample[np.newaxis, :])\n",
        "            augmented_y.append(label[np.newaxis, :])\n",
        "\n",
        "        return np.vstack(augmented_x), np.vstack(augmented_y)\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Metrics\n",
        "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        # Per-class metrics\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training curves to check for overfitting\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val', color='orange')\n",
        "        axes[0, 0].set_title('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val', color='orange')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        axes[1, 0].plot(self.history.history['precision'], label='Train', color='blue')\n",
        "        axes[1, 0].plot(self.history.history['val_precision'], label='Val', color='orange')\n",
        "        axes[1, 0].set_title('Precision')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recall\n",
        "        axes[1, 1].plot(self.history.history['recall'], label='Train', color='blue')\n",
        "        axes[1, 1].plot(self.history.history['val_recall'], label='Val', color='orange')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class EnsembleLungClassifier:\n",
        "    \"\"\"Ensemble approach for robust predictions\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.models = []\n",
        "        self.model_types = ['simple', 'attention', 'lstm']\n",
        "\n",
        "    def train_ensemble(self, x_train, y_train, x_val, y_val, epochs=60):\n",
        "        \"\"\"Train ensemble of diverse models\"\"\"\n",
        "        print(\"üéØ Training Ensemble of Models...\")\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nüîÑ Training {model_type} model...\")\n",
        "\n",
        "            classifier = ImprovedLungSoundClassifier(self.input_shape, self.num_classes)\n",
        "            history = classifier.train_with_data_augmentation(\n",
        "                x_train, y_train, x_val, y_val,\n",
        "                model_type=model_type,\n",
        "                epochs=epochs,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.models.append(classifier.model)\n",
        "            print(f\"‚úÖ {model_type} model trained!\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def predict_ensemble(self, x_test):\n",
        "        \"\"\"Make ensemble predictions\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained!\")\n",
        "            return None\n",
        "\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(x_test, verbose=0)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Average predictions\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate_ensemble(self, x_test, y_test):\n",
        "        \"\"\"Evaluate ensemble performance\"\"\"\n",
        "        ensemble_pred = self.predict_ensemble(x_test)\n",
        "        if ensemble_pred is None:\n",
        "            return None\n",
        "\n",
        "        y_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüéØ Ensemble Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "\n",
        "# Improved Training Pipeline\n",
        "def train_improved_lung_classifier(x_train, y_train, x_val, y_val, x_test, y_test,\n",
        "                                 use_ensemble=False):\n",
        "    \"\"\"\n",
        "    Improved training pipeline focused on generalization\n",
        "\n",
        "    Key improvements:\n",
        "    1. Simpler architectures to reduce overfitting\n",
        "    2. Data augmentation for better generalization\n",
        "    3. Conservative training settings\n",
        "    4. Ensemble option for robust predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Improved Lung Sound Classification Pipeline\")\n",
        "    print(\"Focus: Better generalization on unseen data\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if use_ensemble:\n",
        "        # Train ensemble\n",
        "        ensemble = EnsembleLungClassifier()\n",
        "        models = ensemble.train_ensemble(x_train, y_train, x_val, y_val)\n",
        "        results = ensemble.evaluate_ensemble(x_test, y_test)\n",
        "        return ensemble, results\n",
        "    else:\n",
        "        # Train single improved model\n",
        "        classifier = ImprovedLungSoundClassifier()\n",
        "\n",
        "        # Try simple model first\n",
        "        history = classifier.train_with_data_augmentation(\n",
        "            x_train, y_train, x_val, y_val,\n",
        "            model_type='simple',\n",
        "            epochs=150,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        results = classifier.evaluate_model(x_test, y_test)\n",
        "\n",
        "        # Plot training curves\n",
        "        classifier.plot_training_history()\n",
        "\n",
        "        return classifier, results\n",
        "\n",
        "\n",
        "# Usage instructions\n",
        "def usage_example():\n",
        "    \"\"\"How to use the improved classifier\"\"\"\n",
        "    print(\"\\nüìù Usage Example:\")\n",
        "    print(\"# For single improved model:\")\n",
        "    # classifier, results = train_improved_lung_classifier(\n",
        "    #   x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru)\n",
        "    print(\")\")\n",
        "    print(\"\\n# For ensemble approach (better but slower):\")\n",
        "    # classifier, results = train_improved_lung_classifier(\n",
        "    # x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru,)\n",
        "    print(\"    use_ensemble=True\")\n",
        "    print(\")\")\n",
        "\n",
        "    print(\"\\nüí° Key Improvements:\")\n",
        "    print(\"‚Ä¢ Simpler architecture to prevent overfitting\")\n",
        "    print(\"‚Ä¢ Data augmentation for better generalization\")\n",
        "    print(\"‚Ä¢ Conservative training with early stopping\")\n",
        "    print(\"‚Ä¢ Ensemble option for robust predictions\")\n",
        "    print(\"‚Ä¢ Better regularization strategies\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # usage_example()\n",
        "  classifier, results = train_improved_lung_classifier(\n",
        "      x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVCsN127w2vW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EJkgPR5sAeJ"
      },
      "source": [
        "#### aoc roc curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK2LMMiVsACS"
      },
      "outputs": [],
      "source": [
        "# AUC and ROC Curve Analysis for Lung Sound Classification\n",
        "# Comprehensive evaluation metrics for multi-class classification\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "\n",
        "class ModelEvaluationMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation metrics including ROC curves and AUC scores\n",
        "    for multi-class lung sound classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_names=['Healthy', 'Asthma', 'COPD']):\n",
        "        self.class_names = class_names\n",
        "        self.n_classes = len(class_names)\n",
        "        self.colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "\n",
        "    def compute_roc_auc(self, y_true, y_pred_proba, plot=True):\n",
        "        \"\"\"\n",
        "        Compute ROC curves and AUC scores for multi-class classification\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : array-like, shape = [n_samples]\n",
        "            True class labels (integer encoded)\n",
        "        y_pred_proba : array-like, shape = [n_samples, n_classes]\n",
        "            Predicted class probabilities\n",
        "        plot : bool, default=True\n",
        "            Whether to plot ROC curves\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary containing AUC scores and ROC data\n",
        "        \"\"\"\n",
        "\n",
        "        # Binarize the output for multi-class ROC\n",
        "        y_true_bin = label_binarize(y_true, classes=range(self.n_classes))\n",
        "\n",
        "        # For binary classification, label_binarize returns 1D array\n",
        "        if self.n_classes == 2:\n",
        "            y_true_bin = np.column_stack([1 - y_true_bin, y_true_bin])\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "\n",
        "        for i in range(self.n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "        # Compute macro-average ROC curve and ROC area\n",
        "        # First aggregate all false positive rates\n",
        "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(self.n_classes)]))\n",
        "\n",
        "        # Then interpolate all ROC curves at this points\n",
        "        mean_tpr = np.zeros_like(all_fpr)\n",
        "        for i in range(self.n_classes):\n",
        "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "        # Finally average it and compute AUC\n",
        "        mean_tpr /= self.n_classes\n",
        "\n",
        "        fpr[\"macro\"] = all_fpr\n",
        "        tpr[\"macro\"] = mean_tpr\n",
        "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "        # Plot ROC curves\n",
        "        if plot:\n",
        "            self.plot_roc_curves(fpr, tpr, roc_auc)\n",
        "\n",
        "        # Print AUC scores\n",
        "        print(\"\\nüìä ROC AUC Scores:\")\n",
        "        print(f\"   ‚Ä¢ Micro-average AUC: {roc_auc['micro']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
        "        print(\"\\n   Per-class AUC:\")\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            print(f\"   ‚Ä¢ {class_name}: {roc_auc[i]:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "            'roc_auc': roc_auc,\n",
        "            'micro_auc': roc_auc['micro'],\n",
        "            'macro_auc': roc_auc['macro']\n",
        "        }\n",
        "\n",
        "    def plot_roc_curves(self, fpr, tpr, roc_auc):\n",
        "        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot ROC curve for each class\n",
        "        colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "        for i, color in zip(range(self.n_classes), colors):\n",
        "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                    label=f'{self.class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
        "\n",
        "        # Plot micro-average ROC curve\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "                label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n",
        "                color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "        # Plot macro-average ROC curve\n",
        "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "                label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n",
        "                color='navy', linestyle=':', linewidth=3)\n",
        "\n",
        "        # Plot random classifier line\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate', fontsize=12)\n",
        "        plt.ylabel('True Positive Rate', fontsize=12)\n",
        "        plt.title('ROC Curves - Lung Sound Classification', fontsize=14, fontweight='bold')\n",
        "        plt.legend(loc=\"lower right\", fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_precision_recall_auc(self, y_true, y_pred_proba, plot=True):\n",
        "        \"\"\"\n",
        "        Compute Precision-Recall curves and AUC scores\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : array-like, shape = [n_samples]\n",
        "            True class labels (integer encoded)\n",
        "        y_pred_proba : array-like, shape = [n_samples, n_classes]\n",
        "            Predicted class probabilities\n",
        "        plot : bool, default=True\n",
        "            Whether to plot PR curves\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary containing PR AUC scores and curve data\n",
        "        \"\"\"\n",
        "\n",
        "        # Binarize the output for multi-class PR curves\n",
        "        y_true_bin = label_binarize(y_true, classes=range(self.n_classes))\n",
        "\n",
        "        # For binary classification, label_binarize returns 1D array\n",
        "        if self.n_classes == 2:\n",
        "            y_true_bin = np.column_stack([1 - y_true_bin, y_true_bin])\n",
        "\n",
        "        # Compute Precision-Recall curve and average precision for each class\n",
        "        precision = dict()\n",
        "        recall = dict()\n",
        "        pr_auc = dict()\n",
        "\n",
        "        for i in range(self.n_classes):\n",
        "            precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
        "            pr_auc[i] = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n",
        "\n",
        "        # Compute micro-average precision-recall curve\n",
        "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
        "            y_true_bin.ravel(), y_pred_proba.ravel())\n",
        "        pr_auc[\"micro\"] = average_precision_score(y_true_bin, y_pred_proba, average=\"micro\")\n",
        "\n",
        "        # Compute macro-average\n",
        "        pr_auc[\"macro\"] = average_precision_score(y_true_bin, y_pred_proba, average=\"macro\")\n",
        "\n",
        "        # Plot PR curves\n",
        "        if plot:\n",
        "            self.plot_precision_recall_curves(precision, recall, pr_auc)\n",
        "\n",
        "        # Print PR AUC scores\n",
        "        print(\"\\nüìä Precision-Recall AUC Scores:\")\n",
        "        print(f\"   ‚Ä¢ Micro-average PR-AUC: {pr_auc['micro']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Macro-average PR-AUC: {pr_auc['macro']:.4f}\")\n",
        "        print(\"\\n   Per-class PR-AUC:\")\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            print(f\"   ‚Ä¢ {class_name}: {pr_auc[i]:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'pr_auc': pr_auc,\n",
        "            'micro_pr_auc': pr_auc['micro'],\n",
        "            'macro_pr_auc': pr_auc['macro']\n",
        "        }\n",
        "\n",
        "    def plot_precision_recall_curves(self, precision, recall, pr_auc):\n",
        "        \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot PR curve for each class\n",
        "        colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "        for i, color in zip(range(self.n_classes), colors):\n",
        "            plt.plot(recall[i], precision[i], color=color, lw=2,\n",
        "                    label=f'{self.class_names[i]} (AP = {pr_auc[i]:.3f})')\n",
        "\n",
        "        # Plot micro-average PR curve\n",
        "        plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
        "                label=f'Micro-average (AP = {pr_auc[\"micro\"]:.3f})',\n",
        "                color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('Recall', fontsize=12)\n",
        "        plt.ylabel('Precision', fontsize=12)\n",
        "        plt.title('Precision-Recall Curves - Lung Sound Classification', fontsize=14, fontweight='bold')\n",
        "        plt.legend(loc=\"lower left\", fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def comprehensive_evaluation(self, y_true, y_pred_proba):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation including both ROC and PR curves\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : array-like, shape = [n_samples]\n",
        "            True class labels (integer encoded)\n",
        "        y_pred_proba : array-like, shape = [n_samples, n_classes]\n",
        "            Predicted class probabilities\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary containing all evaluation metrics\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üéØ Comprehensive Model Evaluation\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # ROC Analysis\n",
        "        roc_results = self.compute_roc_auc(y_true, y_pred_proba, plot=True)\n",
        "\n",
        "        # Precision-Recall Analysis\n",
        "        pr_results = self.compute_precision_recall_auc(y_true, y_pred_proba, plot=True)\n",
        "\n",
        "        # Combined results\n",
        "        results = {\n",
        "            'roc_auc': roc_results,\n",
        "            'pr_auc': pr_results,\n",
        "            'summary': {\n",
        "                'micro_roc_auc': roc_results['micro_auc'],\n",
        "                'macro_roc_auc': roc_results['macro_auc'],\n",
        "                'micro_pr_auc': pr_results['micro_pr_auc'],\n",
        "                'macro_pr_auc': pr_results['macro_pr_auc']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_combined_metrics(self, y_true, y_pred_proba):\n",
        "        \"\"\"Plot ROC and PR curves side by side\"\"\"\n",
        "\n",
        "        # Compute metrics\n",
        "        roc_results = self.compute_roc_auc(y_true, y_pred_proba, plot=False)\n",
        "        pr_results = self.compute_precision_recall_auc(y_true, y_pred_proba, plot=False)\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # ROC Curves\n",
        "        colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "        for i, color in zip(range(self.n_classes), colors):\n",
        "            ax1.plot(roc_results['fpr'][i], roc_results['tpr'][i], color=color, lw=2,\n",
        "                    label=f'{self.class_names[i]} (AUC = {roc_results[\"roc_auc\"][i]:.3f})')\n",
        "\n",
        "        ax1.plot(roc_results['fpr'][\"micro\"], roc_results['tpr'][\"micro\"],\n",
        "                label=f'Micro-avg (AUC = {roc_results[\"roc_auc\"][\"micro\"]:.3f})',\n",
        "                color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "        ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
        "        ax1.set_xlim([0.0, 1.0])\n",
        "        ax1.set_ylim([0.0, 1.05])\n",
        "        ax1.set_xlabel('False Positive Rate')\n",
        "        ax1.set_ylabel('True Positive Rate')\n",
        "        ax1.set_title('ROC Curves')\n",
        "        ax1.legend(loc=\"lower right\", fontsize=9)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # PR Curves\n",
        "        colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "        for i, color in zip(range(self.n_classes), colors):\n",
        "            ax2.plot(pr_results['recall'][i], pr_results['precision'][i], color=color, lw=2,\n",
        "                    label=f'{self.class_names[i]} (AP = {pr_results[\"pr_auc\"][i]:.3f})')\n",
        "\n",
        "        ax2.plot(pr_results['recall'][\"micro\"], pr_results['precision'][\"micro\"],\n",
        "                label=f'Micro-avg (AP = {pr_results[\"pr_auc\"][\"micro\"]:.3f})',\n",
        "                color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "        ax2.set_xlim([0.0, 1.0])\n",
        "        ax2.set_ylim([0.0, 1.05])\n",
        "        ax2.set_xlabel('Recall')\n",
        "        ax2.set_ylabel('Precision')\n",
        "        ax2.set_title('Precision-Recall Curves')\n",
        "        ax2.legend(loc=\"lower left\", fontsize=9)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def evaluate_lung_classifier_with_curves(classifier, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained lung classifier with ROC and PR curves\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    classifier : ImprovedLungSoundClassifier\n",
        "        Trained classifier object\n",
        "    x_test : array-like\n",
        "        Test features\n",
        "    y_test : array-like\n",
        "        Test labels (one-hot encoded)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Comprehensive evaluation results\n",
        "    \"\"\"\n",
        "\n",
        "    if classifier.model is None:\n",
        "        print(\"‚ùå Model not trained yet!\")\n",
        "        return None\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred_proba = classifier.model.predict(x_test, verbose=0)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = ModelEvaluationMetrics(class_names=['Healthy', 'Asthma', 'COPD'])\n",
        "\n",
        "    # Comprehensive evaluation\n",
        "    results = evaluator.comprehensive_evaluation(y_true, y_pred_proba)\n",
        "\n",
        "    # Combined plot\n",
        "    print(\"\\nüìä Combined ROC and PR Curves:\")\n",
        "    evaluator.plot_combined_metrics(y_true, y_pred_proba)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_ensemble_with_curves(ensemble_classifier, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate ensemble classifier with ROC and PR curves\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ensemble_classifier : EnsembleLungClassifier\n",
        "        Trained ensemble classifier\n",
        "    x_test : array-like\n",
        "        Test features\n",
        "    y_test : array-like\n",
        "        Test labels (one-hot encoded)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Comprehensive evaluation results\n",
        "    \"\"\"\n",
        "\n",
        "    # Get ensemble predictions\n",
        "    y_pred_proba = ensemble_classifier.predict_ensemble(x_test)\n",
        "    if y_pred_proba is None:\n",
        "        print(\"‚ùå Ensemble not trained yet!\")\n",
        "        return None\n",
        "\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = ModelEvaluationMetrics(class_names=['Healthy', 'Asthma', 'COPD'])\n",
        "\n",
        "    # Comprehensive evaluation\n",
        "    results = evaluator.comprehensive_evaluation(y_true, y_pred_proba)\n",
        "\n",
        "    # Combined plot\n",
        "    print(\"\\nüìä Ensemble - Combined ROC and PR Curves:\")\n",
        "    evaluator.plot_combined_metrics(y_true, y_pred_proba)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_models_curves(models_dict, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Compare multiple models using ROC curves\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models_dict : dict\n",
        "        Dictionary of {'model_name': model} pairs\n",
        "    x_test : array-like\n",
        "        Test features\n",
        "    y_test : array-like\n",
        "        Test labels (one-hot encoded)\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple', 'brown'])\n",
        "\n",
        "    for (model_name, model), color in zip(models_dict.items(), colors):\n",
        "        # Get predictions\n",
        "        if hasattr(model, 'predict_ensemble'):\n",
        "            y_pred_proba = model.predict_ensemble(x_test)\n",
        "        else:\n",
        "            y_pred_proba = model.predict(x_test, verbose=0)\n",
        "\n",
        "        # Compute micro-average AUC\n",
        "        y_true_bin = label_binarize(y_true, classes=range(3))\n",
        "        micro_auc = roc_auc_score(y_true_bin, y_pred_proba, average='micro')\n",
        "\n",
        "        # Compute micro-average ROC curve\n",
        "        fpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
        "\n",
        "        plt.plot(fpr_micro, tpr_micro, color=color, lw=2,\n",
        "                label=f'{model_name} (AUC = {micro_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('Model Comparison - ROC Curves (Micro-Average)', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Usage Examples\n",
        "def usage_examples():\n",
        "    \"\"\"\n",
        "    Example usage of the ROC and AUC evaluation functions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüìù Usage Examples:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"\\n1. Single Model Evaluation:\")\n",
        "    results = evaluate_lung_classifier_with_curves(classifier, x_test_gru, y_test_gru)\n",
        "\n",
        "\n",
        "    print(\"\\n2. Ensemble Model Evaluation:\")\n",
        "    print(\"```python\")\n",
        "    print(\"# After training ensemble\")\n",
        "    print(\"results = evaluate_ensemble_with_curves(ensemble, x_test, y_test)\")\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n3. Model Comparison:\")\n",
        "    print(\"```python\")\n",
        "    print(\"models_dict = {\")\n",
        "    print(\"    'Simple Model': classifier1.model,\")\n",
        "    print(\"    'Attention Model': classifier2.model,\")\n",
        "    print(\"    'Ensemble': ensemble_classifier\")\n",
        "    print(\"}\")\n",
        "    print(\"compare_models_curves(models_dict, x_test, y_test)\")\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n4. Custom Evaluation:\")\n",
        "    print(\"```python\")\n",
        "    print(\"evaluator = ModelEvaluationMetrics()\")\n",
        "    print(\"results = evaluator.comprehensive_evaluation(y_true, y_pred_proba)\")\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\nüí° Key Metrics Explained:\")\n",
        "    print(\"‚Ä¢ ROC AUC: Area under ROC curve (0.5 = random, 1.0 = perfect)\")\n",
        "    print(\"‚Ä¢ PR AUC: Area under Precision-Recall curve (accounts for class imbalance)\")\n",
        "    print(\"‚Ä¢ Micro-average: Global metric across all classes\")\n",
        "    print(\"‚Ä¢ Macro-average: Average of per-class metrics\")\n",
        "    print(\"‚Ä¢ Higher values indicate better performance\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    usage_examples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NReL2Q_QrqR1"
      },
      "source": [
        "# **unseen data prediction**\n",
        "# ü´Å Lung Sound Classification (Normal | Asthma | COPD)\n",
        "\n",
        " performs lung sound classification using a pre-trained deep learning model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKO6UwuIrp3x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from scipy.signal import hilbert\n",
        "import pywt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define the custom metric that was used during training\n",
        "@register_keras_serializable()\n",
        "def weighted_categorical_accuracy(y_true, y_pred):\n",
        "    \"\"\"Custom weighted categorical accuracy metric\"\"\"\n",
        "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "class LungSoundPredictor:\n",
        "    \"\"\"\n",
        "    Complete Lung Sound Classification System for Unseen Data\n",
        "    Supports: ASTHMA, COPD, NORMAL classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path='lung_sound_model_best.keras', scaler_path='scaler.pkl'):\n",
        "        \"\"\"\n",
        "        Initialize the predictor with trained model and scaler\n",
        "\n",
        "        Parameters:\n",
        "        - model_path: Path to saved Keras model\n",
        "        - scaler_path: Path to saved StandardScaler\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.scaler_path = scaler_path\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.class_names = ['Normal', 'Asthma', 'COPD']\n",
        "\n",
        "        # Load model and scaler\n",
        "        self.load_model_and_scaler()\n",
        "\n",
        "    def load_model_and_scaler(self):\n",
        "        \"\"\"Load the trained model and feature scaler\"\"\"\n",
        "        try:\n",
        "            # Load the trained model with custom objects\n",
        "            if os.path.exists(self.model_path):\n",
        "                custom_objects = {\n",
        "                    'weighted_categorical_accuracy': weighted_categorical_accuracy\n",
        "                }\n",
        "                self.model = load_model(self.model_path, custom_objects=custom_objects)\n",
        "                print(f\"‚úÖ Model loaded successfully from: {self.model_path}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Model file not found: {self.model_path}\")\n",
        "                print(\"Please ensure you have trained and saved the model first.\")\n",
        "                return False\n",
        "\n",
        "            # Load the scaler\n",
        "            if os.path.exists(self.scaler_path):\n",
        "                self.scaler = joblib.load(self.scaler_path)\n",
        "                print(f\"‚úÖ Scaler loaded successfully from: {self.scaler_path}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Scaler file not found: {self.scaler_path}\")\n",
        "                print(\"Please ensure you have the scaler.pkl file from training.\")\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model or scaler: {e}\")\n",
        "\n",
        "            # Alternative loading method - compile=False\n",
        "            try:\n",
        "                print(\"üîÑ Trying alternative loading method...\")\n",
        "                self.model = load_model(self.model_path, compile=False)\n",
        "\n",
        "                # Recompile the model with standard metrics\n",
        "                self.model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                print(f\"‚úÖ Model loaded successfully with alternative method\")\n",
        "\n",
        "                # Load scaler\n",
        "                if os.path.exists(self.scaler_path):\n",
        "                    self.scaler = joblib.load(self.scaler_path)\n",
        "                    print(f\"‚úÖ Scaler loaded successfully from: {self.scaler_path}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"‚ùå Scaler file not found: {self.scaler_path}\")\n",
        "                    return False\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"‚ùå Alternative loading method also failed: {e2}\")\n",
        "                return False\n",
        "\n",
        "    # Feature extraction functions (same as your training code)\n",
        "    def add_noise(self, data, noise_level=0.005):\n",
        "        \"\"\"Add Gaussian noise to audio data with improved stability.\"\"\"\n",
        "        if len(data) == 0:\n",
        "            return data\n",
        "        noise = np.random.randn(len(data)) * noise_level\n",
        "        noisy_data = data + noise\n",
        "        return np.clip(noisy_data, -1.0, 1.0)\n",
        "\n",
        "    def extract_advanced_mfcc_features(self, data, sampling_rate, n_mfcc=40):\n",
        "        \"\"\"Extract MFCCs with delta and delta-delta features plus statistical moments.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_mfcc * 9)\n",
        "\n",
        "            # Extract MFCCs\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mfcc=n_mfcc,\n",
        "                n_fft=2048,\n",
        "                hop_length=512\n",
        "            )\n",
        "\n",
        "            if mfccs.shape[1] == 0:\n",
        "                return np.zeros(n_mfcc * 9)\n",
        "\n",
        "            # Compute Delta and Delta-Delta\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            # Statistical moments\n",
        "            mfcc_mean = np.mean(mfccs, axis=1)\n",
        "            mfcc_std = np.std(mfccs, axis=1)\n",
        "            mfcc_skew = stats.skew(mfccs, axis=1)\n",
        "\n",
        "            delta_mean = np.mean(delta_mfccs, axis=1)\n",
        "            delta_std = np.std(delta_mfccs, axis=1)\n",
        "            delta_skew = stats.skew(delta_mfccs, axis=1)\n",
        "\n",
        "            delta2_mean = np.mean(delta2_mfccs, axis=1)\n",
        "            delta2_std = np.std(delta2_mfccs, axis=1)\n",
        "            delta2_skew = stats.skew(delta2_mfccs, axis=1)\n",
        "\n",
        "            # Combine all MFCC-based features\n",
        "            advanced_mfcc_features = np.concatenate([\n",
        "                mfcc_mean, mfcc_std, mfcc_skew,\n",
        "                delta_mean, delta_std, delta_skew,\n",
        "                delta2_mean, delta2_std, delta2_skew\n",
        "            ])\n",
        "\n",
        "            return np.nan_to_num(advanced_mfcc_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Advanced MFCC extraction failed: {e}\")\n",
        "            return np.zeros(n_mfcc * 9)\n",
        "\n",
        "    def extract_fbse_features(self, data, sampling_rate, n_bands=10):\n",
        "        \"\"\"Extract Fourier-Bessel Spectral Entropy features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_bands)\n",
        "\n",
        "            # Compute power spectral density\n",
        "            stft = librosa.stft(data)\n",
        "            psd = np.abs(stft)**2\n",
        "\n",
        "            # Divide frequency range into bands\n",
        "            freq_bands = np.linspace(0, sampling_rate//2, n_bands + 1)\n",
        "            entropy_features = []\n",
        "\n",
        "            for i in range(n_bands):\n",
        "                start_idx = int(freq_bands[i] * len(psd) / (sampling_rate//2))\n",
        "                end_idx = int(freq_bands[i+1] * len(psd) / (sampling_rate//2))\n",
        "\n",
        "                if end_idx > start_idx:\n",
        "                    band_psd = np.mean(psd[start_idx:end_idx], axis=0)\n",
        "                    band_psd_norm = band_psd / (np.sum(band_psd) + 1e-10)\n",
        "                    entropy = -np.sum(band_psd_norm * np.log(band_psd_norm + 1e-10))\n",
        "                    entropy_features.append(np.mean(entropy))\n",
        "                else:\n",
        "                    entropy_features.append(0.0)\n",
        "\n",
        "            return np.nan_to_num(np.array(entropy_features), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: FBSE extraction failed: {e}\")\n",
        "            return np.zeros(n_bands)\n",
        "\n",
        "    def extract_enhanced_melspectrogram(self, data, sampling_rate, n_mels=128):\n",
        "        \"\"\"Extract enhanced Mel-spectrogram features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(n_mels * 4)\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mels=n_mels,\n",
        "                n_fft=2048,\n",
        "                hop_length=512,\n",
        "                fmax=sampling_rate//2\n",
        "            )\n",
        "\n",
        "            if mel_spec.size == 0:\n",
        "                return np.zeros(n_mels * 4)\n",
        "\n",
        "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            if mel_spec_db.shape[1] > 0:\n",
        "                mel_mean = np.mean(mel_spec_db, axis=1)\n",
        "                mel_std = np.std(mel_spec_db, axis=1)\n",
        "                mel_max = np.max(mel_spec_db, axis=1)\n",
        "                mel_min = np.min(mel_spec_db, axis=1)\n",
        "                mel_features = np.concatenate([mel_mean, mel_std, mel_max, mel_min])\n",
        "            else:\n",
        "                mel_features = np.zeros(n_mels * 4)\n",
        "\n",
        "            return np.nan_to_num(mel_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Enhanced Mel-spectrogram extraction failed: {e}\")\n",
        "            return np.zeros(n_mels * 4)\n",
        "\n",
        "    def extract_wavelet_features(self, data, wavelet='db4', levels=5):\n",
        "        \"\"\"Extract wavelet features for transient detection.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(levels * 4 + 4)\n",
        "\n",
        "            coeffs = pywt.wavedec(data, wavelet, level=levels)\n",
        "            wavelet_features = []\n",
        "\n",
        "            for coeff in coeffs:\n",
        "                if len(coeff) > 0:\n",
        "                    wavelet_features.extend([\n",
        "                        np.mean(np.abs(coeff)),\n",
        "                        np.std(coeff),\n",
        "                        np.max(np.abs(coeff)),\n",
        "                        np.sum(coeff**2)\n",
        "                    ])\n",
        "                else:\n",
        "                    wavelet_features.extend([0.0, 0.0, 0.0, 0.0])\n",
        "\n",
        "            return np.nan_to_num(np.array(wavelet_features), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Wavelet feature extraction failed: {e}\")\n",
        "            return np.zeros(levels * 4 + 4)\n",
        "\n",
        "    def extract_sequence_features(self, data, sampling_rate, frame_length=2048, hop_length=512):\n",
        "        \"\"\"Extract sequence-based features.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(26)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=data,\n",
        "                sr=sampling_rate,\n",
        "                n_mfcc=13,\n",
        "                n_fft=frame_length,\n",
        "                hop_length=hop_length\n",
        "            )\n",
        "\n",
        "            if mfccs.shape[1] == 0:\n",
        "                return np.zeros(26)\n",
        "\n",
        "            frame_variations = np.mean(np.abs(np.diff(mfccs, axis=1)), axis=1)\n",
        "            long_term_mean = np.mean(mfccs, axis=1)\n",
        "            sequence_features = np.concatenate([frame_variations, long_term_mean])\n",
        "\n",
        "            return np.nan_to_num(sequence_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Sequence feature extraction failed: {e}\")\n",
        "            return np.zeros(26)\n",
        "\n",
        "    def extract_spectral_features(self, data, sampling_rate):\n",
        "        \"\"\"Extract spectral features with improved tonnetz handling.\"\"\"\n",
        "        try:\n",
        "            if len(data) == 0:\n",
        "                return np.zeros(7)\n",
        "\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sampling_rate)\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate)\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=sampling_rate)\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
        "            zero_crossing_rate = librosa.feature.zero_crossing_rate(data)\n",
        "            chroma = librosa.feature.chroma_stft(y=data, sr=sampling_rate)\n",
        "            chroma_mean = np.mean(chroma)\n",
        "\n",
        "            # Improved tonnetz handling\n",
        "            try:\n",
        "                if sampling_rate >= 8000:\n",
        "                    tonnetz = librosa.feature.tonnetz(y=data, sr=sampling_rate)\n",
        "                    tonnetz_mean = np.mean(tonnetz)\n",
        "                elif sampling_rate >= 4000:\n",
        "                    chroma_cqt = librosa.feature.chroma_cqt(\n",
        "                        y=data,\n",
        "                        sr=sampling_rate,\n",
        "                        fmin=librosa.note_to_hz('C1'),\n",
        "                        n_chroma=12\n",
        "                    )\n",
        "                    tonnetz_mean = np.mean(chroma_cqt) * 0.5\n",
        "                else:\n",
        "                    tonnetz_mean = 0.0\n",
        "            except:\n",
        "                tonnetz_mean = 0.0\n",
        "\n",
        "            spectral_features = np.array([\n",
        "                np.mean(spectral_centroid),\n",
        "                np.mean(spectral_bandwidth),\n",
        "                np.mean(spectral_rolloff),\n",
        "                np.mean(spectral_flatness),\n",
        "                np.mean(zero_crossing_rate),\n",
        "                chroma_mean,\n",
        "                tonnetz_mean\n",
        "            ])\n",
        "\n",
        "            return np.nan_to_num(spectral_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Spectral feature extraction failed: {e}\")\n",
        "            return np.zeros(7)\n",
        "\n",
        "    def fourier_bessel_features(self, data, sampling_rate, n_coeff=20):\n",
        "        \"\"\"Enhanced Fourier-Bessel feature extraction.\"\"\"\n",
        "        if len(data) == 0:\n",
        "            return np.zeros(n_coeff)\n",
        "\n",
        "        t = np.arange(len(data)) / sampling_rate\n",
        "        fb_coeff = np.zeros(n_coeff)\n",
        "        t_norm = t / np.max(t) if np.max(t) > 0 else t\n",
        "\n",
        "        for i in range(n_coeff):\n",
        "            j = i + 1\n",
        "            cosine_term = np.cos(2 * np.pi * j * t_norm)\n",
        "            fb_coeff[i] = np.sum(data * cosine_term) / len(data)\n",
        "\n",
        "        return np.nan_to_num(fb_coeff, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    def extract_features_from_audio(self, audio_file_path):\n",
        "        \"\"\"\n",
        "        Extract all features from a single audio file\n",
        "\n",
        "        Parameters:\n",
        "        - audio_file_path: Path to the audio file\n",
        "\n",
        "        Returns:\n",
        "        - features: Normalized feature vector ready for prediction\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            data, sampling_rate = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "            if len(data) == 0:\n",
        "                print(f\"‚ùå Empty audio file: {audio_file_path}\")\n",
        "                return None\n",
        "\n",
        "            print(f\"üéµ Processing: {os.path.basename(audio_file_path)}\")\n",
        "            print(f\"   ‚Ä¢ Duration: {len(data)/sampling_rate:.2f}s\")\n",
        "            print(f\"   ‚Ä¢ Sampling Rate: {sampling_rate}Hz\")\n",
        "\n",
        "            # Extract all features (same as training)\n",
        "            n_mfcc = 40\n",
        "            fb_coeffs = 20\n",
        "            n_mels = 128\n",
        "            wavelet_levels = 5\n",
        "            fbse_bands = 10\n",
        "\n",
        "            # Extract each feature type\n",
        "            advanced_mfcc_features = self.extract_advanced_mfcc_features(data, sampling_rate, n_mfcc)\n",
        "            fbse_features = self.extract_fbse_features(data, sampling_rate, fbse_bands)\n",
        "            enhanced_mel_features = self.extract_enhanced_melspectrogram(data, sampling_rate, n_mels)\n",
        "            wavelet_features = self.extract_wavelet_features(data, 'db4', wavelet_levels)\n",
        "            sequence_features = self.extract_sequence_features(data, sampling_rate)\n",
        "            spectral_features = self.extract_spectral_features(data, sampling_rate)\n",
        "            fb_features = self.fourier_bessel_features(data, sampling_rate, fb_coeffs)\n",
        "\n",
        "            # Combine all features\n",
        "            combined_features = np.concatenate([\n",
        "                advanced_mfcc_features,\n",
        "                fbse_features,\n",
        "                enhanced_mel_features,\n",
        "                wavelet_features,\n",
        "                sequence_features,\n",
        "                spectral_features,\n",
        "                fb_features\n",
        "            ])\n",
        "\n",
        "            # Final validation\n",
        "            combined_features = np.nan_to_num(combined_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            print(f\"   ‚Ä¢ Features extracted: {len(combined_features)} dimensions\")\n",
        "\n",
        "            # Normalize using training scaler\n",
        "            if self.scaler is not None:\n",
        "                features_normalized = self.scaler.transform(combined_features.reshape(1, -1))\n",
        "                return features_normalized[0]\n",
        "            else:\n",
        "                print(\"‚ùå Scaler not loaded. Cannot normalize features.\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extracting features from {audio_file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_single_file(self, audio_file_path, show_confidence=True):\n",
        "        \"\"\"\n",
        "        Predict lung condition for a single audio file\n",
        "\n",
        "        Parameters:\n",
        "        - audio_file_path: Path to the audio file\n",
        "        - show_confidence: Whether to show confidence scores\n",
        "\n",
        "        Returns:\n",
        "        - prediction_result: Dictionary with prediction details\n",
        "        \"\"\"\n",
        "        if self.model is None or self.scaler is None:\n",
        "            print(\"‚ùå Model or scaler not loaded properly.\")\n",
        "            return None\n",
        "\n",
        "        # Extract features\n",
        "        features = self.extract_features_from_audio(audio_file_path)\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Reshape for model input (1, 1, feature_size) format\n",
        "            features_reshaped = features.reshape(1, 1, -1)\n",
        "\n",
        "            print(f\"   ‚Ä¢ Input shape: {features_reshaped.shape}\")\n",
        "\n",
        "            # Make prediction\n",
        "            prediction_proba = self.model.predict(features_reshaped, verbose=0)\n",
        "            predicted_class_idx = np.argmax(prediction_proba[0])\n",
        "            predicted_class = self.class_names[predicted_class_idx]\n",
        "            confidence = prediction_proba[0][predicted_class_idx]\n",
        "\n",
        "            # Prepare result\n",
        "            result = {\n",
        "                'file': os.path.basename(audio_file_path),\n",
        "                'predicted_class': predicted_class,\n",
        "                'confidence': confidence,\n",
        "                'all_probabilities': {\n",
        "                    self.class_names[i]: prediction_proba[0][i]\n",
        "                    for i in range(len(self.class_names))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Display results\n",
        "            print(f\"\\nüéØ Prediction Results:\")\n",
        "            print(f\"   ‚Ä¢ File: {result['file']}\")\n",
        "            print(f\"   ‚Ä¢ Predicted Class: {predicted_class}\")\n",
        "            print(f\"   ‚Ä¢ Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "            if show_confidence:\n",
        "                print(f\"   ‚Ä¢ Detailed Probabilities:\")\n",
        "                for class_name, prob in result['all_probabilities'].items():\n",
        "                    print(f\"     - {class_name}: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "\n",
        "            # Confidence interpretation\n",
        "            if confidence >= 0.8:\n",
        "                confidence_level = \"High\"\n",
        "                emoji = \"üü¢\"\n",
        "            elif confidence >= 0.6:\n",
        "                confidence_level = \"Medium\"\n",
        "                emoji = \"üü°\"\n",
        "            else:\n",
        "                confidence_level = \"Low\"\n",
        "                emoji = \"üî¥\"\n",
        "\n",
        "            print(f\"   ‚Ä¢ Confidence Level: {emoji} {confidence_level}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_multiple_files(self, audio_files_list):\n",
        "        \"\"\"\n",
        "        Predict lung conditions for multiple audio files\n",
        "\n",
        "        Parameters:\n",
        "        - audio_files_list: List of audio file paths\n",
        "\n",
        "        Returns:\n",
        "        - results: List of prediction results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(f\"üîÑ Processing {len(audio_files_list)} audio files...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for i, audio_file in enumerate(audio_files_list):\n",
        "            print(f\"\\n[{i+1}/{len(audio_files_list)}] Processing: {os.path.basename(audio_file)}\")\n",
        "\n",
        "            result = self.predict_single_file(audio_file, show_confidence=False)\n",
        "            if result is not None:\n",
        "                results.append(result)\n",
        "\n",
        "        # Summary\n",
        "        if results:\n",
        "            print(f\"\\nüìä Summary of {len(results)} successful predictions:\")\n",
        "            class_counts = {}\n",
        "            for result in results:\n",
        "                pred_class = result['predicted_class']\n",
        "                class_counts[pred_class] = class_counts.get(pred_class, 0) + 1\n",
        "\n",
        "            for class_name, count in class_counts.items():\n",
        "                percentage = (count / len(results)) * 100\n",
        "                print(f\"   ‚Ä¢ {class_name}: {count} files ({percentage:.1f}%)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_model_info(self):\n",
        "        \"\"\"Display information about the loaded model\"\"\"\n",
        "        if self.model is not None:\n",
        "            print(\"üè• Model Information:\")\n",
        "            print(f\"   ‚Ä¢ Input shape: {self.model.input_shape}\")\n",
        "            print(f\"   ‚Ä¢ Output shape: {self.model.output_shape}\")\n",
        "            print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "            print(f\"   ‚Ä¢ Classes: {self.class_names}\")\n",
        "            if self.scaler is not None:\n",
        "                print(f\"   ‚Ä¢ Feature dimensions: {len(self.scaler.mean_)}\")\n",
        "        else:\n",
        "            print(\"‚ùå No model loaded\")\n",
        "\n",
        "\n",
        "# Simple usage functions\n",
        "def predict_single_audio(audio_file_path, model_path='/content/simple_lung_model_best.keras', scaler_path='/content/scaler.pkl'):\n",
        "    \"\"\"\n",
        "    Simple function to predict a single audio file\n",
        "\n",
        "    Parameters:\n",
        "    - audio_file_path: Path to your audio file\n",
        "    - model_path: Path to saved model (default: 'lung_sound_model_best.keras')\n",
        "    - scaler_path: Path to saved scaler (default: 'scaler.pkl')\n",
        "\n",
        "    Returns:\n",
        "    - prediction_result: Dictionary with prediction details\n",
        "    \"\"\"\n",
        "    predictor = LungSoundPredictor(model_path, scaler_path)\n",
        "    return predictor.predict_single_file(audio_file_path)\n",
        "\n",
        "def predict_audio_directory(directory_path, model_path='/content/lung_sound_model_best.keras', scaler_path='scaler.pkl'):\n",
        "    \"\"\"\n",
        "    Simple function to predict all audio files in a directory\n",
        "\n",
        "    Parameters:\n",
        "    - directory_path: Directory containing audio files\n",
        "    - model_path: Path to saved model\n",
        "    - scaler_path: Path to saved scaler\n",
        "\n",
        "    Returns:\n",
        "    - results: List of prediction results\n",
        "    \"\"\"\n",
        "    import glob\n",
        "\n",
        "    # Find all audio files in directory\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n",
        "    audio_files = []\n",
        "\n",
        "    for ext in audio_extensions:\n",
        "        audio_files.extend(glob.glob(os.path.join(directory_path, ext)))\n",
        "        audio_files.extend(glob.glob(os.path.join(directory_path, ext.upper())))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"‚ùå No audio files found in: {directory_path}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"üìÅ Found {len(audio_files)} audio files in: {directory_path}\")\n",
        "\n",
        "    predictor = LungSoundPredictor(model_path, scaler_path)\n",
        "    return predictor.predict_multiple_files(audio_files)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü´Å Lung Sound Classification for Unseen Data\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Example 1: Predict a single file\n",
        "    print(\"\\nüìñ Example 1: Predict single file\")\n",
        "    result = predict_single_audio('/content/BP50_N,N,P R L ,27,M.wav')\n",
        "\n",
        "    # Example 2: Predict multiple files\n",
        "    print(\"\\nüìñ Example 2: Predict all files in directory\")\n",
        "    print(\"results = predict_audio_directory('path/to/your/audio/directory')\")\n",
        "\n",
        "    # Example 3: Using the class directly\n",
        "    print(\"\\nüìñ Example 3: Using the class directly\")\n",
        "    print(\"predictor = LungSoundPredictor()\")\n",
        "    print(\"predictor.get_model_info()  # Show model details\")\n",
        "    print(\"result = predictor.predict_single_file('audio_file.wav')\")\n",
        "\n",
        "    print(\"\\n‚úÖ Ready to use! Make sure you have:\")\n",
        "    print(\"   1. Your trained model file (lung_sound_model_best.keras)\")\n",
        "    print(\"   2. Your scaler file (scaler.pkl)\")\n",
        "    print(\"   3. Audio files to classify\")\n",
        "\n",
        "    # Test if files exist\n",
        "    print(\"\\nüîç Checking for required files...\")\n",
        "    if os.path.exists('lung_sound_model_best.keras'):\n",
        "        print(\"   ‚úÖ Model file found\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Model file 'lung_sound_model_best.keras' not found\")\n",
        "\n",
        "    if os.path.exists('scaler.pkl'):\n",
        "        print(\"   ‚úÖ Scaler file found\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Scaler file 'scaler.pkl' not found\")\n",
        "\n",
        "\n",
        "# ans ==> /content/142_1b1_Pl_mc_LittC2SE.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQC9hQqr7Oh"
      },
      "source": [
        "#### aoc roc curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U852dXLCr60R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ImcJTMRsSad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define class labels\n",
        "classes = [\"Normal\", \"Asthma\", \"COPD\"]\n",
        "\n",
        "# Get predictions\n",
        "preds = model.model.predict(x_test_gru)\n",
        "classpreds = [np.argmax(t) for t in preds]\n",
        "y_testclass = [np.argmax(t) for t in y_test_gru]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_testclass, classpreds)\n",
        "\n",
        "# Normalize confusion matrix to display accuracy per class\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentage\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "ax = sns.heatmap(cm_percent, cmap='Blues', annot=True, fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix with Class-wise Accuracy (%)')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D3TQwhssULb"
      },
      "outputs": [],
      "source": [
        "# Combine counts and percentages into annotation labels\n",
        "labels = np.array([[f'{int(cm[i,j])}\\n{cm_percent[i,j]:.1f}%' for j in range(len(classes))] for i in range(len(classes))])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_percent, cmap='Blues', annot=labels, fmt='', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Counts & Accuracy %)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YQanxRJsV1J"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_testclass, classpreds, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwqBWaaL3hwL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbThn8NTbg5"
      },
      "source": [
        "# ü§ñü´Å Model 2 ‚Äì Ensemble-Based Lung Sound Classification\n",
        "\n",
        "Welcome to **Model 2**, where we apply the power of **ensemble learning** to enhance lung sound classification accuracy. This model intelligently combines predictions from multiple base learners (e.g., GRU, LSTM, CNN) to improve generalization and reliability.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What Does It Detect?\n",
        "This ensemble model classifies respiratory sounds into:\n",
        "- ‚úÖ **Normal**\n",
        "- ‚ö†Ô∏è **Asthma**\n",
        "- üö® **COPD**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Ensemble Learning?\n",
        "Ensemble techniques help improve robustness by:\n",
        "- üîÅ Combining multiple deep learning models\n",
        "- üß™ Reducing overfitting and bias\n",
        "- üìà Improving performance on **unseen clinical data**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ht7oCTThqE"
      },
      "outputs": [],
      "source": [
        "# Improved Lung Sound Classification - Focus on Generalization\n",
        "# Addresses overfitting issues for better unseen data performance\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU, LSTM,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU, SpatialDropout1D,\n",
        "    MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Improved Neural Network for Lung Sound Classification\n",
        "    Focus: Better generalization on unseen data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        \"\"\"Create a simpler, more generalizable model\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Simple feature extraction with lighter regularization\n",
        "        x = Conv1D(filters=32, kernel_size=7, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # Single RNN layer to reduce complexity\n",
        "        x = Bidirectional(\n",
        "            GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "        # Simpler dense layers\n",
        "        x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='SimpleLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_attention_model(self):\n",
        "        \"\"\"Create attention-based model for better feature learning\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Light conv preprocessing\n",
        "        x = Conv1D(filters=32, kernel_size=5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # RNN processing\n",
        "        x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=48,\n",
        "            dropout=0.2\n",
        "        )(x, x)\n",
        "        x = LayerNormalization()(x + attention)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = Dense(32, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='AttentionLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_ensemble_ready_model(self, model_variant='simple'):\n",
        "        \"\"\"Create different model variants for ensemble\"\"\"\n",
        "        if model_variant == 'simple':\n",
        "            return self.create_simple_model()\n",
        "        elif model_variant == 'attention':\n",
        "            return self.create_attention_model()\n",
        "        elif model_variant == 'lstm':\n",
        "            return self.create_lstm_model()\n",
        "        else:\n",
        "            return self.create_simple_model()\n",
        "\n",
        "    def create_lstm_model(self):\n",
        "        \"\"\"LSTM variant for ensemble diversity\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Feature extraction\n",
        "        x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # LSTM layers\n",
        "        x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification\n",
        "        x = Dense(48, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LSTMLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.0005):\n",
        "        \"\"\"Compile with conservative settings for better generalization\"\"\"\n",
        "        optimizer = Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        \"\"\"Compute balanced class weights\"\"\"\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(f\"üìä Class weights:\")\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        for i, weight in self.class_weights.items():\n",
        "            print(f\"   ‚Ä¢ {class_names[i]}: {weight:.3f}\")\n",
        "\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='improved_lung_model'):\n",
        "        \"\"\"Conservative callbacks for better generalization\"\"\"\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,  # Shorter patience to prevent overfitting\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,  # Reduce LR more aggressively\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train_with_data_augmentation(self, x_train, y_train, x_val, y_val,\n",
        "                                   model_type='esamble', epochs=80, batch_size=32):\n",
        "        \"\"\"Train with data augmentation for better generalization\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Training {model_type} model with data augmentation...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "\n",
        "        # Data augmentation\n",
        "        x_train_aug, y_train_aug = self.augment_data(x_train, y_train)\n",
        "        print(f\"   ‚Ä¢ Augmented training samples: {x_train_aug.shape[0]}\")\n",
        "\n",
        "        # Create model\n",
        "        self.model = self.create_ensemble_ready_model(model_type)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = self.compute_class_weights(y_train_aug)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = self.create_callbacks(f'{model_type}_lung_model')\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture ({model_type}):\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            x_train_aug, y_train_aug,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def augment_data(self, x_data, y_data, augment_factor=0.5):\n",
        "        \"\"\"Simple data augmentation techniques\"\"\"\n",
        "        augmented_x = []\n",
        "        augmented_y = []\n",
        "\n",
        "        # Original data\n",
        "        augmented_x.append(x_data)\n",
        "        augmented_y.append(y_data)\n",
        "\n",
        "        n_augment = int(len(x_data) * augment_factor)\n",
        "        indices = np.random.choice(len(x_data), n_augment, replace=True)\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = x_data[idx].copy()\n",
        "            label = y_data[idx].copy()\n",
        "\n",
        "            # Random noise addition (5% of signal std)\n",
        "            noise_level = 0.05 * np.std(sample)\n",
        "            sample += np.random.normal(0, noise_level, sample.shape)\n",
        "\n",
        "            # Random scaling (¬±10%)\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            sample *= scale_factor\n",
        "\n",
        "            augmented_x.append(sample[np.newaxis, :])\n",
        "            augmented_y.append(label[np.newaxis, :])\n",
        "\n",
        "        return np.vstack(augmented_x), np.vstack(augmented_y)\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Metrics\n",
        "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        # Per-class metrics\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training curves to check for overfitting\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val', color='orange')\n",
        "        axes[0, 0].set_title('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val', color='orange')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        axes[1, 0].plot(self.history.history['precision'], label='Train', color='blue')\n",
        "        axes[1, 0].plot(self.history.history['val_precision'], label='Val', color='orange')\n",
        "        axes[1, 0].set_title('Precision')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recall\n",
        "        axes[1, 1].plot(self.history.history['recall'], label='Train', color='blue')\n",
        "        axes[1, 1].plot(self.history.history['val_recall'], label='Val', color='orange')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class EnsembleLungClassifier:\n",
        "    \"\"\"Ensemble approach for robust predictions\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.models = []\n",
        "        self.model_types = ['simple', 'attention', 'lstm']\n",
        "\n",
        "    def train_ensemble(self, x_train, y_train, x_val, y_val, epochs=60):\n",
        "        \"\"\"Train ensemble of diverse models\"\"\"\n",
        "        print(\"üéØ Training Ensemble of Models...\")\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nüîÑ Training {model_type} model...\")\n",
        "\n",
        "            classifier = ImprovedLungSoundClassifier(self.input_shape, self.num_classes)\n",
        "            history = classifier.train_with_data_augmentation(\n",
        "                x_train, y_train, x_val, y_val,\n",
        "                model_type=model_type,\n",
        "                epochs=epochs,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.models.append(classifier.model)\n",
        "            print(f\"‚úÖ {model_type} model trained!\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def predict_ensemble(self, x_test):\n",
        "        \"\"\"Make ensemble predictions\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained!\")\n",
        "            return None\n",
        "\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(x_test, verbose=0)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Average predictions\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate_ensemble(self, x_test, y_test):\n",
        "        \"\"\"Evaluate ensemble performance\"\"\"\n",
        "        ensemble_pred = self.predict_ensemble(x_test)\n",
        "        if ensemble_pred is None:\n",
        "            return None\n",
        "\n",
        "        y_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüéØ Ensemble Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "\n",
        "# Improved Training Pipeline\n",
        "def train_improved_lung_classifier(x_train, y_train, x_val, y_val, x_test, y_test,\n",
        "                                 use_ensemble=True):\n",
        "    \"\"\"\n",
        "    Improved training pipeline focused on generalization\n",
        "\n",
        "    Key improvements:\n",
        "    1. Simpler architectures to reduce overfitting\n",
        "    2. Data augmentation for better generalization\n",
        "    3. Conservative training settings\n",
        "    4. Ensemble option for robust predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Improved Lung Sound Classification Pipeline\")\n",
        "    print(\"Focus: Better generalization on unseen data\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if use_ensemble:\n",
        "        # Train ensemble\n",
        "        ensemble = EnsembleLungClassifier()\n",
        "        models = ensemble.train_ensemble(x_train, y_train, x_val, y_val)\n",
        "        results = ensemble.evaluate_ensemble(x_test, y_test)\n",
        "        return ensemble, results\n",
        "    else:\n",
        "        # Train single improved model\n",
        "        classifier = ImprovedLungSoundClassifier()\n",
        "\n",
        "        # Try simple model first\n",
        "        history = classifier.train_with_data_augmentation(\n",
        "            x_train, y_train, x_val, y_val,\n",
        "            model_type='ensemble ',\n",
        "            epochs=150,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        results = classifier.evaluate_model(x_test, y_test)\n",
        "\n",
        "        # Plot training curves\n",
        "        classifier.plot_training_history()\n",
        "\n",
        "        return classifier, results\n",
        "\n",
        "\n",
        "# Usage instructions\n",
        "def usage_example():\n",
        "    \"\"\"How to use the improved classifier\"\"\"\n",
        "    print(\"\\nüìù Usage Example:\")\n",
        "    print(\"# For single improved model:\")\n",
        "    # classifier, results = train_improved_lung_classifier(\n",
        "    #   x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru)\n",
        "    print(\")\")\n",
        "    print(\"\\n# For ensemble approach (better but slower):\")\n",
        "    classifier, results = train_improved_lung_classifier(\n",
        "    x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru,)\n",
        "    print(\"    use_ensemble=True\")\n",
        "    print(\")\")\n",
        "\n",
        "    print(\"\\nüí° Key Improvements:\")\n",
        "    print(\"‚Ä¢ Simpler architecture to prevent overfitting\")\n",
        "    print(\"‚Ä¢ Data augmentation for better generalization\")\n",
        "    print(\"‚Ä¢ Conservative training with early stopping\")\n",
        "    print(\"‚Ä¢ Ensemble option for robust predictions\")\n",
        "    print(\"‚Ä¢ Better regularization strategies\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    usage_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFYTFDvI1Rl"
      },
      "source": [
        "# üß†‚ú® Model 2 ‚Äì BiGRU + Attention-Based Lung Sound Classification\n",
        "\n",
        "This notebook uses a **pure attention-based deep learning model** to classify lung sound recordings into clinical categories. Attention helps the model **focus on important parts** of the signal ‚Äî just like a doctor listens for subtle patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Target Classes:\n",
        "- ‚úÖ **Normal**\n",
        "- üå¨Ô∏è **Asthma**\n",
        "- üòÆ‚Äçüí® **COPD**\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Why Attention Models?\n",
        "Attention mechanisms enable:\n",
        "- üéØ Focus on the most relevant acoustic features\n",
        "- üîÑ Better temporal dynamics over raw sequential models\n",
        "- üìà Improved accuracy with fewer parameters compared to large CNN stacks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v909U069T30C"
      },
      "outputs": [],
      "source": [
        "# Improved Lung Sound Classification - Focus on Generalization\n",
        "# Addresses overfitting issues for better unseen data performance\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU, LSTM,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU, SpatialDropout1D,\n",
        "    MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Improved Neural Network for Lung Sound Classification\n",
        "    Focus: Better generalization on unseen data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        \"\"\"Create a simpler, more generalizable model\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Simple feature extraction with lighter regularization\n",
        "        x = Conv1D(filters=32, kernel_size=7, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # Single RNN layer to reduce complexity\n",
        "        x = Bidirectional(\n",
        "            GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "        # Simpler dense layers\n",
        "        x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='SimpleLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_attention_model(self):\n",
        "        \"\"\"Create attention-based model for better feature learning\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Light conv preprocessing\n",
        "        x = Conv1D(filters=32, kernel_size=5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # RNN processing\n",
        "        x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=48,\n",
        "            dropout=0.2\n",
        "        )(x, x)\n",
        "        x = LayerNormalization()(x + attention)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = Dense(32, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='AttentionLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_ensemble_ready_model(self, model_variant='attention'):\n",
        "        \"\"\"Create different model variants for ensemble\"\"\"\n",
        "        if model_variant == 'simple':\n",
        "            return self.create_simple_model()\n",
        "        elif model_variant == 'attention':\n",
        "            return self.create_attention_model()\n",
        "        elif model_variant == 'lstm':\n",
        "            return self.create_lstm_model()\n",
        "        else:\n",
        "            return self.create_simple_model()\n",
        "\n",
        "    def create_lstm_model(self):\n",
        "        \"\"\"LSTM variant for ensemble diversity\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Feature extraction\n",
        "        x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # LSTM layers\n",
        "        x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification\n",
        "        x = Dense(48, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LSTMLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.0005):\n",
        "        \"\"\"Compile with conservative settings for better generalization\"\"\"\n",
        "        optimizer = Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        \"\"\"Compute balanced class weights\"\"\"\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(f\"üìä Class weights:\")\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        for i, weight in self.class_weights.items():\n",
        "            print(f\"   ‚Ä¢ {class_names[i]}: {weight:.3f}\")\n",
        "\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='improved_lung_model'):\n",
        "        \"\"\"Conservative callbacks for better generalization\"\"\"\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,  # Shorter patience to prevent overfitting\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,  # Reduce LR more aggressively\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train_with_data_augmentation(self, x_train, y_train, x_val, y_val,\n",
        "                                   model_type='attention', epochs=80, batch_size=32):\n",
        "        \"\"\"Train with data augmentation for better generalization\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Training {model_type} model with data augmentation...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "\n",
        "        # Data augmentation\n",
        "        x_train_aug, y_train_aug = self.augment_data(x_train, y_train)\n",
        "        print(f\"   ‚Ä¢ Augmented training samples: {x_train_aug.shape[0]}\")\n",
        "\n",
        "        # Create model\n",
        "        self.model = self.create_ensemble_ready_model(model_type)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = self.compute_class_weights(y_train_aug)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = self.create_callbacks(f'{model_type}_lung_model')\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture ({model_type}):\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            x_train_aug, y_train_aug,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def augment_data(self, x_data, y_data, augment_factor=0.5):\n",
        "        \"\"\"Simple data augmentation techniques\"\"\"\n",
        "        augmented_x = []\n",
        "        augmented_y = []\n",
        "\n",
        "        # Original data\n",
        "        augmented_x.append(x_data)\n",
        "        augmented_y.append(y_data)\n",
        "\n",
        "        n_augment = int(len(x_data) * augment_factor)\n",
        "        indices = np.random.choice(len(x_data), n_augment, replace=True)\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = x_data[idx].copy()\n",
        "            label = y_data[idx].copy()\n",
        "\n",
        "            # Random noise addition (5% of signal std)\n",
        "            noise_level = 0.05 * np.std(sample)\n",
        "            sample += np.random.normal(0, noise_level, sample.shape)\n",
        "\n",
        "            # Random scaling (¬±10%)\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            sample *= scale_factor\n",
        "\n",
        "            augmented_x.append(sample[np.newaxis, :])\n",
        "            augmented_y.append(label[np.newaxis, :])\n",
        "\n",
        "        return np.vstack(augmented_x), np.vstack(augmented_y)\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Metrics\n",
        "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        # Per-class metrics\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training curves to check for overfitting\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val', color='orange')\n",
        "        axes[0, 0].set_title('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val', color='orange')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        axes[1, 0].plot(self.history.history['precision'], label='Train', color='blue')\n",
        "        axes[1, 0].plot(self.history.history['val_precision'], label='Val', color='orange')\n",
        "        axes[1, 0].set_title('Precision')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recall\n",
        "        axes[1, 1].plot(self.history.history['recall'], label='Train', color='blue')\n",
        "        axes[1, 1].plot(self.history.history['val_recall'], label='Val', color='orange')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class EnsembleLungClassifier:\n",
        "    \"\"\"Ensemble approach for robust predictions\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.models = []\n",
        "        self.model_types = ['simple', 'attention', 'lstm']\n",
        "\n",
        "    def train_ensemble(self, x_train, y_train, x_val, y_val, epochs=60):\n",
        "        \"\"\"Train ensemble of diverse models\"\"\"\n",
        "        print(\"üéØ Training Ensemble of Models...\")\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nüîÑ Training {model_type} model...\")\n",
        "\n",
        "            classifier = ImprovedLungSoundClassifier(self.input_shape, self.num_classes)\n",
        "            history = classifier.train_with_data_augmentation(\n",
        "                x_train, y_train, x_val, y_val,\n",
        "                model_type=model_type,\n",
        "                epochs=epochs,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.models.append(classifier.model)\n",
        "            print(f\"‚úÖ {model_type} model trained!\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def predict_ensemble(self, x_test):\n",
        "        \"\"\"Make ensemble predictions\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained!\")\n",
        "            return None\n",
        "\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(x_test, verbose=0)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Average predictions\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate_ensemble(self, x_test, y_test):\n",
        "        \"\"\"Evaluate ensemble performance\"\"\"\n",
        "        ensemble_pred = self.predict_ensemble(x_test)\n",
        "        if ensemble_pred is None:\n",
        "            return None\n",
        "\n",
        "        y_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüéØ Ensemble Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "\n",
        "# Improved Training Pipeline\n",
        "def train_improved_lung_classifier(x_train, y_train, x_val, y_val, x_test, y_test,\n",
        "                                 use_ensemble=False):\n",
        "    \"\"\"\n",
        "    Improved training pipeline focused on generalization\n",
        "\n",
        "    Key improvements:\n",
        "    1. Simpler architectures to reduce overfitting\n",
        "    2. Data augmentation for better generalization\n",
        "    3. Conservative training settings\n",
        "    4. Ensemble option for robust predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Improved Lung Sound Classification Pipeline\")\n",
        "    print(\"Focus: Better generalization on unseen data\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if use_ensemble:\n",
        "        # Train ensemble\n",
        "        ensemble = EnsembleLungClassifier()\n",
        "        models = ensemble.train_ensemble(x_train, y_train, x_val, y_val)\n",
        "        results = ensemble.evaluate_ensemble(x_test, y_test)\n",
        "        return ensemble, results\n",
        "    else:\n",
        "        # Train single improved model\n",
        "        classifier = ImprovedLungSoundClassifier()\n",
        "\n",
        "        # Try simple model first\n",
        "        history = classifier.train_with_data_augmentation(\n",
        "            x_train, y_train, x_val, y_val,\n",
        "            model_type='attention ',\n",
        "            epochs=150,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        results = classifier.evaluate_model(x_test, y_test)\n",
        "\n",
        "        # Plot training curves\n",
        "        classifier.plot_training_history()\n",
        "\n",
        "        return classifier, results\n",
        "\n",
        "\n",
        "# Usage instructions\n",
        "def usage_example():\n",
        "    \"\"\"How to use the improved classifier\"\"\"\n",
        "    print(\"\\nüìù Usage Example:\")\n",
        "    print(\"# For single improved model:\")\n",
        "    classifier, results = train_improved_lung_classifier(\n",
        "      x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru)\n",
        "    print(\")\")\n",
        "    print(\"\\n# For ensemble approach (better but slower):\")\n",
        "    # classifier, results = train_improved_lung_classifier(\n",
        "    # x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru,)\n",
        "    print(\"    use_ensemble=True\")\n",
        "    print(\")\")\n",
        "\n",
        "    print(\"\\nüí° Key Improvements:\")\n",
        "    print(\"‚Ä¢ Simpler architecture to prevent overfitting\")\n",
        "    print(\"‚Ä¢ Data augmentation for better generalization\")\n",
        "    print(\"‚Ä¢ Conservative training with early stopping\")\n",
        "    print(\"‚Ä¢ Ensemble option for robust predictions\")\n",
        "    print(\"‚Ä¢ Better regularization strategies\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    usage_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGqQ0HQ9IqEy"
      },
      "source": [
        "# üß†üìâ Model 2 ‚Äì LSTM-Based Lung Sound Classification\n",
        "\n",
        "This notebook uses a **Long Short-Term Memory (LSTM)** model for classifying lung sound recordings into disease categories. LSTM networks are powerful in handling **sequential and temporal data**, making them ideal for analyzing respiratory sound signals.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Classification Targets:\n",
        "- ‚úÖ **Normal**\n",
        "- üå¨Ô∏è **Asthma**\n",
        "- üòÆ‚Äçüí® **COPD**\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Why LSTM?\n",
        "- üîÅ Remembers patterns over time (important for wheezes/crackles)\n",
        "- ‚è≥ Excellent for sequential signal modeling\n",
        "- üß† Lightweight compared to CNN+Attention hybrids\n",
        "- ‚úÖ Proven for physiological time-series like ECG, audio, etc.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVS2kefrT3wd"
      },
      "outputs": [],
      "source": [
        "# Improved Lung Sound Classification - Focus on Generalization\n",
        "# Addresses overfitting issues for better unseen data performance\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU, LSTM,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU, SpatialDropout1D,\n",
        "    MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Improved Neural Network for Lung Sound Classification\n",
        "    Focus: Better generalization on unseen data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        \"\"\"Create a simpler, more generalizable model\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Simple feature extraction with lighter regularization\n",
        "        x = Conv1D(filters=32, kernel_size=7, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # Single RNN layer to reduce complexity\n",
        "        x = Bidirectional(\n",
        "            GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "        # Simpler dense layers\n",
        "        x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='SimpleLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_attention_model(self):\n",
        "        \"\"\"Create attention-based model for better feature learning\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Light conv preprocessing\n",
        "        x = Conv1D(filters=32, kernel_size=5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # RNN processing\n",
        "        x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=48,\n",
        "            dropout=0.2\n",
        "        )(x, x)\n",
        "        x = LayerNormalization()(x + attention)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = Dense(32, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='AttentionLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_ensemble_ready_model(self, model_variant='lstm'):\n",
        "        \"\"\"Create different model variants for ensemble\"\"\"\n",
        "        if model_variant == 'simple':\n",
        "            return self.create_simple_model()\n",
        "        elif model_variant == 'attention':\n",
        "            return self.create_attention_model()\n",
        "        elif model_variant == 'lstm':\n",
        "            return self.create_lstm_model()\n",
        "        else:\n",
        "            return self.create_simple_model()\n",
        "\n",
        "    def create_lstm_model(self):\n",
        "        \"\"\"LSTM variant for ensemble diversity\"\"\"\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "\n",
        "        # Feature extraction\n",
        "        x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "        # LSTM layers\n",
        "        x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Classification\n",
        "        x = Dense(48, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LSTMLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.0005):\n",
        "        \"\"\"Compile with conservative settings for better generalization\"\"\"\n",
        "        optimizer = Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        \"\"\"Compute balanced class weights\"\"\"\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(f\"üìä Class weights:\")\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        for i, weight in self.class_weights.items():\n",
        "            print(f\"   ‚Ä¢ {class_names[i]}: {weight:.3f}\")\n",
        "\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='improved_lung_model'):\n",
        "        \"\"\"Conservative callbacks for better generalization\"\"\"\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,  # Shorter patience to prevent overfitting\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,  # Reduce LR more aggressively\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train_with_data_augmentation(self, x_train, y_train, x_val, y_val,\n",
        "                                   model_type='lstm', epochs=80, batch_size=32):\n",
        "        \"\"\"Train with data augmentation for better generalization\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Training {model_type} model with data augmentation...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "\n",
        "        # Data augmentation\n",
        "        x_train_aug, y_train_aug = self.augment_data(x_train, y_train)\n",
        "        print(f\"   ‚Ä¢ Augmented training samples: {x_train_aug.shape[0]}\")\n",
        "\n",
        "        # Create model\n",
        "        self.model = self.create_ensemble_ready_model(model_type)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = self.compute_class_weights(y_train_aug)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = self.create_callbacks(f'{model_type}_lung_model')\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture ({model_type}):\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            x_train_aug, y_train_aug,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def augment_data(self, x_data, y_data, augment_factor=0.5):\n",
        "        \"\"\"Simple data augmentation techniques\"\"\"\n",
        "        augmented_x = []\n",
        "        augmented_y = []\n",
        "\n",
        "        # Original data\n",
        "        augmented_x.append(x_data)\n",
        "        augmented_y.append(y_data)\n",
        "\n",
        "        n_augment = int(len(x_data) * augment_factor)\n",
        "        indices = np.random.choice(len(x_data), n_augment, replace=True)\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = x_data[idx].copy()\n",
        "            label = y_data[idx].copy()\n",
        "\n",
        "            # Random noise addition (5% of signal std)\n",
        "            noise_level = 0.05 * np.std(sample)\n",
        "            sample += np.random.normal(0, noise_level, sample.shape)\n",
        "\n",
        "            # Random scaling (¬±10%)\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            sample *= scale_factor\n",
        "\n",
        "            augmented_x.append(sample[np.newaxis, :])\n",
        "            augmented_y.append(label[np.newaxis, :])\n",
        "\n",
        "        return np.vstack(augmented_x), np.vstack(augmented_y)\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Metrics\n",
        "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        # Per-class metrics\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training curves to check for overfitting\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val', color='orange')\n",
        "        axes[0, 0].set_title('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val', color='orange')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision\n",
        "        axes[1, 0].plot(self.history.history['precision'], label='Train', color='blue')\n",
        "        axes[1, 0].plot(self.history.history['val_precision'], label='Val', color='orange')\n",
        "        axes[1, 0].set_title('Precision')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recall\n",
        "        axes[1, 1].plot(self.history.history['recall'], label='Train', color='blue')\n",
        "        axes[1, 1].plot(self.history.history['val_recall'], label='Val', color='orange')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class EnsembleLungClassifier:\n",
        "    \"\"\"Ensemble approach for robust predictions\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.models = []\n",
        "        self.model_types = ['simple', 'attention', 'lstm']\n",
        "\n",
        "    def train_ensemble(self, x_train, y_train, x_val, y_val, epochs=60):\n",
        "        \"\"\"Train ensemble of diverse models\"\"\"\n",
        "        print(\"üéØ Training Ensemble of Models...\")\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nüîÑ Training {model_type} model...\")\n",
        "\n",
        "            classifier = ImprovedLungSoundClassifier(self.input_shape, self.num_classes)\n",
        "            history = classifier.train_with_data_augmentation(\n",
        "                x_train, y_train, x_val, y_val,\n",
        "                model_type=model_type,\n",
        "                epochs=epochs,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.models.append(classifier.model)\n",
        "            print(f\"‚úÖ {model_type} model trained!\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def predict_ensemble(self, x_test):\n",
        "        \"\"\"Make ensemble predictions\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained!\")\n",
        "            return None\n",
        "\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(x_test, verbose=0)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Average predictions\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate_ensemble(self, x_test, y_test):\n",
        "        \"\"\"Evaluate ensemble performance\"\"\"\n",
        "        ensemble_pred = self.predict_ensemble(x_test)\n",
        "        if ensemble_pred is None:\n",
        "            return None\n",
        "\n",
        "        y_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüéØ Ensemble Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "\n",
        "# Improved Training Pipeline\n",
        "def train_improved_lung_classifier(x_train, y_train, x_val, y_val, x_test, y_test,\n",
        "                                 use_ensemble=False):\n",
        "    \"\"\"\n",
        "    Improved training pipeline focused on generalization\n",
        "\n",
        "    Key improvements:\n",
        "    1. Simpler architectures to reduce overfitting\n",
        "    2. Data augmentation for better generalization\n",
        "    3. Conservative training settings\n",
        "    4. Ensemble option for robust predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Improved Lung Sound Classification Pipeline\")\n",
        "    print(\"Focus: Better generalization on unseen data\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if use_ensemble:\n",
        "        # Train ensemble\n",
        "        ensemble = EnsembleLungClassifier()\n",
        "        models = ensemble.train_ensemble(x_train, y_train, x_val, y_val)\n",
        "        results = ensemble.evaluate_ensemble(x_test, y_test)\n",
        "        return ensemble, results\n",
        "    else:\n",
        "        # Train single improved model\n",
        "        classifier = ImprovedLungSoundClassifier()\n",
        "\n",
        "        # Try simple model first\n",
        "        history = classifier.train_with_data_augmentation(\n",
        "            x_train, y_train, x_val, y_val,\n",
        "            model_type='lstm',\n",
        "            epochs=150,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        results = classifier.evaluate_model(x_test, y_test)\n",
        "\n",
        "        # Plot training curves\n",
        "        classifier.plot_training_history()\n",
        "\n",
        "        return classifier, results\n",
        "\n",
        "\n",
        "# Usage instructions\n",
        "def usage_example():\n",
        "    \"\"\"How to use the improved classifier\"\"\"\n",
        "    print(\"\\nüìù Usage Example:\")\n",
        "    print(\"# For single improved model:\")\n",
        "    classifier, results = train_improved_lung_classifier(\n",
        "      x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru)\n",
        "    print(\")\")\n",
        "    print(\"\\n# For ensemble approach (better but slower):\")\n",
        "    # classifier, results = train_improved_lung_classifier(\n",
        "    # x_train_gru, y_train_gru, x_val_gru, y_val_gru, x_test_gru, y_test_gru,)\n",
        "    print(\"    use_ensemble=True\")\n",
        "    print(\")\")\n",
        "\n",
        "    print(\"\\nüí° Key Improvements:\")\n",
        "    print(\"‚Ä¢ Simpler architecture to prevent overfitting\")\n",
        "    print(\"‚Ä¢ Data augmentation for better generalization\")\n",
        "    print(\"‚Ä¢ Conservative training with early stopping\")\n",
        "    print(\"‚Ä¢ Ensemble option for robust predictions\")\n",
        "    print(\"‚Ä¢ Better regularization strategies\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    usage_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RknVH3kT3oU"
      },
      "outputs": [],
      "source": [
        "# prompt: plot classification using plotly for all the three appoach of model2  as we had already trained above\n",
        "\n",
        "from itertools import cycle\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, GRU, LSTM,\n",
        "    Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
        "    Concatenate, Conv1D, LeakyReLU, SpatialDropout1D,\n",
        "    MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "\n",
        "\n",
        "# Define the custom metric if not already defined\n",
        "@register_keras_serializable()\n",
        "def weighted_categorical_accuracy(y_true, y_pred):\n",
        "    \"\"\"Custom weighted categorical accuracy metric\"\"\"\n",
        "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "# Re-define the ImprovedLungSoundClassifier class if necessary (based on the last code block)\n",
        "# Or assume it's already defined and accessible in the environment\n",
        "class ImprovedLungSoundClassifier:\n",
        "    \"\"\"\n",
        "    Improved Neural Network for Lung Sound Classification\n",
        "    Focus: Better generalization on unseen data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "        x = Conv1D(filters=32, kernel_size=7, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "        x = Bidirectional(\n",
        "            GRU(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
        "        )(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = Concatenate()([avg_pool, max_pool])\n",
        "        x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='SimpleLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_attention_model(self):\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "        x = Conv1D(filters=32, kernel_size=5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "        x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "        attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=48,\n",
        "            dropout=0.2\n",
        "        )(x, x)\n",
        "        x = LayerNormalization()(x + attention)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dense(32, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='AttentionLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_lstm_model(self):\n",
        "        inputs = Input(shape=self.input_shape, name='lung_sound_input')\n",
        "        x = inputs\n",
        "        x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = SpatialDropout1D(0.2)(x)\n",
        "        x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dense(48, kernel_regularizer=l2(0.001))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='LSTMLungClassifier')\n",
        "        return model\n",
        "\n",
        "    def create_ensemble_ready_model(self, model_variant='simple'):\n",
        "        if model_variant == 'simple':\n",
        "            return self.create_simple_model()\n",
        "        elif model_variant == 'attention':\n",
        "            return self.create_attention_model()\n",
        "        elif model_variant == 'lstm':\n",
        "            return self.create_lstm_model()\n",
        "        else:\n",
        "            return self.create_simple_model()\n",
        "\n",
        "    def compile_model(self, model, learning_rate=0.0005):\n",
        "        optimizer = Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        y_indices = np.argmax(y_train, axis=1)\n",
        "        classes = np.unique(y_indices)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)\n",
        "        self.class_weights = dict(zip(classes, class_weights))\n",
        "        return self.class_weights\n",
        "\n",
        "    def create_callbacks(self, model_name='improved_lung_model'):\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=f'{model_name}_best.keras',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "        return callbacks\n",
        "\n",
        "    def augment_data(self, x_data, y_data, augment_factor=0.5):\n",
        "        \"\"\"Simple data augmentation techniques\"\"\"\n",
        "        augmented_x = []\n",
        "        augmented_y = []\n",
        "\n",
        "        # Original data\n",
        "        augmented_x.append(x_data)\n",
        "        augmented_y.append(y_data)\n",
        "\n",
        "        n_augment = int(len(x_data) * augment_factor)\n",
        "        indices = np.random.choice(len(x_data), n_augment, replace=True)\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = x_data[idx].copy()\n",
        "            label = y_data[idx].copy()\n",
        "\n",
        "            # Random noise addition (5% of signal std)\n",
        "            noise_level = 0.05 * np.std(sample)\n",
        "            sample += np.random.normal(0, noise_level, sample.shape)\n",
        "\n",
        "            # Random scaling (¬±10%)\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            sample *= scale_factor\n",
        "\n",
        "            augmented_x.append(sample[np.newaxis, :])\n",
        "            augmented_y.append(label[np.newaxis, :])\n",
        "\n",
        "        return np.vstack(augmented_x), np.vstack(augmented_y)\n",
        "\n",
        "    def train_with_data_augmentation(self, x_train, y_train, x_val, y_val,\n",
        "                                   model_type='simple', epochs=80, batch_size=32):\n",
        "\n",
        "        print(f\"üöÄ Training {model_type} model with data augmentation...\")\n",
        "        print(f\"   ‚Ä¢ Training samples: {x_train.shape[0]}\")\n",
        "        print(f\"   ‚Ä¢ Validation samples: {x_val.shape[0]}\")\n",
        "\n",
        "        x_train_aug, y_train_aug = self.augment_data(x_train, y_train)\n",
        "        print(f\"   ‚Ä¢ Augmented training samples: {x_train_aug.shape[0]}\")\n",
        "\n",
        "        self.model = self.create_ensemble_ready_model(model_type)\n",
        "        self.model = self.compile_model(self.model)\n",
        "\n",
        "        class_weights = self.compute_class_weights(y_train_aug)\n",
        "\n",
        "        callbacks = self.create_callbacks(f'{model_type}_lung_model')\n",
        "\n",
        "        print(f\"\\nüèóÔ∏è Model Architecture ({model_type}):\")\n",
        "        print(f\"   ‚Ä¢ Total parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            x_train_aug, y_train_aug,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, x_test, y_test):\n",
        "        if self.model is None:\n",
        "            print(\"‚ùå Model not trained yet!\")\n",
        "            return\n",
        "\n",
        "        y_pred_proba = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüìä Test Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {test_prec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {test_rec:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "        class_names = ['Healthy', 'Asthma', 'COPD']\n",
        "        print(f\"\\nüìã Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1_score': test_f1,\n",
        "            'predictions': y_pred_proba,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        if self.history is None:\n",
        "            print(\"‚ùå No training history!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Train', color='blue')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val', color='orange')\n",
        "        axes[0, 0].set_title('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Train', color='blue')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Val', color='orange')\n",
        "        axes[0, 1].set_title('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1, 0].plot(self.history.history['precision'], label='Train', color='blue')\n",
        "        axes[1, 0].plot(self.history.history['val_precision'], label='Val', color='orange')\n",
        "        axes[1, 0].set_title('Precision')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1, 1].plot(self.history.history['recall'], label='Train', color='blue')\n",
        "        axes[1, 1].plot(self.history.history['val_recall'], label='Val', color='orange')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "class EnsembleLungClassifier:\n",
        "    def __init__(self, input_shape=(1, 959), num_classes=3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.models = []\n",
        "        self.model_types = ['simple', 'attention', 'lstm']\n",
        "\n",
        "    def train_ensemble(self, x_train, y_train, x_val, y_val, epochs=60):\n",
        "        print(\"üéØ Training Ensemble of Models...\")\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nüîÑ Training {model_type} model...\")\n",
        "\n",
        "            classifier = ImprovedLungSoundClassifier(self.input_shape, self.num_classes)\n",
        "            history = classifier.train_with_data_augmentation(\n",
        "                x_train, y_train, x_val, y_val,\n",
        "                model_type=model_type,\n",
        "                epochs=epochs,\n",
        "                batch_size=32\n",
        "            )\n",
        "\n",
        "            self.models.append(classifier.model)\n",
        "            print(f\"‚úÖ {model_type} model trained!\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def predict_ensemble(self, x_test):\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained!\")\n",
        "            return None\n",
        "\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model.predict(x_test, verbose=0)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate_ensemble(self, x_test, y_test):\n",
        "        ensemble_pred = self.predict_ensemble(x_test)\n",
        "        if ensemble_pred is None:\n",
        "            return None\n",
        "\n",
        "        y_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nüéØ Ensemble Performance:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "\n",
        "# Define the plotting function using Plotly\n",
        "def plot_classification_plotly(model_results: Dict[str, Dict], class_names: List[str]):\n",
        "    \"\"\"\n",
        "    Plots classification results (Confusion Matrix and Metrics) for multiple models using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "    - model_results: A dictionary where keys are model names (e.g., 'Simple', 'Attention', 'LSTM')\n",
        "                     and values are dictionaries containing 'confusion_matrix' and 'report' (from classification_report).\n",
        "    - class_names: List of class names.\n",
        "    \"\"\"\n",
        "    print(\"üìä Generating Plotly Classification Plots...\")\n",
        "\n",
        "    # Create subplots: one row for confusion matrices, one row for metrics bars\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=len(model_results),\n",
        "        specs=[[{'type': 'heatmap'}] * len(model_results),\n",
        "               [{'type': 'bar'}] * len(model_results)],\n",
        "        subplot_titles=[f'Confusion Matrix: {name}' for name in model_results.keys()] +\n",
        "                       [f'Metrics: {name}' for name in model_results.keys()],\n",
        "        vertical_spacing=0.1,\n",
        "        horizontal_spacing=0.05\n",
        "    )\n",
        "\n",
        "    row_cm = 1\n",
        "    row_metrics = 2\n",
        "    col = 1\n",
        "\n",
        "    for model_name, results in model_results.items():\n",
        "        cm = results.get('confusion_matrix')\n",
        "        report_str = results.get('report') # Get the string output from classification_report\n",
        "\n",
        "        if cm is not None:\n",
        "            # Confusion Matrix Plotly\n",
        "            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            cm_text = np.array([[f'{cm[i,j]}\\n({cm_percent[i,j]*100:.1f}%)' for j in range(len(class_names))] for i in range(len(class_names))])\n",
        "\n",
        "            heatmap = go.Heatmap(\n",
        "                z=cm_percent,\n",
        "                x=class_names,\n",
        "                y=class_names,\n",
        "                colorscale='Blues',\n",
        "                colorbar=dict(title='%'),\n",
        "                text=cm_text,\n",
        "                texttemplate=\"%{text}\",\n",
        "                hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{text:.0f}<br>Accuracy: %{z:.1%}<extra></extra>'\n",
        "            )\n",
        "            fig.add_trace(heatmap, row=row_cm, col=col)\n",
        "\n",
        "        if report_str:\n",
        "            # Parse classification report string\n",
        "            report_data = {}\n",
        "            lines = report_str.split('\\n')\n",
        "            # Find the line after which class-wise metrics start (usually after headers)\n",
        "            data_lines = [line for line in lines if line.strip() and not line.startswith(' ')]\n",
        "            header_end_index = 0\n",
        "            for i, line in enumerate(lines):\n",
        "                 if \"precision\" in line and \"recall\" in line and \"f1-score\" in line:\n",
        "                    header_end_index = i + 1\n",
        "                    break\n",
        "\n",
        "            metrics_lines = lines[header_end_index:]\n",
        "            # Filter out empty lines and support line\n",
        "            metrics_lines = [line for line in metrics_lines if line.strip() and not line.strip().startswith('support')]\n",
        "\n",
        "            for line in metrics_lines:\n",
        "                 parts = line.split()\n",
        "                 if len(parts) >= 4: # Expect class, precision, recall, f1-score\n",
        "                     class_label = parts[0]\n",
        "                     if class_label in class_names:\n",
        "                         try:\n",
        "                             precision_val = float(parts[1])\n",
        "                             recall_val = float(parts[2])\n",
        "                             f1_val = float(parts[3])\n",
        "                             report_data[class_label] = {'precision': precision_val, 'recall': recall_val, 'f1-score': f1_val}\n",
        "                         except ValueError:\n",
        "                             continue # Skip if parsing fails\n",
        "\n",
        "            # Metrics Bar Chart Plotly\n",
        "            metrics = ['precision', 'recall', 'f1-score']\n",
        "            class_metrics = {metric: [] for metric in metrics}\n",
        "\n",
        "            for class_name in class_names:\n",
        "                if class_name in report_data:\n",
        "                     for metric in metrics:\n",
        "                         class_metrics[metric].append(report_data[class_name][metric])\n",
        "                else: # Handle cases where a class might have no predictions\n",
        "                     for metric in metrics:\n",
        "                         class_metrics[metric].append(0.0) # Append 0 or NaN if appropriate\n",
        "\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=class_names,\n",
        "                y=class_metrics['precision'],\n",
        "                name='Precision',\n",
        "                marker_color='rgba(58,200,225,.5)',\n",
        "                hovertemplate='Class: %{x}<br>Precision: %{y:.2f}<extra></extra>'\n",
        "            ), row=row_metrics, col=col)\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=class_names,\n",
        "                y=class_metrics['recall'],\n",
        "                name='Recall',\n",
        "                marker_color='rgba(200,58,58,.5)',\n",
        "                 hovertemplate='Class: %{x}<br>Recall: %{y:.2f}<extra></extra>'\n",
        "            ), row=row_metrics, col=col)\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=class_names,\n",
        "                y=class_metrics['f1-score'],\n",
        "                name='F1-Score',\n",
        "                marker_color='rgba(58,200,58,.5)',\n",
        "                 hovertemplate='Class: %{x}<br>F1-Score: %{y:.2f}<extra></extra>'\n",
        "            ), row=row_metrics, col=col)\n",
        "\n",
        "        # Update layout for the column\n",
        "        fig.update_yaxes(title_text=\"True Label\", row=row_cm, col=col)\n",
        "        fig.update_xaxes(title_text=\"Predicted Label\", row=row_cm, col=col)\n",
        "        fig.update_yaxes(title_text=\"Score\", range=[0, 1], row=row_metrics, col=col)\n",
        "        fig.update_xaxes(title_text=\"Class\", row=row_metrics, col=col)\n",
        "\n",
        "\n",
        "        col += 1 # Move to the next column for the next model\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=\"Model 2 Classification Performance Comparison (Plotly)\",\n",
        "        height=800,\n",
        "        showlegend=True # Show one legend for all metric bars\n",
        "    )\n",
        "\n",
        "    # Adjust legend positioning to avoid overlap\n",
        "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.1, xanchor=\"center\", x=0.5))\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# Assume x_test_gru and y_test_gru are available from previous steps\n",
        "# Assume 'classifier' (the trained ensemble object) is also available\n",
        "\n",
        "# Re-train models to get individual classifiers and their test results\n",
        "print(\"\\n retraining individual models for comparison...\")\n",
        "\n",
        "# Simple Model\n",
        "simple_classifier = ImprovedLungSoundClassifier()\n",
        "simple_classifier.train_with_data_augmentation(\n",
        "    x_train_gru, y_train_gru, x_val_gru, y_val_gru,\n",
        "    model_type='simple', epochs=150, batch_size=32\n",
        ")\n",
        "simple_results = simple_classifier.evaluate_model(x_test_gru, y_test_gru)\n",
        "# Capture the classification report string\n",
        "y_true_simple = np.argmax(y_test_gru, axis=1)\n",
        "y_pred_proba_simple = simple_classifier.model.predict(x_test_gru, verbose=0)\n",
        "y_pred_simple = np.argmax(y_pred_proba_simple, axis=1)\n",
        "simple_report_str = classification_report(y_true_simple, y_pred_simple, target_names=[\"Healthy\", \"Asthma\", \"COPD\"])\n",
        "simple_results['report'] = simple_report_str\n",
        "\n",
        "\n",
        "# Attention Model\n",
        "attention_classifier = ImprovedLungSoundClassifier()\n",
        "attention_classifier.train_with_data_augmentation(\n",
        "    x_train_gru, y_train_gru, x_val_gru, y_val_gru,\n",
        "    model_type='attention', epochs=150, batch_size=32\n",
        ")\n",
        "attention_results = attention_classifier.evaluate_model(x_test_gru, y_test_gru)\n",
        "# Capture the classification report string\n",
        "y_true_attention = np.argmax(y_test_gru, axis=1)\n",
        "y_pred_proba_attention = attention_classifier.model.predict(x_test_gru, verbose=0)\n",
        "y_pred_attention = np.argmax(y_pred_proba_attention, axis=1)\n",
        "attention_report_str = classification_report(y_true_attention, y_pred_attention, target_names=[\"Healthy\", \"Asthma\", \"COPD\"])\n",
        "attention_results['report'] = attention_report_str\n",
        "\n",
        "\n",
        "# LSTM Model\n",
        "lstm_classifier = ImprovedLungSoundClassifier()\n",
        "lstm_classifier.train_with_data_augmentation(\n",
        "    x_train_gru, y_train_gru, x_val_gru, y_val_gru,\n",
        "    model_type='lstm', epochs=150, batch_size=32\n",
        ")\n",
        "lstm_results = lstm_classifier.evaluate_model(x_test_gru, y_test_gru)\n",
        "# Capture the classification report string\n",
        "y_true_lstm = np.argmax(y_test_gru, axis=1)\n",
        "y_pred_proba_lstm = lstm_classifier.model.predict(x_test_gru, verbose=0)\n",
        "y_pred_lstm = np.argmax(y_pred_proba_lstm, axis=1)\n",
        "lstm_report_str = classification_report(y_true_lstm, y_pred_lstm, target_names=[\"Healthy\", \"Asthma\", \"COPD\"])\n",
        "lstm_results['report'] = lstm_report_str\n",
        "\n",
        "\n",
        "# Prepare data for Plotly\n",
        "model2_comparison_results = {\n",
        "    'Simple Model': simple_results,\n",
        "    'Attention Model': attention_results,\n",
        "    'LSTM Model': lstm_results,\n",
        "}\n",
        "\n",
        "class_names = [\"Healthy\", \"Asthma\", \"COPD\"]\n",
        "\n",
        "# Generate Plotly classification plots\n",
        "plot_classification_plotly(model2_comparison_results, class_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uimpknFJ65wL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}